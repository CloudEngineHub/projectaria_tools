{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "616f1c01-e67c-46a4-a36a-7946d0b01ac9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import projectaria_tools.core.mps as mps\n",
        "import rerun as rr\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "from projectaria_tools.core import data_provider, sensor_data, sophus\n",
        "from projectaria_tools.core.calibration import CameraCalibration, DeviceCalibration\n",
        "from projectaria_tools.core.mps.utils import get_nearest_wrist_and_palm_pose\n",
        "from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\n",
        "from projectaria_tools.core.stream_id import RecordableTypeId, StreamId\n",
        "from projectaria_tools.utils.rerun_helpers import ToTransform3D\n",
        "from tqdm import tqdm\n",
        "\n",
        "import mediapipe as mp\n",
        "from mediapipe import solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bf93bb3-c284-4942-b2ea-929c405b1957",
      "metadata": {},
      "source": [
        "### Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f3f0c23-fd67-4961-a01a-7b8773854e40",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download media pipe task description for Hand landmark detection\n",
        "!mkdir -p ~/mediapipe/\n",
        "!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task -O ~/mediapipe/hand_landmarker.task\n",
        "mp_hand_landmarker_task_path = \"~/mediapipe/hand_landmarker.task\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "620c2ace-1c3e-4b2a-9e9a-9b76464c85d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download one sequence from public HOT3D dataset (see https://github.com/facebookresearch/hot3d?tab=readme-ov-file#step-3-download-the-data)\n",
        "vrs_path = \"~/hot3d_data/P0003_c701bd11/recording.vrs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2ed9f81-53da-4e8e-a455-bed21ee62449",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make sure Aria MPS HT data is available. Usually can be found in the parent folder of the vrs file\n",
        "# See https://facebookresearch.github.io/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_getting_started#getting-started for requesting a MPS run\n",
        "mps_wrist_csv_path = \"~/hot3d_data/P0003_c701bd11/mps/ht/wrist_output.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efd943c1-e0ed-47f3-a670-449813e48461",
      "metadata": {},
      "source": [
        "### Data processing utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04eabd30-03d3-4d84-a59c-34398214c2b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_frame_data(\n",
        "    provider: data_provider.VrsDataProvider, stream_label: str, frame_idx: int\n",
        "):\n",
        "    stream_id = provider.get_stream_id_from_label(stream_label)\n",
        "    image_data = provider.get_image_data_by_index(stream_id, frame_idx)\n",
        "    return image_data\n",
        "\n",
        "\n",
        "def get_camera_calibration(\n",
        "    provider: data_provider.VrsDataProvider, stream_label: str\n",
        ") -> CameraCalibration:\n",
        "    device_calib: Optional[DeviceCalibration] = provider.get_device_calibration()\n",
        "    assert device_calib is not None, \"Cannot get device calibration\"\n",
        "    cam_calib: Optional[CameraCalibration] = device_calib.get_camera_calib(stream_label)\n",
        "    assert cam_calib is not None, f\"Cannot get calibration for {stream_label}\"\n",
        "\n",
        "    return cam_calib\n",
        "\n",
        "\n",
        "def gray_to_rgb(img: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    mediapipe input has to be 3 channel RGB image, so we convert the grayscale\n",
        "    image to RGB image\n",
        "    \"\"\"\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bad9afa3-1935-4e98-9100-f6df5b12315d",
      "metadata": {},
      "source": [
        "### Visualization utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4bc2e21-8971-4aad-b793-53c47c408bb7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# From https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "    MARGIN = 10  # pixels\n",
        "    FONT_SIZE = 1\n",
        "    FONT_THICKNESS = 1\n",
        "    HANDEDNESS_TEXT_COLOR = (88, 205, 54)  # vibrant green\n",
        "\n",
        "    hand_landmarks_list = detection_result.hand_landmarks\n",
        "    handedness_list = detection_result.handedness\n",
        "    annotated_image = np.copy(rgb_image)\n",
        "\n",
        "    # Loop through the detected hands to visualize.\n",
        "    for idx in range(len(hand_landmarks_list)):\n",
        "        hand_landmarks = hand_landmarks_list[idx]\n",
        "        handedness = handedness_list[idx]\n",
        "\n",
        "        # Draw the hand landmarks.\n",
        "        hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "        hand_landmarks_proto.landmark.extend(\n",
        "            [\n",
        "                landmark_pb2.NormalizedLandmark(\n",
        "                    x=landmark.x, y=landmark.y, z=landmark.z\n",
        "                )\n",
        "                for landmark in hand_landmarks\n",
        "            ]\n",
        "        )\n",
        "        solutions.drawing_utils.draw_landmarks(\n",
        "            annotated_image,\n",
        "            hand_landmarks_proto,\n",
        "            solutions.hands.HAND_CONNECTIONS,\n",
        "            solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
        "            solutions.drawing_styles.get_default_hand_connections_style(),\n",
        "        )\n",
        "\n",
        "        # Get the top left corner of the detected hand's bounding box.\n",
        "        height, width, _ = annotated_image.shape\n",
        "        x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
        "        y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
        "        text_x = int(min(x_coordinates) * width)\n",
        "        text_y = int(min(y_coordinates) * height) - MARGIN\n",
        "\n",
        "        # Draw handedness (left or right hand) on the image.\n",
        "        cv2.putText(\n",
        "            annotated_image,\n",
        "            f\"{handedness[0].category_name}\",\n",
        "            (text_x, text_y),\n",
        "            cv2.FONT_HERSHEY_DUPLEX,\n",
        "            FONT_SIZE,\n",
        "            HANDEDNESS_TEXT_COLOR,\n",
        "            FONT_THICKNESS,\n",
        "            cv2.LINE_AA,\n",
        "        )\n",
        "\n",
        "    return annotated_image\n",
        "\n",
        "\n",
        "def show_hand_skeleton(\n",
        "    pts: np.ndarray, connections: List, color_int8: List[int], label: str\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize a hand skeleton as 3d line set\n",
        "    \"\"\"\n",
        "    line3d = []\n",
        "    for edge in connections:\n",
        "        start_idx, end_idx = edge[0], edge[1]\n",
        "        landmark_pair = [pts[start_idx], pts[end_idx]]\n",
        "        line3d.append(landmark_pair)\n",
        "    colors = [color_int8] * len(line3d)\n",
        "    rr.log(label, rr.LineStrips3D(line3d, radii=0.002, colors=colors))\n",
        "    colors = [color_int8] * len(pts)\n",
        "    rr.log(label, rr.Points3D(pts, radii=0.0025, colors=colors))\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Intrinsics:\n",
        "    \"\"\"\n",
        "    Pinhole camera intrinsics\n",
        "    \"\"\"\n",
        "\n",
        "    focal: float\n",
        "    cx: float\n",
        "    cy: float\n",
        "    w: int\n",
        "    h: int\n",
        "\n",
        "\n",
        "def log_cam(T_world_cam: sophus.SE3, intrinsics: Intrinsics, label: str) -> None:\n",
        "    \"\"\"\n",
        "    Logs a pinhole camera with given pose to Rerun\n",
        "    \"\"\"\n",
        "    rr.log(label, ToTransform3D(T_world_cam, False))\n",
        "    rr.log(\n",
        "        label,\n",
        "        rr.Pinhole(\n",
        "            focal_length=intrinsics.focal,\n",
        "            width=intrinsics.w,\n",
        "            height=intrinsics.h,\n",
        "            principal_point=[intrinsics.cx, intrinsics.cy],\n",
        "        ),\n",
        "    )\n",
        "\n",
        "\n",
        "def log_pose(pose: sophus.SE3, label: str, static=False) -> None:\n",
        "    rr.log(label, ToTransform3D(pose, False), static=static)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb7a2413-b177-4c84-bee3-5e84aa1d7d61",
      "metadata": {},
      "source": [
        "### Hand tracking utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73d77c56-8c9e-4684-b5ff-236bf99d6632",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_ht_detector():\n",
        "    base_options = python.BaseOptions(\n",
        "        model_asset_path=mp_hand_landmarker_task_path\n",
        "    )\n",
        "    options = vision.HandLandmarkerOptions(\n",
        "        base_options=base_options,\n",
        "        running_mode=vision.RunningMode.IMAGE,\n",
        "        num_hands=2,\n",
        "        min_hand_detection_confidence=0.5,\n",
        "        min_hand_presence_confidence=0.5,\n",
        "    )\n",
        "    detector = vision.HandLandmarker.create_from_options(options)\n",
        "\n",
        "    return detector\n",
        "\n",
        "\n",
        "def run_mediapipe_ht(detector: vision.HandLandmarker, img: np.ndarray):\n",
        "    # convert grayscale to RGB by replicating channel 3 times\n",
        "    if img.ndim == 2:\n",
        "        img = gray_to_rgb(img)\n",
        "\n",
        "    mp_img = mp.Image(image_format=mp.ImageFormat.SRGB, data=img)\n",
        "    detection_result = detector.detect(mp_img)\n",
        "\n",
        "    return detection_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65c8dc0a-4639-4e90-b762-ee17cbc57e59",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here is how to run MediaPipe HT on an image\n",
        "vrs_provider: data_provider.VrsDataProvider = data_provider.create_vrs_data_provider(\n",
        "    vrs_path\n",
        ")\n",
        "sample_frame_index = 120\n",
        "img = get_frame_data(\n",
        "    provider=vrs_provider, stream_label=\"camera-rgb\", frame_idx=sample_frame_index\n",
        ")[0].to_numpy_array()\n",
        "\n",
        "# Run hand detector\n",
        "detector = create_ht_detector()\n",
        "det_res = run_mediapipe_ht(detector, img)\n",
        "\n",
        "# Overlay landmarks on image\n",
        "img_w_landmarks = draw_landmarks_on_image(img, det_res)\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(img_w_landmarks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dea50aed-71f6-4e1a-9a50-353944099bbe",
      "metadata": {},
      "source": [
        "## Solve hand pose in camera space"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3104be9-a5c1-4a51-a113-acd6244e78bf",
      "metadata": {},
      "source": [
        "Now we show how to put detected hand at wrist location provided by Aria MPS to have 3d hands in camera space.\n",
        "\n",
        "First we define some utility for un-projection, and hand detection data conversion."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fdd4199-c83e-4cd4-8da9-34cdd0d3905e",
      "metadata": {},
      "source": [
        "### Utility for geometry, hand tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0167b2ba-082f-4571-b6c8-af4a156bfe60",
      "metadata": {},
      "outputs": [],
      "source": [
        "def unproj_img_pts(\n",
        "    pts_im: np.ndarray, depth: np.ndarray, cam: CameraCalibration\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Unproject floating 2.5d pixel coordinates along its ray to the 3d point at the corresponding depth.\n",
        "    Returns:\n",
        "    - unprojected 3d points in camera\n",
        "    - whether a point is valid or not (due to vignette masking, etc.)\n",
        "\n",
        "    - `pts_im`: shape (N, 3), array of 2.5d points, each has (x, y, relative-depth)\n",
        "    - `depth`: shape (N,), each entry provides corresponding depth a point should land at\n",
        "    - `cam`: the camera model used to perform unprojection\n",
        "    \"\"\"\n",
        "    pts_cam = np.zeros([len(pts_im), 3])\n",
        "    valid_flags = np.zeros(len(pts_im)).astype(bool)\n",
        "    for i, p in enumerate(pts_im):\n",
        "        valid = cam.is_visible(p)\n",
        "        valid_flags[i] = valid\n",
        "        p_cam = cam.unproject(camera_pixel=p)\n",
        "        if valid:\n",
        "            assert p_cam is not None\n",
        "            # scale by depth\n",
        "            pts_cam[i] = p_cam * depth[i] / p_cam[2]\n",
        "        else:\n",
        "            assert p_cam is None\n",
        "\n",
        "    # pts_cam[:, 2][valid_flags == True] = depth[valid_flags == True]\n",
        "    return pts_cam, valid_flags\n",
        "\n",
        "\n",
        "def get_landmark_img_pts(\n",
        "    det_res: vision.HandLandmarkerResult,\n",
        "    img_wh: Tuple[int, int],\n",
        "    wrist_depth: List[float],\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Extract 2.5D landmark as np array, where each point follows:\n",
        "    [pix_x, pix_y, relative_depth_to_wrist]\n",
        "    Return is a tuple of two hands\n",
        "    \"\"\"\n",
        "    w, h = img_wh\n",
        "    ret = [None, None]\n",
        "    for hi, one_hand_lmks in enumerate(det_res.hand_landmarks):\n",
        "        if len(one_hand_lmks) == 0:\n",
        "            continue\n",
        "\n",
        "        hand_cat: mp.tasks.components.containers.Category = det_res.handedness[hi][0]\n",
        "        hand_index = 0 if hand_cat.category_name == \"Left\" else 1\n",
        "        wrist_depth_cur_hand = wrist_depth[hand_index]\n",
        "        n_lmks = len(one_hand_lmks)\n",
        "        hand_lmks = np.zeros([n_lmks, 3])\n",
        "\n",
        "        # Shif all joints by wrist depth\n",
        "        for i, lmk in enumerate(one_hand_lmks):\n",
        "            hand_lmks[i] = [lmk.x * w, lmk.y * h, lmk.z]\n",
        "            if wrist_depth is not None:\n",
        "                hand_lmks[i][2] += wrist_depth_cur_hand\n",
        "        ret[hand_index] = hand_lmks\n",
        "    return tuple(ret)\n",
        "\n",
        "\n",
        "def get_landmark_cam_pts(\n",
        "    landmarks_img: np.ndarray, cam: CameraCalibration\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    pts_cam, valid_flags = unproj_img_pts(\n",
        "        pts_im=landmarks_img[:, :2], depth=landmarks_img[:, 2], cam=cam\n",
        "    )\n",
        "    return pts_cam, valid_flags\n",
        "\n",
        "\n",
        "def convert_mp_det_to_cam_landmarks(\n",
        "    det_res: vision.HandLandmarkerResult,\n",
        "    wrist_depth: List[float],\n",
        "    cam: CameraCalibration,\n",
        "    verbose: bool = False,\n",
        ") -> List[Optional[np.ndarray]]:\n",
        "    w, h = cam.get_image_size()\n",
        "    lmks_per_hand = get_landmark_img_pts(\n",
        "        det_res=det_res, img_wh=(w, h), wrist_depth=wrist_depth\n",
        "    )\n",
        "    pts_per_hand = [None, None]\n",
        "    for hi, lmks_one_hand in enumerate(lmks_per_hand):\n",
        "        if lmks_one_hand is not None:\n",
        "            pts_cam, valid_flags = get_landmark_cam_pts(\n",
        "                landmarks_img=lmks_one_hand, cam=cam\n",
        "            )\n",
        "            if not valid_flags.all():\n",
        "                if verbose:\n",
        "                    print(f\"Hand {hi} has invalid joints. Skipped.\")\n",
        "                continue\n",
        "            pts_per_hand[hi] = pts_cam\n",
        "\n",
        "    return pts_per_hand\n",
        "\n",
        "\n",
        "def convert_world_to_camera(pts: np.ndarray, cam: CameraCalibration) -> np.ndarray:\n",
        "    T_device_cam: sophus.SE3 = cam.get_transform_device_camera()\n",
        "    pts_cam = T_device_cam.inverse() @ pts\n",
        "    return pts_cam\n",
        "\n",
        "\n",
        "def compute_hands_with_wrist_location(\n",
        "    wrist_per_hand_device: np.ndarray,\n",
        "    det_res: vision.HandLandmarkerResult,\n",
        "    cam: CameraCalibration,\n",
        "):\n",
        "    T_cam_device: sophus.SE3 = cam.get_transform_device_camera().inverse()\n",
        "    wrist_per_hand_cam = T_cam_device @ wrist_per_hand_device\n",
        "    wrist_depth = wrist_per_hand_cam.T[:, 2]  # for two hands\n",
        "    pts_per_hand = convert_mp_det_to_cam_landmarks(det_res, wrist_depth, cam)\n",
        "\n",
        "    return pts_per_hand\n",
        "\n",
        "\n",
        "def rr_show_hands_in_cam(\n",
        "    wrist_per_hand_device: np.ndarray,\n",
        "    img: np.ndarray,\n",
        "    det_res: vision.HandLandmarkerResult,\n",
        "    cam: CameraCalibration,\n",
        "    cam_label: str,\n",
        "    hide_hands_w_negative_depth: bool,\n",
        "    verbose: bool = False,\n",
        "):\n",
        "    # Convert hand landmarks into camera space with the given wrist positions\n",
        "    pts_per_hand = compute_hands_with_wrist_location(\n",
        "        wrist_per_hand_device=wrist_per_hand_device, det_res=det_res, cam=cam\n",
        "    )\n",
        "\n",
        "    pts_per_hand_vis = pts_per_hand\n",
        "    if hide_hands_w_negative_depth:\n",
        "        for hi, pts in enumerate(pts_per_hand):\n",
        "            if pts is None:\n",
        "                continue\n",
        "            negative_mask = pts[:, 2] <= 0.0\n",
        "            if negative_mask.any():\n",
        "                pts_per_hand_vis[hi] = None\n",
        "                if verbose:\n",
        "                    print(f\"Hide hand {hi}\")\n",
        "\n",
        "    orig_im = img\n",
        "    if len(img.shape) == 2:\n",
        "        # ensure it's RGB\n",
        "        orig_im = cv2.cvtColor(np.ascontiguousarray(img), cv2.COLOR_GRAY2RGB)\n",
        "    img_w_hand = draw_landmarks_on_image(orig_im, det_res)\n",
        "\n",
        "    hands_color = [\n",
        "        # Left: green\n",
        "        [0, 128, 0],\n",
        "        # Right: blue\n",
        "        [0, 0, 128],\n",
        "    ]\n",
        "    for hi, pts in enumerate(pts_per_hand_vis):\n",
        "        hand_label = f\"world/hand_{hi}/joints\"\n",
        "        if pts is not None:\n",
        "            show_hand_skeleton(\n",
        "                pts,\n",
        "                connections=solutions.hands.HAND_CONNECTIONS,\n",
        "                label=hand_label,\n",
        "                color_int8=hands_color[hi],\n",
        "            )\n",
        "    rr_device_label = f\"world/{cam_label}\"\n",
        "    cx, cy = cam.get_principal_point()\n",
        "    w, h = cam.get_image_size()\n",
        "    intrinsics = Intrinsics(\n",
        "        focal=float(cam.get_focal_lengths()[0]), cx=cx, cy=cy, w=w, h=h\n",
        "    )\n",
        "    # show camera frustum\n",
        "    log_cam(sophus.SE3(), intrinsics=intrinsics, label=rr_device_label)\n",
        "    # show image overlayed with hand\n",
        "    rr.log(rr_device_label, rr.Image(img_w_hand))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd002c18-74cd-4df6-8490-780b074243a1",
      "metadata": {},
      "source": [
        "Here is the function that implements our algorithm:\n",
        "1. We detect 2.5D hand landmarks in RGB image using MediaPipe. Note the depth is relative to the wrist, along the principal axis of the camera\n",
        "2. Shift hand landmarks altogether s.t. wrist is located at the MPS wrist location in the camera space\n",
        "3. Unproject each landmark in the image to the depth found in above step, leading to hand 3d landmarks in the camera space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "913095d6-a392-4d40-ba9c-f53faf442aca",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_mp_ht_on_vrs(\n",
        "    vrs_path: str,\n",
        "    cam_label: str,\n",
        "    mps_wrist_csv_path: str,\n",
        "    start_frame: Optional[int] = None,\n",
        "    max_frames: Optional[int] = None,\n",
        "):\n",
        "    vrs_provider: data_provider.VrsDataProvider = (\n",
        "        data_provider.create_vrs_data_provider(vrs_path)\n",
        "    )\n",
        "    assert vrs_provider is not None, f\"Cannot open vrs file {vrs_path}\"\n",
        "\n",
        "    wrist_and_palm_poses = mps.hand_tracking.read_wrist_and_palm_poses(\n",
        "        mps_wrist_csv_path\n",
        "    )\n",
        "\n",
        "    stream_id = vrs_provider.get_stream_id_from_label(cam_label)\n",
        "    timestamps_ns = vrs_provider.get_timestamps_ns(stream_id, TimeDomain.DEVICE_TIME)\n",
        "    cam = get_camera_calibration(vrs_provider, cam_label)\n",
        "    detector = create_ht_detector()\n",
        "    # print(timestamps_ns)\n",
        "\n",
        "    if start_frame is None or start_frame <= 0:\n",
        "        start_frame = 0\n",
        "    if max_frames is None or max_frames <= 0:\n",
        "        max_frames = len(timestamps_ns)\n",
        "    end_frame = min(start_frame + max_frames, len(timestamps_ns))\n",
        "    for t_ns in tqdm(timestamps_ns[start_frame:end_frame]):\n",
        "        rr.set_time_nanos(\"synchronization_time\", int(t_ns))\n",
        "        rr.set_time_sequence(\"timestamp\", t_ns)\n",
        "\n",
        "        img_data: sensor_data.ImageData = vrs_provider.get_image_data_by_time_ns(\n",
        "            stream_id, t_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n",
        "        )[0]\n",
        "        img = img_data.to_numpy_array()\n",
        "        det_res: vision.HandLandmarkerResult = run_mediapipe_ht(detector, img=img)\n",
        "\n",
        "        # get wrist location from MPS HT results\n",
        "        wrist_and_palm_pose = get_nearest_wrist_and_palm_pose(\n",
        "            wrist_and_palm_poses, query_timestamp_ns=t_ns\n",
        "        )\n",
        "        wrist_per_hand = [\n",
        "            (\n",
        "                wrist_and_palm_pose.left_hand.wrist_position_device\n",
        "                if wrist_and_palm_pose.left_hand.confidence > 0.0\n",
        "                else None\n",
        "            ),\n",
        "            (\n",
        "                wrist_and_palm_pose.right_hand.wrist_position_device\n",
        "                if wrist_and_palm_pose.right_hand.confidence > 0.0\n",
        "                else None\n",
        "            ),\n",
        "        ]\n",
        "        wrist_device_array = np.zeros([3, 2])\n",
        "        for hi, wrist in enumerate(wrist_per_hand):\n",
        "            if wrist is None:\n",
        "                continue\n",
        "            wrist_device_array[:, hi] = wrist\n",
        "\n",
        "        # visualize everything for this frame\n",
        "        for hi in range(2):\n",
        "            hand_label = f\"world/hand_{hi}/joints\"\n",
        "            rr.log(hand_label, rr.Clear(recursive=True))\n",
        "        rr_show_hands_in_cam(\n",
        "            wrist_device_array,\n",
        "            img=img,\n",
        "            det_res=det_res,\n",
        "            cam=cam,\n",
        "            cam_label=cam_label,\n",
        "            hide_hands_w_negative_depth=True,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "079caa30-f862-4bf6-9f0c-b937a04c9a7c",
      "metadata": {},
      "source": [
        "### Run hand tracking on the given VRS for certain frames"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f22d2e18-9a3c-4542-9b90-d4d77f645fc6",
      "metadata": {},
      "source": [
        "The following code runs the algorithm for each frame. 3D hand landmarks and RGB frame will be visualized in Rerun window.\n",
        "\n",
        "Note it's possible sometimes MediaPipe detection is shown in image, but no corresponding 3D hands are visualized due to some joints are close \n",
        "to the vignette boundary of the fisheye image, leading to invalid unprojection.\n",
        "\n",
        "Also as a minor point, because the camera is visualized using a pinhole model which is different than the fisheye camera model for the image, \n",
        "the mouse hover in the image does not match accurately with the 3d ray automatically visualized by Rerun and can only serve for illustration purpose at best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b530518a-a10f-46b9-97b5-03b59e9df5ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Spawn rr visualization window\n",
        "rr.init(\"Hand Tracking on VRS\", spawn=True)\n",
        "\n",
        "run_mp_ht_on_vrs(\n",
        "    vrs_path=vrs_path,\n",
        "    cam_label=\"camera-rgb\",\n",
        "    mps_wrist_csv_path=mps_wrist_csv_path,\n",
        "    # Set start_frame=0 to start from the beginning of VRS\n",
        "    start_frame=sample_frame_index,\n",
        "    # Set max_frames=-1 to use all frames\n",
        "    max_frames=100,\n",
        ")\n",
        "\n",
        "# You can also show rr in notebook, but could be slow to load. In that case: `spawn=False`\n",
        "# rr.notebook_show(timeout_ms=\"100000\")"
      ]
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "42544ec3-cc99-4a55-9455-1c3818f12668",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "aria_ml",
      "language": "python",
      "name": "aria_ml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  }
}
