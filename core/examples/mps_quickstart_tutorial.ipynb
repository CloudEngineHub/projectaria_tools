{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b9f77ab3",
      "metadata": {
        "id": "b9f77ab3"
      },
      "source": [
        "# MPS Tutorial\n",
        "This sample will show you how to use the Aria MPS data via the MPS apis.\n",
        "Please refer to the MPS wiki for more information about data formats and schemas\n",
        "\n",
        "### Notebook stuck?\n",
        "Note that because of Jupyter and Plotly issues, sometimes the code may stuck at visualization. We recommend **restart the kernels** and try again to see if the issue is resolved.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8145b90-be35-47b7-9926-e40a398c21ca",
      "metadata": {
        "id": "e8145b90-be35-47b7-9926-e40a398c21ca"
      },
      "source": [
        "## Download the MPS sample dataset locally\n",
        "> The sample dataset will get downloaded to a **tmp** folder by default. Please modify the path if necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b30b8598-2bbb-4faa-936e-0c8fee12dafa",
      "metadata": {
        "id": "b30b8598-2bbb-4faa-936e-0c8fee12dafa"
      },
      "outputs": [],
      "source": [
        "import subprocess, os\n",
        "\n",
        "google_colab_env = 'google.colab' in str(get_ipython())\n",
        "if google_colab_env:\n",
        "    print(\"Running from Google Colab, installing projectaria_tools and getting sample data\")\n",
        "    mps_sample_path = \"./mps_sample_data/\"\n",
        "else:\n",
        "    mps_sample_path = \"/tmp/mps_sample_data/\"\n",
        "\n",
        "base_url = \"https://www.projectaria.com/async/sample/download/?bucket=mps&filename=\"\n",
        "options = \"-C - -O -L\"\n",
        "\n",
        "command_list = [\n",
        "    f\"mkdir -p {mps_sample_path}\",\n",
        "    # Getting sample data\n",
        "    f'curl -o {mps_sample_path}/sample.vrs {options} \"{base_url}sample.vrs\"',\n",
        "    f'curl -o {mps_sample_path}/trajectory.zip {options} \"{base_url}trajectory.zip\"',\n",
        "    f'curl -o {mps_sample_path}/eye_gaze.zip {options} \"{base_url}eye_gaze.zip\"',\n",
        "    # Unzip amd move the MPS output\n",
        "    f\"unzip -o {mps_sample_path}/eye_gaze.zip  -d {mps_sample_path}\",\n",
        "    f\"unzip -o {mps_sample_path}/trajectory.zip -d {mps_sample_path}\",\n",
        "]\n",
        "\n",
        "if google_colab_env:\n",
        "  !pip install projectaria-tools\n",
        "  for command in command_list:\n",
        "    !$command\n",
        "else:\n",
        "  for command in command_list:\n",
        "    subprocess.run(command, shell=True, check=True)\n",
        "\n",
        "# Temporarily rename files to new schema, until we update the dataset on cloud\n",
        "old_to_new = {\n",
        "    f\"trajectory/global_points.csv.gz\": f\"trajectory/semidense_points.csv.gz\",\n",
        "    f\"eye_gaze/generalized_eye_gaze.csv\": f\"eye_gaze/general_eye_gaze.csv\",\n",
        "    f\"eye_gaze/calibrated_eye_gaze.csv\": f\"eye_gaze/personalized_eye_gaze.csv\",\n",
        "}\n",
        "for old, new in old_to_new.items():\n",
        "  if os.path.isfile(os.path.join(mps_sample_path, old)):\n",
        "    os.rename(os.path.join(mps_sample_path, old), os.path.join(mps_sample_path, new))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ef125ae",
      "metadata": {
        "id": "5ef125ae"
      },
      "source": [
        "## Load the trajectory, point cloud and eye gaze using the MPS apis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4a6f498",
      "metadata": {
        "id": "e4a6f498"
      },
      "outputs": [],
      "source": [
        "from projectaria_tools.core import data_provider, mps\n",
        "from projectaria_tools.core.stream_id import StreamId\n",
        "import numpy as np\n",
        "\n",
        "# If necessary, set path to your own dataset and vrs file here\n",
        "# mps_sample_path=\"/content/drive/MyDrive/MPS/sample_data\"\n",
        "# Load the VRS file\n",
        "vrsfile = os.path.join(mps_sample_path, \"sample.vrs\")\n",
        "\n",
        "# Trajectory and global points\n",
        "closed_loop_trajectory = os.path.join(mps_sample_path, \"trajectory\",\"closed_loop_trajectory.csv\")\n",
        "semidense_points = os.path.join(mps_sample_path, \"trajectory\", \"semidense_points.csv.gz\")\n",
        "\n",
        "# Eye Gaze\n",
        "general_eye_gaze_path = os.path.join(mps_sample_path, \"eye_gaze\", \"general_eye_gaze.csv\")\n",
        "personalized_eye_gaze_path = os.path.join(mps_sample_path, \"eye_gaze\", \"personalized_eye_gaze.csv\")\n",
        "personalize_gaze_available = os.path.isfile(personalized_eye_gaze_path)\n",
        "\n",
        "# Create data provider and get T_device_rgb\n",
        "provider = data_provider.create_vrs_data_provider(vrsfile)\n",
        "# Since we want to display the position of the RGB camera, we are querying its relative location\n",
        "# from the device and will apply it to the device trajectory.\n",
        "T_device_RGB = provider.get_device_calibration().get_transform_device_sensor(\"camera-rgb\")\n",
        "\n",
        "## Load trajectory and global points\n",
        "mps_trajectory = mps.read_closed_loop_trajectory(closed_loop_trajectory)\n",
        "points = mps.read_global_point_cloud(semidense_points, mps.StreamCompressionMode.GZIP)\n",
        "\n",
        "## Load eyegaze\n",
        "general_eye_gazes = mps.read_eyegaze(general_eye_gaze_path)\n",
        "if personalize_gaze_available:\n",
        "    personalized_eye_gazes = mps.read_eyegaze(personalized_eye_gaze_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98903e99",
      "metadata": {
        "id": "98903e99"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e4c81d7",
      "metadata": {
        "id": "9e4c81d7"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objs as go\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Helper function to build the frustum\n",
        "def build_cam_frustum(transform_world_device):\n",
        "    points = np.array([[0, 0, 0], [0.5, 0.5, 1], [-0.5, 0.5, 1], [-0.5, -0.5, 1], [0.5, -0.5, 1]]) * 0.6\n",
        "    transform_world_rgb = transform_world_device @ T_device_RGB\n",
        "    points_transformed = transform_world_rgb @ points.transpose()\n",
        "    return go.Mesh3d(x=points_transformed[0,:], y=points_transformed[1, :], z=points_transformed[2, :],\n",
        "        i=[0, 0, 0, 0, 1, 1], j=[1, 2, 3, 4, 2, 3], k=[2, 3, 4, 1, 3, 4], showscale=False, visible=False,\n",
        "        colorscale=\"jet\", intensity=points[:, 2], opacity=1.0, hoverinfo='none')\n",
        "\n",
        "# Helper function to get nearest eye gaze for a timestamp\n",
        "def get_nearest_eye_gaze(eye_gazes, query_timestamp_ns):\n",
        "    return eye_gazes[min(range(len(eye_gazes)), key = lambda i: abs(eye_gazes[i].tracking_timestamp.total_seconds()*1e9 - query_timestamp_ns))]\n",
        "\n",
        "# Helper function to project a gaze output onto the RGB image, assuming a fixed depth of 1m\n",
        "def get_gaze_center_in_pixels(eye_gaze):\n",
        "    depth_m = 1.0 # Select a fixed depth of 1m\n",
        "    gaze_center_in_cpf = mps.get_eyegaze_point_at_depth(eye_gaze.yaw, eye_gaze.pitch, depth_m)\n",
        "    transform_cpf_sensor = provider.get_device_calibration().get_transform_cpf_sensor(provider.get_label_from_stream_id(rgb_stream_id))\n",
        "    gaze_center_in_camera = transform_cpf_sensor.inverse() @ gaze_center_in_cpf\n",
        "    gaze_center_in_camera = gaze_center_in_camera / gaze_center_in_camera[2]\n",
        "    gaze_center_in_pixels = cam_calibration.project(gaze_center_in_camera)\n",
        "    return gaze_center_in_pixels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f5d0aa",
      "metadata": {
        "id": "f6f5d0aa"
      },
      "source": [
        "## Visualize the trajectory and point cloud in a 3D interactive plot\n",
        "* Load trajectory\n",
        "* Load global point cloud\n",
        "* Render dense trajectory (1Khz) as points.\n",
        "* Render subsampled 6DOF poses via camera frustum. Use calibration to transform RGB camera pose to world frame\n",
        "* Render subsampled point cloud\n",
        "\n",
        "_Please wait a minute for all the data to load. Zoom in to the point cloud and adjust your view. Then use the time slider to move the camera_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8502d31",
      "metadata": {
        "id": "e8502d31"
      },
      "outputs": [],
      "source": [
        "# Load all world positions from the trajectory\n",
        "traj = np.empty([len(mps_trajectory), 3])\n",
        "for i in range(len(mps_trajectory)):\n",
        "    traj[i, :] = mps_trajectory[i].transform_world_device.translation()\n",
        "\n",
        "# Subsample trajectory for quick display\n",
        "skip = 1000\n",
        "mps_trajectory_subset = mps_trajectory[::skip]\n",
        "steps = [None]*len(mps_trajectory_subset)\n",
        "\n",
        "# Load each pose as a camera frustum trace\n",
        "cam_frustums = [None]*len(mps_trajectory_subset)\n",
        "\n",
        "for i in range(len(mps_trajectory_subset)):\n",
        "    pose = mps_trajectory_subset[i]\n",
        "    cam_frustums[i] = build_cam_frustum(pose.transform_world_device)\n",
        "    timestamp = pose.tracking_timestamp.total_seconds()\n",
        "    step = dict(method=\"update\", args=[{\"visible\": [False] * len(cam_frustums) + [True] * 2}, {\"title\": \"Trajectory and Point Cloud\"},], label=timestamp,)\n",
        "    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n",
        "    steps[i] = step\n",
        "cam_frustums[0].visible = True\n",
        "\n",
        "# Filter the point cloud by inv depth and depth and load\n",
        "threshold_invdep = 0.001\n",
        "threshold_dep = 0.15\n",
        "skip = 1\n",
        "point_cloud = np.empty([len(points), 3])\n",
        "j = 0\n",
        "for i in range(0, len(points), skip):\n",
        "    if (points[i].inverse_distance_std < threshold_invdep and points[i].distance_std < threshold_dep):\n",
        "        point_cloud[j,:] = points[i].position_world\n",
        "        j=j+1\n",
        "point_cloud = point_cloud[:j,:]\n",
        "\n",
        "# Create slider to allow scrubbing and set the layout\n",
        "sliders = [dict(currentvalue={\"suffix\": \" s\", \"prefix\": \"Time :\"}, pad={\"t\": 5}, steps=steps,)]\n",
        "layout = go.Layout(sliders=sliders, scene=dict(bgcolor='lightgray', dragmode='orbit', aspectmode='data', xaxis_visible=False, yaxis_visible=False,zaxis_visible=False))\n",
        "\n",
        "# Plot trajectory and point cloud\n",
        "# We color the points by their z coordinate\n",
        "trajectory = go.Scatter3d(x=traj[:, 0], y=traj[:, 1], z=traj[:, 2], mode=\"markers\", marker={\"size\": 2, \"opacity\": 0.8, \"color\": \"red\"}, name=\"Trajectory\", hoverinfo='none')\n",
        "semidense_points = go.Scatter3d(x=point_cloud[:, 0], y=point_cloud[:, 1], z=point_cloud[:, 2], mode=\"markers\",\n",
        "    marker={\"size\" : 1.5, \"color\": point_cloud[:, 2], \"cmin\": -1.5, \"cmax\": 2, \"colorscale\": \"viridis\",},\n",
        "    name=\"Global Points\", hoverinfo='none')\n",
        "\n",
        "# draw\n",
        "plot_figure = go.Figure(data=cam_frustums + [trajectory, semidense_points], layout=layout)\n",
        "plot_figure.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e83cf57b",
      "metadata": {
        "id": "e83cf57b"
      },
      "source": [
        "## Visualize general and personalized eye gaze projection on an rgb image.\n",
        "* Load Eyegaze MPS output\n",
        "* Select a random RGB frame\n",
        "* Find the closest eye gaze data for the RGB frame\n",
        "* Project the eye gaze for the RGB frame by **using a fixed depth of 1m**.\n",
        "* Show the gaze cross on the RGB image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db594f26",
      "metadata": {
        "id": "db594f26"
      },
      "outputs": [],
      "source": [
        "rgb_stream_id = StreamId(\"214-1\")\n",
        "num_rgb_frames = provider.get_num_data(rgb_stream_id)\n",
        "rgb_frame = provider.get_image_data_by_index(rgb_stream_id, (int)(num_rgb_frames-5))\n",
        "assert rgb_frame[0] is not None, \"no rgb frame\"\n",
        "\n",
        "image = rgb_frame[0].to_numpy_array()\n",
        "capture_timestamp_ns = rgb_frame[1].capture_timestamp_ns\n",
        "general_eye_gaze = get_nearest_eye_gaze(general_eye_gazes, capture_timestamp_ns)\n",
        "if personalize_gaze_available:\n",
        "    personalized_eye_gaze = get_nearest_eye_gaze(personalized_eye_gazes, capture_timestamp_ns)\n",
        "# get projection function\n",
        "device_calibration = provider.get_device_calibration()\n",
        "cam_calibration = device_calibration.get_camera_calib(provider.get_label_from_stream_id(rgb_stream_id))\n",
        "assert cam_calibration is not None, \"no camera calibration\"\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 10))\n",
        "\n",
        "# Draw a cross at the projected gaze center location on the RGB image at depth 1m\n",
        "general_gaze_center_in_pixels = get_gaze_center_in_pixels(general_eye_gaze)\n",
        "if general_gaze_center_in_pixels is not None:\n",
        "    ax1.imshow(image)\n",
        "    ax1.plot(general_gaze_center_in_pixels[0], general_gaze_center_in_pixels[1], '+', c=\"red\", mew=1, ms=20)\n",
        "    ax1.set_title(\"Generalized Eye Gaze\")\n",
        "else:\n",
        "    print(f\"Eye gaze center projected to {general_gaze_center_in_pixels}, which is out of camera sensor plane.\")\n",
        "\n",
        "if personalize_gaze_available:\n",
        "    personalized_gaze_center_in_pixels = get_gaze_center_in_pixels(personalized_eye_gaze)\n",
        "    if personalized_gaze_center_in_pixels is not None:\n",
        "        ax2.imshow(image)\n",
        "        ax2.plot(personalized_gaze_center_in_pixels[0], personalized_gaze_center_in_pixels[1], '+', c=\"red\", mew=1, ms=20)\n",
        "        ax2.set_title(\"Personalized Eye Gaze\")\n",
        "    else:\n",
        "        print(f\"Eye gaze center projected to {personalized_gaze_center_in_pixels}, which is out of camera sensor plane.\")\n",
        "else:\n",
        "    print(\"Personalized gaze not available for this dataset\\n\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
