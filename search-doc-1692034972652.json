[{"title":"Aria Research Kit","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/about_ARK","content":"","keywords":""},{"title":"Hardware Requirements​","type":1,"pageTitle":"Aria Research Kit","url":"/projectaria_tools/docs/ARK/about_ARK#hardware-requirements","content":"To be able to use Aria glasses you will need to use the Mobile Companion app Mobile Companion App: Android device - 64-bit is preferred, although 32-bit is supportedAndroid OS version 10 or above installedARM64 processor preferredARCore Depth API (https://developers.google.com/ar/devices) support needed for eye-tracking calibration To be able to request Machine Perception Services or stream data, you will need to use the Desktop app. Desktop App runs on the following platforms: MacOS Big Sur 11.3+, both Intel and Apple SiliconWindows 10 x86_64 with DirectX (or OpenGL but the latest drivers should be installed)Windows 11 may work, but is not actively supported at this timeLinux, as a debian package for Ubuntu, 22.04 LTS version. The app was only tested for that specific version under Gnome 42.5 and X11 (X Server) as well as Wayland. Any other debian distribution (Ubuntu 22.04 fork such as Kubuntu, Mint etc.) or environment may or may not work. "},{"title":"Quickstart","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/ARK_quickstart","content":"","keywords":""},{"title":"Get set up​","type":1,"pageTitle":"Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#get-set-up","content":"Once you've been approved and can receive your Aria glasses you will get two emails: Welcome to Aria Contains your account details, use these for signing into the Mobile and Desktop Companion apps Join [Person] in Project Aria Academic Partner Announcements, Feedback &amp; Support Follow the prompts to join Aria's research community space where researchers and engineers at Meta and all our Academic partners can connect, ask questions, share ideas and provide support.See How to join the Academic Partner Workplace group for further instructions. "},{"title":"Get connected​","type":1,"pageTitle":"Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#get-connected","content":"Before your glasses arrive we encourage you to join the Academic Partners Workplace group. It's a great place to get the latest announcements, provide feedback, ask questions. Unboxing videos are very welcome! "},{"title":"Get to know your glasses​","type":1,"pageTitle":"Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#get-to-know-your-glasses","content":"About the Aria Research Kit Includes hardware requirements for using ARK apps Technical Specifications Go to the Tech Spec part of the wiki to find out about Aria capabilities, recording profiles etc Glasses Manual Provides information about Project Aria glasses buttons, powering on and off, the privacy switch, how to do a factory reset, LED states, etc   "},{"title":"Update your glasses​","type":1,"pageTitle":"Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#update-your-glasses","content":"You'll need to update your Aria glasses using the Mobile companion app before you can use them. "},{"title":"Install the Mobile companion app​","type":1,"pageTitle":"Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#install-the-mobile-companion-app","content":"On your Android phone, go to portal.projectaria.comSelect the menu bar on the top right of the screen to sign into your accountScroll down to Aria for Android You'll only be able to see the Android app link if you are browsing from an Android device Select Aria for Android and the app will start downloading If your phone only accepts 32-bit apps, please reach out to AriaOps@meta.com for the 32-bit .apk. "},{"title":"Pair your glasses​","type":1,"pageTitle":"Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#pair-your-glasses","content":"Plug your glasses into power using the provided cable.Sign into the app using your provided user name and passwordFollow the prompts to pair your glasses Your glasses are automatically updated when you first pair themGo to the Mobile app page for further information Further updates will be queued automatically when you use your glasses via the Mobile Companion App. If you need to manually check for updates, select the gear icon (settings) next to your glasses and then select Check for OS Updates. "},{"title":"Set your Default Recording Profile​","type":1,"pageTitle":"Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#set-your-default-recording-profile","content":"You can set the default recording profile via Mobile or Desktop Companion App. This is the recording profile used when you initiate recording via the capture button or Desktop app. When you initiate recording via the Mobile Companion app you can select the recording profile per instance and examine the sensors used with each recording profile. In the Mobile Companion App, you can change the Default Recording Profile at any time via Device Settings (select the settings/gear icon next to your glasses on the dashboard). "},{"title":"Install the Desktop app (Optional)​","type":1,"pageTitle":"Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#install-the-desktop-app-optional","content":"The Desktop app is required for requesting Machine Perception Services and streaming data, but you don't need the Desktop app to make recordings or download data. You can also visualize data in the Desktop app. Go to the Desktop app page for more detailed instructions. Go to portal.projectaria.comSelect the menu bar on the top right of the screen to sign into your accountScroll down to Aria for Desktop It will display for MacOS, Windows or Linux, depending on what sort of operating system you're running Select Aria for Mac/Windows to download and installUse the username and password provided in your welcome email to sign into the app. "},{"title":"Record Data​","type":1,"pageTitle":"Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#record-data","content":"It's best to initiate recording using the Mobile Companion app or by pressing the Capture button. Make sure you don't use the Privacy Switch to finish a recording, as this will delete your data. info The longer your glasses record for, the longer it takes a recording to stop. This is because the larger the VRS file, the more time it takes for a recording to finish indexing. The recording has not fully stopped until the Recording LEDs have turned off. "},{"title":"Press the Capture button​","type":1,"pageTitle":"Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#press-the-capture-button","content":"The Capture button is located on the top right of your glasses. The capture button will use the recording profile set by the Desktop App. Once you've pressed the capture button, please allow a few seconds before the recording starts. You'll know recording has started when the Recording LEDs turn on. Press the same button to stop and save the recording. tip Engaging the privacy switch will stop the recording and discard your recording. Discarded recordings cannot be retrieved. "},{"title":"Mobile Companion app​","type":1,"pageTitle":"Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#mobile-companion-app","content":"Select New Recording Session in the Aria Dashboard to start recording. Go to the Mobile Companion App page for more information You can alter the Name and Notes of a recording by going to the Recordings menu, selecting a recording and then selecting Edit. Please note, this does not change the name of the VRS file, it changes the Name and Notes properties in the associated vrs.json file. "},{"title":"Download data​","type":1,"pageTitle":"Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#download-data","content":"Aria recordings are stored as a VRS file and an accompanying JSON file that includes the recording's metadata. Metadata includes: The name and description of the recording as shown in the Mobile Companion AppThe recording profile usedWhat version of the Mobile Companion App was used to create the recording Aria recordings are directly accessible from the glasses' SDCARD. There are three ways you can download data to your local machine using: MTPADB commandsAria Desktop Companion App We recommend using MTP or ADB commands for a faster download experience. "},{"title":"Use MTP via File Explorer​","type":1,"pageTitle":"Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#use-mtp-via-file-explorer","content":"Windows​ When you plug in your Aria glasses to your computer, you can navigate to it as if it were any other USB external storage device. Plug your Aria glasses into your computer, using the supplied cable Please allow a few minutes for your glasses to be detected Your glasses will appear in your directory as an external drive called “Aria”Select Aria and then go to Internal Storage &gt; recordingIn this folder you will see the .vrs file and .json file that stores the .vrs files' metadata You'll also see a thumbnails folder, which contains the thumbnails that are used to provide previews of your content in the Mobile app Copy the data to local storage MacOS​ MTP is not provided natively on MacOS, but there are lightweight tools that you can use, such as Android File Transfer. Download and install Android File Transfer https://www.android.com/filetransfer/Connect your Aria glasses to your computerDrag &amp; drop files "},{"title":"Use ADB​","type":1,"pageTitle":"Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#use-adb","content":"Android Debug Bridge (adb) is a command line tool that can be used with Aria glasses. To download all your data: adb pull /sdcard/recording /home/unixname/MyVRSFolder  To download a single VRS file adb pull /sdcard/recording/myVrsFile.vrs /home/unixname/MyVRSFolder/  To download a single metadata file adb pull /sdcard/recording/myVrsFile.json /home/unixname/MyVRSFolder/  "},{"title":"Use the Desktop App​","type":1,"pageTitle":"Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#use-the-desktop-app","content":"Go to the Recordings section in the Desktop app to download your data. See the Desktop App page for further information. "},{"title":"Streaming​","type":1,"pageTitle":"Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#streaming","content":"Using the Desktop app, Aria Sensor data can be live streamed via Wi-Fi. We recommend only using Profiles 12 and 18, which are optimized for streaming. Go to the Desktop app page for detailed instructions. "},{"title":"Project Aria Frame Sizes","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/frame_sizing","content":"","keywords":""},{"title":"How to find out your Aria Frame Size​","type":1,"pageTitle":"Project Aria Frame Sizes","url":"/projectaria_tools/docs/ARK/frame_sizing#how-to-find-out-your-aria-frame-size","content":""},{"title":"Option 1 - Order Head Sizing Kit​","type":1,"pageTitle":"Project Aria Frame Sizes","url":"/projectaria_tools/docs/ARK/frame_sizing#option-1---order-head-sizing-kit","content":"Ask your Project Aria contact to order you a Head Sizing kit. "},{"title":"Option 2 - DIY Head Sizing​","type":1,"pageTitle":"Project Aria Frame Sizes","url":"/projectaria_tools/docs/ARK/frame_sizing#option-2---diy-head-sizing","content":"Place your head snugly between two objects and then measure the difference.   If it's under 152mm, order a Small. If it's 152mm or more order a Large "},{"title":"Option 3 - Check the frame size of existing glasses/sunglasses​","type":1,"pageTitle":"Project Aria Frame Sizes","url":"/projectaria_tools/docs/ARK/frame_sizing#option-3---check-the-frame-size-of-existing-glassessunglasses","content":"This may be less accurate than other methods, given normal glasses have stiffer arms that don't fit snugly around your head. But it can get you going faster if you don't want to do DIY Frame Sizing. Find a pair of glasses or sunglasses that fit you comfortablyMeasure the distance between the two armsIf it's under 152mm, order a Small. If it's 152mm or more order a Large "},{"title":"Aria Desktop Companion App","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/desktop_companion_app","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#overview","content":"The Aria Desktop Companion App, provides the ability to record, transfer, process, validate and visualize Aria's data through a desktop interface. These instructions are only useful if you have Project Aria glasses. "},{"title":"Features​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#features","content":"Handle and select between multiple plugged in Aria glassesSet the default recording profile for recordings initiated by the capture button or Desktop appSelect a recording profile and visualize a live stream of Aria's sensors according to that recording profileDisplay, extract, validate VRS recordings' internal data layoutAccess Eye Gaze and Trajectory Machine Perception Services (MPS) to process Aria VRS dataVisualize the generated ground-truth to validate itDirect access to documentation and guides "},{"title":"Before you start - Device OS Update​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#before-you-start---device-os-update","content":"caution Before using the Desktop app, you must use the Aria Mobile Companion App to update your Aria glasses' OS. "},{"title":"To install​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#to-install","content":""},{"title":"Aria For MacOS Installer (Intel & Apple Silicon)​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#aria-for-macos-installer-intel--apple-silicon","content":"Go to portal.projectaria.comSelect the menu bar on the top right of the screen to sign into your accountScroll down to Aria for Desktop It will display for MacOS, Windows or Linux, depending on what sort of operating system you're running Download Aria's DMG installer from portal.projectaria.comOpen the downloaded fileDrag and drop Aria.app to your Applications folderYou should now be able to open up the app by double clicking on Aria "},{"title":"Aria For Windows Installer​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#aria-for-windows-installer","content":"Go to portal.projectaria.comSelect the menu bar on the top right of the screen to sign into your accountScroll down to Aria for Desktop It will display for MacOS, Windows or Linux, depending on what sort of operating system you're running Select Aria for Mac/Windows to download and installUse the username and password provided in your welcome email to sign into the app.Open the installer ad follow the instructions for the Aria Setup Wizard Sign the End-User License AgreementSelect a Destination folder Once the Aria Setup Wizard is complete you should see Aria as a shortcut on your desktop and under the Start Menu "},{"title":"Aria For Linux Installer​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#aria-for-linux-installer","content":"Aria for Linux has only been tested on Ubuntu 22.04 LTS version. It is important to note the app was only tested for that specific version under Gnome 42.5 and X11 (X Server) as well as Wayland. Any other debian-based distribution (Ubuntu 22.04 fork such as Kubuntu, Mint etc..) or other environment may or may not work. Go to portal.projectaria.comSelect the menu bar on the top right of the screen to sign into your accountScroll down to Aria for Desktop It will display for MacOS, Windows or Linux, depending on what sort of operating system you're running Download Aria's Linux debian packageRight-click on the Deb file and select “Open With Other Application“.Select “Software Install” and then “Select” at the top-right.The Software Center will open up on the screen with information on the app you are installing.Select Install The app will then be visible through &quot;Activities&quot; "},{"title":"Login​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#login","content":"Read the Project Aria Research Community Guidelines and select AcceptLog in using the credentials that were sent to you (the same used to log into portal.projectaria.com). "},{"title":"Dashboard​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#dashboard","content":"After accepting the guidelines and logging in, you should see the Aria Dashboard. The dashboard displays the device info for your Aria glasses and allow you to interact with your glasses, if they are plugged in, as well as links to further information about Aria.  "},{"title":"Pairing​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#pairing","content":"The Desktop App will detect devices that are plugged in via USB. Once plugged in, your device will display as Aria and as Active with its serial number visible in the list of devices next to My Device. If you cannot see your device, make sure you have updated the firmware using the Aria Mobile Companion app "},{"title":"Recording​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#recording","content":"While you can initiate recording via the Desktop app, we recommend using the Mobile Companion app or Capture button. "},{"title":"Download your data​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#download-your-data","content":"Once you've completed recordings with your Aria Glasses you can access and download these recordings using the Desktop Companion app, if you wish. tip While you can download recordings using the Desktop app, we recommend using ADB or MTP. Go to the Quickstart Guide for more information. Every recording will generate a VRS file and a .vrs.json file containing the recording's metadata. To download a recording: Go to the Recordings pageYou'll see thumbnails of all the recordings on your Aria glassesSelect Pull to download the data from your glassesIn the file explorer window, select where you want to save the recordingOnce you've selected a folder location, a dialog window will pop up indicating the current VRS file transfer "},{"title":"Visualize your data​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#visualize-your-data","content":"Once files have been copied to your local directory, the VRS file can be visualized using the Desktop App. In the Recordings page, select ToolsSelect Play VRSWhen the file explorer window appears, select the VRS file you want to playOnce you've selected a file to open, the VRS player window will pop up and start playing your recording Once the visualizer is open, use Open (Path or URI) or Select (Explorer window) to visualize other recordings "},{"title":"Playback Controls​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#playback-controls","content":"Play/pause/stop playback​ The Previous and Next Frame buttons will play at most one frame backward or forward for each visible stream. The Speed controls let you chose to play slower or faster. If there is too much data for your system to process, frames will be dropped. Overlay Selection​ The overlay selector lets you chose what information to display over the frames. The options are: Hide - there is no overlayTags - show stream tagsConfiguration, State or Data - show the metadata found in the last record. "},{"title":"Tooltips​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#tooltips","content":"To know the duration of the image data, use the tooltip found over the time display. Note that the start and end times show the time range in which image or audio data was found. Streams that don't contain image or audio data are ignored, and only data records from image and audio streams are considered. So if a recording contains a single image stream that has a configuration record at timestamp 0 rather than just before the first data record (as is too often the practice), while the first data record is at timestamp 15, the playback start time will be 15. "},{"title":"Menu Bar Commands​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#menu-bar-commands","content":"The Menu Bar offers functionality available only there, don't forget to look there! The Layout menu's top section let you save and manage presets. Save your favorite stream display configurations, including stream orientation, stream order, which streams are visible or hidden, using the Save Preset command. To arrange streams, see the Context Menu section below. The commands in the lower section let you control in how many row of how many views the streams will be arranged. Layout Frames 4x2 means using 2 rows with up to 4 streams each. The layout configurations offered depend on the number of image streams visible. Once at least one preset has been saved, you can recall or delete presets, which automatically get a keyboard shortcut for quick access.  "},{"title":"Keyboard Playback Controls​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#keyboard-playback-controls","content":"Playback can be directly controlled from the keyboard: Use the space bar to play/pause replay.The backspace and the home keys will reset playback to the start of the file, much like the Stop button.The left and right arrow keys will read at most one frame per stream, in either direction.The up and down arrow keys will jump at most 10 frames, in either direction.The page-up and page-down keys will jump at most 100 frames, in either direction. When using the arrow keys, all frames are guaranteed to be read. Use this feature if you want to be sure to view every frame of your file. "},{"title":"Streaming​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#streaming","content":"Aria Sensor data can be live streamed via Wi-Fi. While you can use any recording profile when streaming, we recommend only using Profiles 12 and 18, which are optimized for streaming. Streaming over will not work on any Meta corporate network. To stream data, your Aria glasses need to be on the same network as your computer. You will need to use a &quot;non-corporate&quot; network Corporate/Institution networks are often protected by many layers of security and firewalls which will impede you from streaming. If you are at home, please make sure you're not connected to a VPN. "},{"title":"Wi-Fi connection via Mobile Companion App​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#wi-fi-connection-via-mobile-companion-app","content":"You will have connected your Aria device to Wi-Fi when you paired your device with the Mobile Companion App and updated it. Your computer and Aria device need to be on the same Wi-Fi network If you need to change the Wi-Fi network your Aria is connected to: Open Mobile Companion App In the Paired Glasses section of the Dashboard, select Select Wi-Fi Select your preferred network and follow the prompts to connect You can also forget an existing network from the Wi-Fi menuMake sure it is a non-corporate network that is the same as your computer Once connected, the Wi-Fi network name will appear in the Desktop App under My Device underneath the WiFi icon which will become blue. "},{"title":"Wi-Fi connection via Desktop App​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#wi-fi-connection-via-desktop-app","content":"To connect the device to any available Wi-Fi network, select Wi-Fi on the Dashboard device toolbar under My Device. Make sure it is a non-corporate network that is the same as your computer A list of available networks will be shown.Select your desired network and enter its password. Click on Connect to confirm your selected profile "},{"title":"Start Streaming​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#start-streaming","content":"To start streaming, select Streaming on the Dashboard device toolbar under My Device. The streaming session will then start "},{"title":"Streaming Visualization​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#streaming-visualization","content":"The Aria stream window will pop up showing Aria's sensors live data. The nature of the visualized data will be determined by the chosen recording profile (See Select a Recording Profile above). we recommend only using Profiles 12 and 18, which are optimized for streaming. "},{"title":"Stop Streaming​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#stop-streaming","content":"To stop streaming, select Stop on the same device toolbar Closing the Aria Stream window will not stop the stream. You will need to effectively click on the Stop button for the stream to stop. "},{"title":"Machine Perception Services (MPS)​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#machine-perception-services-mps","content":"The Desktop App exposes a cloud-based web service or MPS allowing you to generate a ground-truth trajectory out of an Aria VRS recording. Go to Access MPS Data for more information. Appendix A - Recording using the Desktop app While we recommend recording using the Capture button or Mobile Companion app, here are the instructions for recording with the Desktop Companion app "},{"title":"Select a Recording Profile​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#select-a-recording-profile","content":"Before you can record using the Desktop app, you'll need to set a recording profile. You can do this via the Desktop or Mobile Companion App. To set the recording profile via Desktop app, select Profile in the device toolbar. Whenever a profile is selected, its description will be shown underneath. Select OK to confirm your selected profile. The profile you select will also be the profile used when you initiate recording using the capture button on your Aria device. "},{"title":"Start Recording​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#start-recording","content":"Plug your Aria device into your computer with the provided cable.In the Dashboard device toolbar under My Device, select Record.Once the recording has started you can unplug your Aria device "},{"title":"Stop Recording​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#stop-recording","content":"Plug your Aria device into your computerSelect Stop "},{"title":"Use Desktop App as CLI​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#use-desktop-app-as-cli","content":"The Desktop App can be run directly from the command line without needing a GUI. It can be found under C:\\Program Files\\Aria\\v3\\AriaHub.exe on Windows and /Applications/Aria.app/Contents/MacOS/AriaHub on Mac. You may print the app's help using the AriaHub --help command. "},{"title":"health​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#health","content":"Usage : AriaHub health vrsFilePath.vrs  Use this to run validity checks on an Aria recording (VRS file). These checks are also run on the VRS file automatically, before the file gets uploaded for MPS processing. The results of those checks can be found under your home directory under ./aria/logs "},{"title":"vrs​","type":1,"pageTitle":"Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#vrs","content":"Usage : AriaHub vrs vrsFilePath.vrs  Use it as a Swiss army knife utility to manipulate VRS files in different ways. Go to the VRS official documentation for a full list of commands "},{"title":"Fit and Comfort","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#overview","content":"There are a few adjustments you can make to your Aria glasses to get a more comfortable fit. "},{"title":"Ideal Fit​","type":1,"pageTitle":"Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#ideal-fit","content":"Proper frame width: The device should match the overall width of your head. When too small, the device will be compressing on the sides of your head creating marks. When too large, the temple arms will feel too loose and not secure.Proper nose pad fit: The device should rest comfortably on your nose without slipping down or causing marks. The Nose pad bracket can be adjusted for proper angle and pad placement.Proper temple arm fit: Temple arms reach all the way behind your ears and flex adjusted to fit. Your frames should align evenly with your eyes, no higher than your eyebrows. For eye-tracking, ensure that your eyes (pupils) are approximately located in between the dotted lines as shown in the picture.  "},{"title":"Nose Pad Adjustments​","type":1,"pageTitle":"Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#nose-pad-adjustments","content":""},{"title":"The nose pads feel too tight or pinching on the nose?​","type":1,"pageTitle":"Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#the-nose-pads-feel-too-tight-or-pinching-on-the-nose","content":"Solution: Move pads further apart Securing the frame eye wire with your hands, use your thumbs to spread the pad apart carefully. Make small incremental adjustments until desired fit is achieved.Using your fingers, you can make small incremental adjustments on the pad angle by moving/turning the pad arm carefully. An ideal fit is achieved by making sure the full surface of the soft nose pad is evenly placed on each side of your nose bridge. "},{"title":"Nose pads are sitting too low on the nose, or feel loose?​","type":1,"pageTitle":"Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#nose-pads-are-sitting-too-low-on-the-nose-or-feel-loose","content":"Solution: Move pads closer to each other Use your thumbs to push the nose pads closer together until they fit snugly against either side of your nose, being careful to not damage the Device. Make small incremental adjustments until desired fit is achieved.Using your fingers, you can make small incremental adjustments on the pad angle by moving/turning the pad arm carefully.An ideal fit is achieved by making sure the full surface of the soft nose pad is evenly placed on each side of your nose bridge. "},{"title":"Flexible Temple Arm Tips​","type":1,"pageTitle":"Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#flexible-temple-arm-tips","content":"IMPORTANT! The flexible temple arm tips are restricted to horizontal inward/outward movement only. DO NOT move them vertically up or down. The ends of Project Aria Device arms are flexible, allowing you to make small adjustments to increase wearing comfort. Using your hands, you can bend the tips inward to make it tighter behind the ears or outward to make it looser. "},{"title":"Ear Hooks​","type":1,"pageTitle":"Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#ear-hooks","content":"You can install Ear Hooks at the end of each temple arm tip, for a more secure and stable grip behind the ears. "},{"title":"Aria Glasses User Manual","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual","content":"","keywords":""},{"title":"Proper Handling and Cleaning​","type":1,"pageTitle":"Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#proper-handling-and-cleaning","content":"Handle the Glasses with care &amp; avoid damage. The Glasses are sensitive electronic equipment and should be handled with care.Do not try to bend the temples arms. Attempting to fold them like normal glasses will damage them.Do not drop, strike, or shake your Glasses excessively. Do not use the Glasses if they are damaged.The Glasses are not intended for use as safety glasses or eye protection.Do not use with other head mounted displays, such as virtual reality headsets or other glasses.Storage. Store the Glasses in a clean, dry, temperature-controlled environment. Do not leave in an unattended vehicle where temperatures may reach hot or cold extremes.Water &amp; liquids. Your Glasses are resistant to water splashes but are not designed for submersion in water or extended exposure to water or other liquids. If water exposure occurs, dry your Glasses thoroughly and clear the charging areas of residue or other debris.Cleaning. Clean using a microfiber cloth or a lens cleaning wipe. Do not soak, rinse, or submerge. "},{"title":"Detailed Cleaning Instructions​","type":1,"pageTitle":"Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#detailed-cleaning-instructions","content":"To sanitize​ Use 70% pre moistened alcohol lens wipes with a light touch. To clean​ Clean the eyeglass lenses with a dry microfiber cloth or with a traditional lens cleaner available at any optical store. Try your best to only use a microfiber cloth for cleaning. Wash the microfiber cloth with soap and water at least weekly and air dry to ensure cleanliness of cloth. Do not use any paper products such as paper towels or Kleenex as it will damage the anti-reflective coating on the lenses. If a microfiber cloth is not available, you can use a 100% soft cotton cloth to wipe your lenses. Be mindful not to get the camera lenses and sensors wet as much as possible. It's best to spray the cloth first (soap+water mixture ok to use too) to dampen then wipe the lenses with it. "},{"title":"Project Aria Glasses​","type":1,"pageTitle":"Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#project-aria-glasses","content":"  "},{"title":"Charging​","type":1,"pageTitle":"Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#charging","content":"Your Aria glasses must be connected to a charger to upload data. It connects to USB via its magnetic connector on the right arm. If you see the LED flashing red on the inside of the right temple arm, your battery is depleted. "},{"title":"Powering On​","type":1,"pageTitle":"Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#powering-on","content":"Connect your glasses to the charger to automatically turn them on. or Hold the power button for 3-5 seconds.Continue holding the button when you see the LED flash green once. You can release the button once you see the LED turn solid blue.  "},{"title":"Powering Off​","type":1,"pageTitle":"Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#powering-off","content":"Hold the power button for 5 seconds and release. Powering off will NOT be instantaneous. It may take several seconds for the LED to turn off. "},{"title":"Privacy Switch​","type":1,"pageTitle":"Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#privacy-switch","content":"The Privacy Switch, located underneath the right arm, toggles privacy mode on/off. Privacy mode is enabled when the switch is toggled toward the crossed out camera (i.e., &quot;back&quot; position relative to the wearer). When the privacy switch is engaged, the Aria glasses cannot record data.Any active recordings will be stopped and deleted when the switch is engaged.  "},{"title":"Capture Button​","type":1,"pageTitle":"Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#capture-button","content":"Located over the right frame.Can be used to start and stop an active recording.The capture and privacy switch are used in combination if you need to factory reset your device. Please note, it may take several seconds for your recording to start or stop. The larger the recording, the longer it will take to finish indexing and stop the recording.  "},{"title":"Proximity Sensor​","type":1,"pageTitle":"Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#proximity-sensor","content":"Senses when the Aria glasses are being worn.  "},{"title":"LED States​","type":1,"pageTitle":"Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#led-states","content":""},{"title":"Right Arm LED​","type":1,"pageTitle":"Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#right-arm-led","content":"Low Battery: Flashing RedRecovery Mode: Alternating Blue and Red (may look violet because of the alternating speed)Powering On: Flashing Green oncePower On: Solid BluePower Off: OffCharge Loop: Solid Red (when the battery is too low to boot successfully)Data Uploading: Pulsing Blue 3 times, repeating this pattern (reminder, your device must be connected to power to upload data)  "},{"title":"User Facing LED​","type":1,"pageTitle":"Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#user-facing-led","content":"Recording in Progress: Solid WhiteThermal Mitigation Reached: Solid Red, may appear Orange (when the Device temperature is too high) "},{"title":"Bystander Facing LED​","type":1,"pageTitle":"Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#bystander-facing-led","content":"Recording in Progress: Solid White  "},{"title":"Power Cycle/Reboot​","type":1,"pageTitle":"Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#power-cyclereboot","content":"Power cycle your device by holding down the power button down for about 10 seconds, until the nearby LED flashes green once and returns to solid blue. "},{"title":"Factory Reset​","type":1,"pageTitle":"Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#factory-reset","content":"caution This will delete any recordings on your glasses. Engage the Privacy Switch (i.e. in the recordings disabled position).Tap (press but do not hold) the power button and the capture button at the same time.The device will reboot. After a while, you will see the LED near the power button flashing purple. "},{"title":"Device ID​","type":1,"pageTitle":"Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#device-id","content":"Aria glasses have a unique device_ID. Unlike the serial number, the device_ID is reset every time the device is a factory reset or the glasses are unpaired. Aria glasses must go through a factory reset before they can be paired to a new user, so the device_ID will always be associated with a single user account (although an individual user may have multiple devices associated with their account). Note: disconnecting your glasses does not trigger a factory reset, only unpairing them so that a new user may use the glasses. The serial number persists and is tied to the device, like any other product. You can find the Device ID for your glasses in the Mobile Companion App Device Settings (select the settings/gear icon next to your glasses on the dashboard). Health and Safety Information In addition to the Health and Safety information provided with your welcome kit, you can also read Aria Glasses health and safety information in the Mobile companion app: Open the Aria companion app on your phoneSelect SettingsSelect Health &amp; Safety "},{"title":"Aria Glasses Recording Profile Guide","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Aria Glasses Recording Profile Guide","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide#overview","content":"This page provides the recording profiles needed for Machine Perception Services(MPS) and an overview of Aria sensor recording profiles that are commonly used when collecting Aria data. Go to the Recording Profiles in Technical Specifications for more detailed information about each profile. Recording profiles enable researchers choosing which sensors to record with as well as what settings to use. Settings options include what camera resolution to use and whether the output is RAW (no encoding) or JPEG (compressed). If you would like more details about any recording profile: Open the Mobile Companion App and select New RecordingSelect Profile and scroll down to see the sensor configuration "},{"title":"Recording Profiles That Support MPS​","type":1,"pageTitle":"Aria Glasses Recording Profile Guide","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide#recording-profiles-that-support-mps","content":"The table below shows which recording profiles have the necessary data to generate Trajectory or Eye Gaze data using MPS. More commonly used profiles are marked with bold text.. Aria Recording Profile\tDescription\tTraj\tETProfile0\tDefault (all sensors)\tYes\tYes Profile2\tRGB and SLAM high frame rate\tYes\tNo Profile4\tRGB high frame rate JPEG with audio\tNo\tNo Profile5\tEye tracking calibration (high res)\tNo\tYes Profile7\tRGB high frame rate RAW with audio\tNo\tNo Profile8\tNoise and Hearing mode\tYes\tYes Profile9\tContextualized AI\tYes\tNo Profile10\tAll sensors enabled with low fps\tYes\tYes Profile12\tStreaming mode with JPEG\tNo\tYes Profile14\tSLAM and ET high frame rate with RGB low frame rate\tYes\tYes Profile15\tAR Replay\tYes\tYes Profile16\tEye tracking high frame rate\tNo\tYes Profile17\tStreaming mode with H.265\tNo\tYes Profile18\tStreaming mode with JPEG and Spatial Audio\tNo\tYes Profile19\tDual capture\tYes\tNo Profile20\tLong Duration\tNo\tNo Profile21\tNoise and hearing mode with high RGB fps (RGB 15fps 2MP, SLAM 15fps VGA, ET 30fps QVGA; Audio on; GPS, Wi-Fi and BLE off)\tNo\tYes "},{"title":"General Guidance​","type":1,"pageTitle":"Aria Glasses Recording Profile Guide","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide#general-guidance","content":"These are some sensor profiles researchers have found useful for particular kinds of research. Check recording profiles on your Mobile Companion App for more details. "},{"title":"If you’re not sure what you want​","type":1,"pageTitle":"Aria Glasses Recording Profile Guide","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide#if-youre-not-sure-what-you-want","content":"Profile 10 is interesting to explore, it gathers data with all sensors and the RGB Camera records at 10 fps. All sensor data is useful for exploring multimodal ML models. If you need high RGB Resolution (2880x2880 rather than 1408x1408), and 1FPS is sufficient shutter speed, use Profile 0. "},{"title":"If you're streaming data​","type":1,"pageTitle":"Aria Glasses Recording Profile Guide","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide#if-youre-streaming-data","content":"While you can use any recording profile when streaming, we recommend only using Profiles 12 and 18, which are optimized for streaming. "},{"title":"If you need a high frame rate​","type":1,"pageTitle":"Aria Glasses Recording Profile Guide","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide#if-you-need-a-high-frame-rate","content":"Use profiles 2, 9 or 15, depending on whether you want EyeTracking or GPS. Profile 2 does not have ET, profile 15 does not have GPS. "},{"title":"If your research focuses on audio​","type":1,"pageTitle":"Aria Glasses Recording Profile Guide","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide#if-your-research-focuses-on-audio","content":"Try profiles 4, 7 (no SLAM) or profile 10. "},{"title":"If your research focuses on EyeTracking(ET)​","type":1,"pageTitle":"Aria Glasses Recording Profile Guide","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide#if-your-research-focuses-on-eyetrackinget","content":"Try profile 5 for high resolution ET recordings at 20fps. "},{"title":"To avoid image pre-processing​","type":1,"pageTitle":"Aria Glasses Recording Profile Guide","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide#to-avoid-image-pre-processing","content":"In situations where you want to use RAW videos and skip the Image Sensor Processor (ISP) as much as possible, profile 7 is helpful. Please note, because profile 7 delivers RAW image files, not JPEGs the data is 8x more costly to store. This profile also uses more energy while recording and may heat up faster than others. "},{"title":"Aria Machine Perception Services (MPS)","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mps","content":"","keywords":""},{"title":"Current MPS offerings​","type":1,"pageTitle":"Aria Machine Perception Services (MPS)","url":"/projectaria_tools/docs/ARK/mps#current-mps-offerings","content":"The following MPS can be requested via the Aria Desktop app for Aria recordings that have been made with a compatible Recording Profile. The Recording Profile Guide provides a quick list of compatible sensor profiles or go to Recording Profiles in Technical Specifications for more granular information about each profile. "},{"title":"6DoF trajectory​","type":1,"pageTitle":"Aria Machine Perception Services (MPS)","url":"/projectaria_tools/docs/ARK/mps#6dof-trajectory","content":"MPS provides two types of high frequency (1kHz) trajectories Open loop trajectory that is a local odometry estimation from visual-inertial odometry (VIO)Closed loop trajectory that is created via batch optimization, using multi-sensors' input (SLAM, IMU, barometer, Wi-Fi and GPS), fully optimized and providing poses in a consistent frame of reference. Request trajectory (location data) in the Desktop app to get these outputs, the recording profile must have SLAM cameras + IMU enabled. "},{"title":"Online sensor calibration​","type":1,"pageTitle":"Aria Machine Perception Services (MPS)","url":"/projectaria_tools/docs/ARK/mps#online-sensor-calibration","content":"The time-varying intrinsic and extrinsic calibrations of cameras and IMUs are estimated at the frequency of SLAM cameras by our multi-sensor state estimation pipeline. Request trajectory (location data) in the Desktop app to get these outputs, the recording profile must have SLAM cameras + IMU enabled. "},{"title":"Semi-dense point cloud​","type":1,"pageTitle":"Aria Machine Perception Services (MPS)","url":"/projectaria_tools/docs/ARK/mps#semi-dense-point-cloud","content":"Semi-dense point cloud data supports researchers who need static scene 3D reconstructions, reliable 2D images tracks or a representative visualization of the environment. In the Desktop app, this can be requested as an addition to trajectory (location) derived data and has the same recording profile requirements. "},{"title":"Eye gaze data​","type":1,"pageTitle":"Aria Machine Perception Services (MPS)","url":"/projectaria_tools/docs/ARK/mps#eye-gaze-data","content":"Eye gaze is the most important indicator of human’s attention, eye gaze direction estimation with uncertainty is provided by MPS. Eye gaze estimation uses the data from the Eye Tracking (ET) cameras. Request Eye Gaze data in the Desktop app to get these outputs, the recording profile must have ET cameras enabled. "},{"title":"About MPS Data Loader APIs​","type":1,"pageTitle":"Aria Machine Perception Services (MPS)","url":"/projectaria_tools/docs/ARK/mps#about-mps-data-loader-apis","content":"Please refer to our MPS data loader APIs (C++ and Python support) to load the MPS outputs into your application. Additionally, the visualization guide shows how to run our rich visualization tools to visualize all the MPS outputs. "},{"title":"Questions & Feedback​","type":1,"pageTitle":"Aria Machine Perception Services (MPS)","url":"/projectaria_tools/docs/ARK/mps#questions--feedback","content":"If you have feedback you'd like to provide about overall trends and experiences or improvement ideas we'd love to hear from you. Please email AriaOps@meta.com or post to the Project Aria Academic Partner Feedback and Support group. "},{"title":"Mobile Companion App","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mobile_companion_app","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#overview","content":"Aria Mobile Companion App, provide the ability to interact &amp; record with your Aria glasses via Bluetooth. This section covers Install, Pair and UpdateRecordingMobile Companion App Screens These instructions are only useful if you have access to Aria glasses. "},{"title":"Mobile Companion App features include:​","type":1,"pageTitle":"Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#mobile-companion-app-features-include","content":"Fully wirelessCheck device status (temperature, GPS, privacy switch etc..)Handle and select between multiple paired Aria devicesUpdate Aria devices to the latest OS buildSelect a recording profile and start recording directly from the mobile appSet default recording profile Install, Pair and Update "},{"title":"Install​","type":1,"pageTitle":"Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#install","content":""},{"title":"Download the App​","type":1,"pageTitle":"Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#download-the-app","content":"On your Android phone, go to portal.projectaria.comSelect the menu bar on the top right of the screen to sign into your accountScroll down to Aria for Android You'll only be able to see the Android app link if you are browsing from an Android device Select Aria for Android and the app will start downloading If your phone only accepts 32-bit apps, please reach out to AriaOps@meta.com for the 32-bit .apk. "},{"title":"Install the App​","type":1,"pageTitle":"Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#install-the-app","content":"Launch the app once it has finished downloading. Installation trouble shooting​ If you see an error message such as “for security, your phone is not allowed to install unknown apps from this source” you’ll need to enable your web browser to install unknown apps.  Go to Settings → Apps &amp; notifications → Advanced → Special app access → Install unknown apps.Select the app (usually your internet browser) that you want to grant permission to install from unknown sources.Select Allow from this source "},{"title":"Sign in and pairing​","type":1,"pageTitle":"Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#sign-in-and-pairing","content":"Plug your Project Aria glasses into their charger This will automatically turn your glasses Make sure the Privacy Switch is not engaged The Privacy Switch should be pushed forwards, towards the lenses Open the Companion app for the first time and sign in using the log in we gave you in your welcome email When launching the Companion App for the first time, you'll need to grant it certain permissions to work correctly, such as location services.Follow the prompts to agree to Project Aria Research Community Guidelines and read the Health and Safety informationSelect Get Started to begin setting up your glasses.The app will begin to look for nearby Project Aria glasses. When a device is discovered, it will be listed at the top of the screen alongside its serial number.After selecting your device, the Companion App will begin pairing with it.Once pairing completes, the app will ask to name the device.Join a Wi-Fi network, your device must be plugged into a charger to complete the setup process.Once connected to Wi-Fi, the device will look for updates and update your glasses' OSOnce you have completed setup, you’ll see the Companion App Dashboard Page "},{"title":"Set Default Recording Profile​","type":1,"pageTitle":"Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#set-default-recording-profile","content":"After updating the Aria device's OS, you should be able to see the Aria Dashboard and a prompt to set your default recording profile. This is the recording profile that will be used if you initiate recording via the Desktop app or the Capture button. When you create a new recording session via the Mobile app, you'll be able to select the recording profile each time. "},{"title":"OS Update​","type":1,"pageTitle":"Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#os-update","content":"To make sure you get the latest device features and bug fixes, you should frequently check for the latest OS updates in Aria device settings. Access Device Settings by selecting the gear next to your Aria device on the main Dashboard of the Mobile app. Your Aria device's OS will also automatically update when it is plugged into power and connected to Wi-Fi. Pairing additional glasses Each pair of Aria glasses can only be paired with a single account. Multiple glasses can be paired with the one account, however. Plug your Project Aria glasses into their charger This will automatically turn your glasses Make sure the Privacy Switch is not engaged The Privacy Switch should be pushed forwards, towards the lenses Select the Add Glasses on the Aria dashboard. The app will then start looking for nearby Aria devices.Follow steps 7-12 in Sign in and PairingSet the default recording profile for your glasses Terminology "},{"title":"Pairing/Unpairing​","type":1,"pageTitle":"Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#pairingunpairing","content":"Pair(ed): connecting a device to the Companion App for the first time.Unpair(ed): the process of unpairing a device from your Companion App. This will delete all data stored locally on the device. "},{"title":"Connecting/Disconnecting​","type":1,"pageTitle":"Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#connectingdisconnecting","content":"Connect(ed): is when the Companion App has active control over the device.Disconnect(ed): is when the Companion App no longer has active control over the device. Recording "},{"title":"To start recording​","type":1,"pageTitle":"Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#to-start-recording","content":"Select New Recording Session on the Aria Dashboard.You have the option to configure your recording session before it begins: Name (optional): This field will define how your recording is named inside the Mobile App, but it does not define the name you'll see via the Desktop App. Name is a value that is provided in the vrs.json file associated with your recording. If this field is left blank, your recording will be listed as &quot;Unnamed Recording&quot; in the Recordings tab of the Mobile App.Notes (optional): Notes are appended to your recordings ID. You can input a short or long text string in this field. Notes is a value that is provided in the vrs.json file associated with your recording.Sensors Used: Select Sensors Used to see details of the recording profile you've selected. From the Sensors Used menu, select Profile to choose different recording profiles to record with. Once you've completed or discarded a recording, you'll return to the New Recording screen, pre-populated with your previous details. "},{"title":"Mobile Companion App Screens​","type":1,"pageTitle":"Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#mobile-companion-app-screens","content":""},{"title":"Recordings menu​","type":1,"pageTitle":"Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#recordings-menu","content":"In the Recordings menu, you'll be able to see recordings that are currently on your Aria device. Please note, the names given to these recordings are metadata listed in the vrs.json file and are not the name given to the VRS file your recordings are in. When you select a recording, you will be able to edit your recording's name and notes (which will be stored in the vrs.json file) and see a range of details including: Recording durationRecording profile usedUp to 10 thumbnail images from your recording "},{"title":"Settings menu​","type":1,"pageTitle":"Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#settings-menu","content":"The Settings menu shows the settings for the Aria app, not your Aria device. You'll be able to see the App version, but not the Aria device version. In the Advanced Settings menu there is the option to Clear Local Data, which can be helpful if you encounter issues that restarting your Mobile App does not resolve. Aria recordings are stored on the glasses, not on the phone, so it will not delete any of your recordings. "},{"title":"Aria device settings​","type":1,"pageTitle":"Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#aria-device-settings","content":"On the main Aria Dashboard in the Mobile app, select the settings (gear) icon next to your Aria device to view your Aria device's settings. The Aria device settings page's contents include: Bluetooth, Wi-Fi, Battery, GPS and Temperature statusOS version and ability to check for OS updatesRemaining storage spaceMAC AddressSerial numberDevice IDCheck Device Mode (it should say Partner)Change the default recording profile "},{"title":"Request MPS","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mps/request_mps","content":"","keywords":""},{"title":"Status Types​","type":1,"pageTitle":"Request MPS","url":"/projectaria_tools/docs/ARK/mps/request_mps#status-types","content":"How long it takes for a file to progress from one state to the next will depend on how big the file is and how many other requests are being processed at that time. Unrequested : This type of derived data was not requestedWaiting : MPS request has been received and is in the queue for processingProcessing : This data is currently being processedComplete : Data Processing is Complete, you can now download your filesError: VRS file did not have the data required for processing, please use a Supported Recording Profile.An error occurred while processing data with a supported recording profile. There are many variables that can impact MPS output. For Trajectory data you can download the summary.json file to get more information. See MPS Trajectory Documentation for more information.If you've checked your recording profile and that is not the issue and you are unable to debug the issue using summary.json, please emailAriaOps@meta.comwith a bug report. Make sure you include the Transaction ID in your report and summary.json if you have one. Screenshots and screen recordings are always welcome. "},{"title":"How to Clear the Desktop App's Cache","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/troubleshooting/clear_cache","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"How to Clear the Desktop App's Cache","url":"/projectaria_tools/docs/ARK/troubleshooting/clear_cache#overview","content":"This page provides information about how to clear the Desktop App's cache. Users may wish to do this if they're encountering issues signing into the app, experiencing instabilities in the app or are directed to do so by User Support to resolve an issue. Please note, clearing the cache will clear all your history in VRS Tools, so you will no longer be able to download previously generated MPS. Use the following commands to clear the cache. "},{"title":"MacOS​","type":1,"pageTitle":"How to Clear the Desktop App's Cache","url":"/projectaria_tools/docs/ARK/troubleshooting/clear_cache#macos","content":"rm -f /Users/unixname/Library/Preferences/com.meta.Aria.plist &amp;&amp; killall -u unixname cfprefsd  "},{"title":"Windows​","type":1,"pageTitle":"How to Clear the Desktop App's Cache","url":"/projectaria_tools/docs/ARK/troubleshooting/clear_cache#windows","content":"Remove-Item -Path 'HKCU:\\SOFTWARE\\Meta\\Aria\\'  Or open the Registry Editor and go to HKEY_CURRENT_USER-&gt;SOFTWARE-&gt;Meta then delete Aria "},{"title":"Linux​","type":1,"pageTitle":"How to Clear the Desktop App's Cache","url":"/projectaria_tools/docs/ARK/troubleshooting/clear_cache#linux","content":"rm -f /home/unixname/.config/Meta/Aria.conf  "},{"title":"How to Capture Logs for the Aria Desktop App","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/troubleshooting/desktop_app_logs","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"How to Capture Logs for the Aria Desktop App","url":"/projectaria_tools/docs/ARK/troubleshooting/desktop_app_logs#overview","content":"This page provides information about how to gather logs from the Aria Desktop app. Logs are not necessary prior to filing a bug report or seeking technical support, but they can be helpful. People are most likely to use this page if they have been asked for log files by Aria User Support (AriaOps@meta.com). You turn on logging when you run the app, you cannot gather logs retrospectively. You will need to use these commands every time you want to run the Desktop app with logging enabled. "},{"title":"Enabling logs​","type":1,"pageTitle":"How to Capture Logs for the Aria Desktop App","url":"/projectaria_tools/docs/ARK/troubleshooting/desktop_app_logs#enabling-logs","content":"On MacOS or Windows, enable logging by closing the Desktop app and then relaunching it via the command line with logging turned on. Logs will be added to aria_output.log, which will be created in your user home directory. If you don't fully quit the app, there is a risk your system may open an existing instance with logging turned off, instead of opening a new instance with logging enabled "},{"title":"MacOS​","type":1,"pageTitle":"How to Capture Logs for the Aria Desktop App","url":"/projectaria_tools/docs/ARK/troubleshooting/desktop_app_logs#macos","content":"Quit the Desktop app, if it is runningOpen your terminal and run: open /Applications/Aria.app --args --log-output The Aria Desktop app should then open with logging enabledTo view your logs, go to your user home folder in Finder (Shift + Command + H) where you will find aria_output.logLogs will continue to be added to this file until you quit the appIf you generate logs at a later time, they will be appended to the end of these logs "},{"title":"Windows​","type":1,"pageTitle":"How to Capture Logs for the Aria Desktop App","url":"/projectaria_tools/docs/ARK/troubleshooting/desktop_app_logs#windows","content":"Quit the Desktop app completely Close the app by right-clicking the Aria logo (it looks like glasses) in the right end side of the taskbar, and selecting &quot;Quit&quot; orOpen Task Manager and end the &quot;AriaHub.exe&quot; running task. Use the run command (Windows + R) or Windows terminal to run: &quot;C:\\Program Files\\Aria\\v3\\AriaHub.exe&quot; --log-output The Aria Desktop app should then open with logging enabled.To view your logs, open File Explorer and go to C:\\Users\\&lt;myusername&gt;. There you will find aria_output.logLogs will continue to be added to this file until you quit the appIf you generate logs at a later time, they will be appended to the end of these logs "},{"title":"Aria Research Kit Release Notes","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sw_release_notes","content":"","keywords":""},{"title":"July Release - new documentation site, Aria Desktop App v37 and Mobile App v120​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#july-release---new-documentation-site-aria-desktop-app-v37-and-mobile-app-v120","content":"7/11/23 Dear Academic Partners, With this July release we’re pleased to announce a new documentation site on GitHub, Desktop Companion App v37, and Aria Mobile Companion App v120. V37 of the Desktop Companion App updates the app to the new Project Aria branding, the ability to run the Desktop app from the command line and users will now be able to request Semi-Dense Point Cloud MPS. Project Aria Tools is our new documentation site on GitHub. Information that used to be at projectaria.com is now in the Aria Research Kit sectionof the site and no login is required to access the documentation. Download the updated companion apps from the Aria Web Portal. If you sign in on your desktop computer, you’ll get the Desktop Companion app. If you sign in on your Android mobile device, you’ll download the Mobile Companion app. "},{"title":"NEW AND UPDATED FEATURES​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-and-updated-features","content":""},{"title":"Documentation now on GitHub​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#documentation-now-on-github","content":"The documentation previously hosted at projectaria.com will now be hosted in Project Aria Tools. Project Aria Tools is the new comprehensive website for all your documentation needs. It contains over 60 pages, covering technical specifications, data formats, data utilities you can use with your data, information and tooling for open data we’ve released and technical insights for deeper dives. The Aria Research Kit section contains information specific to Academic partners who have access to Aria glasses and MPS. We’ve added a few extra pages and updated the documentation for this release, updates include adding: Get the Right Size Glasses - sizing information if you’re ordering glasses (our small is surprisingly large!)How to Join the Academic Partners Workplace GroupAria Glasses User Manual - a combination of old and new information. Proper Handling and Cleaning information is now at the top. There is a lot of new documentation to explore, including: Aria Data Utilities - including new Jupyter notebook tutorialsMore detailed information about recording profiles (scroll the table sideways to see all the columns)More data format information (including coordinate conventions)New Open Datasets with tooling, Aria Digital Twin Dataset and Aria Synthetic Environments Dataset. "},{"title":"Desktop App v37 - Semi-dense point cloud​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#desktop-app-v37---semi-dense-point-cloud","content":"From V37 of the Desktop app onwards, users will be able to request semi-dense point clouds as an addition to trajectory MPS services. The same sensor profile requirements for generating trajectory data apply to semi-dense point cloud. Semi-dense point clouds are used by researchers who need static scene 3D reconstructions, reliable 2D images tracks or a representative visualization of the environment. About Aria Machine Perception Services (MPS)How to Request MPSMPS Output - Semi-Dense Point Cloud "},{"title":"Improved UI & UX​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#improved-ui--ux","content":"The Desktop app has been updated to match Aria’s new brandThe recordings view has been revamped, users will now see a thumbnail for each recording and the upload ID can be copiedThe Desktop app can now be run directly from the command line without needing a GUI. It currently includes two utilities as subcommands: Further documentation about how to run the Desktop app from the command line will be added to the ARK wiki soonhealth: use this to run validity checks on an Aria recording (VRS file) It can be run from the command line as follows: AriaHub health vrsFilePath.vrsThese checks are also run on the VRS file automatically, before the file gets uploaded for MPS processing. The results of those checks can be found under your home directory under ./aria/logs vrs: a Swiss army knife utility to manipulate VRS files in different ways It can be run from the command line as follows: AriaHub vrs vrsFilePath.vrsGo to VRS official documentation for a full list of commands "},{"title":"Mobile App v120 - Quality Screens Feature update​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#mobile-app-v120---quality-screens-feature-update","content":"Some additional features have been added to the quality screen feature that was introduced with v115. You’ll now be able to see timecodes and the overall score percentage for the sensors. "},{"title":"BUG FIXES​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes","content":"Various bug fixes improving the overall usage of the Apps. "},{"title":"PROJECT ARIA LATEST DEVICE OS​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-device-os","content":"The latest Project Aria device OS build 4962591.230.70 was released on June 30, 2023. "},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback","content":"Contact Aria User Support by posting in Project Aria Academic Partner Announcements, Feedback &amp; Support or email AriaOps@meta.com.   "},{"title":"Aria Mobile App v115 is now available​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-mobile-app-v115-is-now-available","content":"Dear Academic Partners, The Aria Mobile App v115 for Android is now available for download from the Aria Web Portal (accessed via your Android internet browser). "},{"title":"NEW & UPDATED FEATURES​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features","content":"Sensor data quality signalsIn-Session personalized eye gaze calibrationDifferentiate paired glasses in the pairing screen When you select Add glasses, the available Aria devices will be split into Paired Glasses and Other Glasses so that it’s easy to tell which glasses are already paired with the app. Sensor Data Quality Signals​ While making recordings, researchers will be able to see whether there are any sensor data quality issues (for example, due to thermal mitigation). To check for any issues, in the Mobile Companion app, view the &quot;Sensor Status&quot; section in the Recording Status Screen. Tap on the row to see full details. If you’ve initiated recording via the Capture button, access the Recording Status by selecting Recording in progress on the Mobile Companion app’s main dashboard. In-Session Personalized Eye Gaze Calibration​ Users will be able to record personalized eye gaze calibrations within an ongoing recording. The eye gaze calibration section of the sequence can be used to improve the eye gaze estimations in the rest of the recording. In the future, Machine Perception Services will be able to consume these recordings and output more accurate gaze information. In the Mobile Companion app, create a new recording using a profile that includes ET and RGB cameras (such as Profile 15 or 25)Once your recording has started, close the recording window Select X on the top left of the screen Go to Device Settings (select the gear next to your glasses)Select Eye Tracking CalibrationConfirm that you’d like to run the during the current recording sessionFollow the prompts to calibrate your glasses Eye Gaze Calibration tips​ Things to Avoid​ ❌ Do not wear a face covering during eye calibration. ❌ Choose an area with ample and even lighting; do not face a bright light, window or reflective surface. ❌ Do not set your phone screen brightness too high compared to your surroundings. ❌ Do not fully extend your arm(s) during eye calibration. Your elbows should be bent so that the phone is roughly 1 ft (30 cm) away from your face. Helpful Tips​ ✅ The phone should be held straight in front of your face, so that you shouldn't look up or down to see the screen. Hold the phone plumb (90 degrees) vertically to the ground. ✅ The &quot;Leveler&quot; stage appears if the position of your phone isn't within specifications for the calibration process. Adjust the phone in front of you and its distance by bending your elbow until the smaller, black circle turns into a green disk with a check mark. ✅ Once the &quot;Leveler&quot; stage is successfully completed, do your best to keep your phone in exactly the same position throughout the full eye calibration process. If your phone is moved to a position no longer suited to calibrate your device, the app will return to the &quot;Leveler&quot; stage. ✅ The eye calibration stages 1 to 10 move your nose towards the direction indicated by the arrow. If you're only following the direction with your gaze without moving your head, the calibration stage will time out and fail. However, you make sure to keep your eyes fixed on the number within the dot the whole time. "},{"title":"BUG FIXES​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-1","content":"Various bug fixes to improve the overall usage of the Companion App. "},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os","content":""},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-1","content":"Contact Aria User Support by posting here in Project Aria Academic Partner Announcements, Feedback &amp; Support or emailing AriaOps@meta.com. "},{"title":"Aria Mobile App v110 is now available​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-mobile-app-v110-is-now-available","content":"Dear Academic Partners,The Aria Mobile App v110 for Android is now available for download from the Aria Web Portal (accessed from your Android internet browser). Here are the updates this new version brings. "},{"title":"NEW & UPDATED FEATURES​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features-1","content":"App icons and splash screens for Android companion apps have been updated to Aria’s new branding. When you update the app it will now look like this!From v110 onwards, users will be more easily able to tell if their Aria device’s OS is out of date and see prompts to update their devices.If the glasses are significantly (currently set as 2 months) out of date, the app will disable recording until they are updated.As before, glasses automatically update when connected to power and Wi-Fi.Users will get an in-app prompt to update the app if the app build is over 8 weeks old. "},{"title":"BUG FIXES​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-2","content":"Fixed a bug that occasionally prevented partner mode glasses from being set up for over-the-air software updates properly. Please double-check that your glasses are able to update to a recent build (April 2023 or later). If your glasses are not updating, please reach out to the Aria team.After glasses are unpaired, the app prevents re-pairing with the glasses until they finish rebooting.Other minor bug fixes "},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-1","content":"The latest Project Aria OS build was released on May 2, 2023. "},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-2","content":"Contact Aria User Support by posting here in Project Aria Academic Partner Announcements, Feedback &amp; Support or emailing AriaOps@meta.com. "},{"title":"May 3, 2023 Aria Mobile App v110 is now available​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#may-3-2023-aria-mobile-app-v110-is-now-available","content":"Dear Academic Partners, The Aria Mobile App v110 for Android is now available for download from theAria Web Portal (accessed from your Android internet browser). Here are the updates this new version brings. "},{"title":"NEW & UPDATED FEATURES​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features-2","content":"App icons and splash screens for Android companion apps have been updated to Aria's new branding.From v110 onwards, users will be more easily able to tell if their Aria device's OS is out of date and see prompts to update their devices.If the glasses are significantly (currently set as 2 months) out of date, the app will disable recording until they are updated.As before, glasses automatically update when connected to power and Wi-Fi.Users will get an in-app prompt to update the app if the app build is over 8 weeks old. "},{"title":"BUG FIXES​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-3","content":"Fixed a bug that occasionally prevented partner mode glasses from being set up for over-the-air software updates properly. Please double-check that your glasses are able to update to a recent build (April 2023 or later). If your glasses are not updating, please reach out to the Aria team.After glasses are unpaired, the app prevents re-pairing with the glasses until they finish rebooting.Other minor bug fixes "},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-2","content":"The latest Project Aria OS build 4961244.1190.70 was released on May 2, 2023 "},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-3","content":"Contact Aria User Support by posting in Project Aria Academic Partner Announcements, Feedback &amp; Support or emailing AriaOps@meta.com. "},{"title":"April 3, 2023 Aria Desktop App v36 is now available​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#april-3-2023--aria-desktop-app-v36-is-now-available","content":""},{"title":"IMPORTANT NOTICE​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#important-notice","content":"v36 will clear the app's cache when you start it for the first time. Please make sure you download all of your MPS artifacts (Trajectory, Eye Gaze) before installing and starting v36. Dear Academic Partners, The Aria Desktop App v36 for Mac and Windows is now available for download from the Aria Web Portal including the brand new Linux version. Here are the updates this new version brings: "},{"title":"NEW FEATURES​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-features","content":"Aria for Linux is now available as a debian package for Ubuntu, more precisely the 22.04 LTS version. It is important to note the app was only tested for that specific version under Gnome 42.5 and X11 (X Server) as well as Wayland. Any other debian distribution (Ubuntu 22.04 fork such as Kubuntu, Mint etc..) or environment may or may not work. Find updated instructions in the Aria For Linux Installer section of the Desktop App page to find out how to install Aria on Ubuntu. "},{"title":"IMPROVEMENTS​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#improvements","content":"Reduced app size bundle (both pre and post install)Reduced app startup time "},{"title":"BUG FIXES​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-4","content":"Various bug fixes improving the overall usage of the Desktop App. "},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-4","content":"Contact Aria User Support by posting in Project Aria Academic Partner Announcements, Feedback &amp; Support or emailing AriaOps@meta.com.  "},{"title":"March 24, 2023 Aria Mobile App v105 is now available​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#march-24-2023-aria-mobile-app-v105-is-now-available","content":"Dear Academic Partners, The Aria Mobile App v105 for Android is now available for download from the Aria Web Portal (accessed from your Android internet browser). Here are the updates this new version brings. "},{"title":"NEW & UPDATED FEATURES​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features-3","content":"The default recording profile, engaged when starting a recording by pressing the Capture button on the Project Aria device directly, can now be set and viewed on the Mobile Aria App on Android (see screenshot below). This feature used to be only available on the Desktop Aria App.Tapping “Unpair Glasses” mentions the number of on-device recordings (not uploaded) the unpairing will delete through factory reset.The Device ID is now listed in the app, below the serial number in the device settings page. Go to the Device Info page for more information about Device IDs. "},{"title":"BUG FIXES​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-5","content":"On Android only, a bug causing difficulties with switching between more than 8 paired glasses has been fixed.The recording setup screen is no longer dismissed if the recording fails to start.Minor bug fixes "},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-3","content":"The latest Project Aria OS build 4959822.780.70 was released on February 22, 2023. "},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-5","content":"Contact Aria User Support by posting in Project Aria Academic Partner Announcements, Feedback &amp; Support or emailing AriaOps@meta.com. "},{"title":"February 22, 2023​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#february-22-2023","content":""},{"title":"Aria Mobile App v100 is now available​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-mobile-app-v100-is-now-available","content":"Dear Academic Partners, The Aria Mobile App v100 for Android is now available for download from the Aria Web Portal (accessed from your Android internet browser). Here are the updates this new version brings. "},{"title":"NEW & UPDATED FEATURES​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features-4","content":"The on-screen status message stating a recording is starting or completing used to render the whole page temporarily unresponsive until the recording was fully started or saved. Now, an equivalent status message shows up on top of the screen (not as an overlay message in the center), allowing the user to dismiss or interact with the page at any time.Several UI adjustments were made to improve visibility and ease of interaction with various app sections and buttons. "},{"title":"BUG FIXES​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-6","content":"The issue causing the recording status to occasionally stay on-screen instead of being dismissed when completing a recording by pressing the Capture button has been fixed.Minor bug fixes "},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-4","content":"The latest Project Aria OS build 4959822.780.70 was released on February 22, 2023. "},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-6","content":"Contact Aria User Support by posting in Project Aria Academic Partner Feedback &amp; Support or emailing AriaOps@meta.com.   "},{"title":"February 9, 2023, Aria Desktop App v35 is now available​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#february-9-2023--aria-desktop-app-v35-is-now-available","content":"Dear Academic Partners, The Aria Desktop App v35 for Mac and Windows is now available for download from the Aria Web Portal. Here are the updates this new version brings. "},{"title":"NEW FEATURES​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-features-1","content":"We revamped the Recordings view to simplify the distinction between accessing Project Aria's device storage and the tools tailored for processing VRS files on the local host with a dedicated VRS Tools tab. Every file operation on the local host (Mac/Windows) is now done using the native file explorer (Finder for Mac, File Explorer for Windows). Find updated instructions in Desktop App instructions, in the Device Storage section, to know how to copy locally VRS files from the Project Aria device storage.Find updated instructions in Desktop App wiki page, in the Playback section, to know how to read your locally copied VRS files.Find updated instructions in MPS wiki page for how to request MPS. "},{"title":"BUG FIXES​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-7","content":"Various bug fixes improving the overall usage. "},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-7","content":"Contact Aria User Support by posting in Project Aria Academic Partner Feedback &amp; Support or emailing AriaOps@meta.com.   "},{"title":"January 20, 2023, Aria Mobile Companion App v95 is now available​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#january-20-2023--aria-mobile-companion-app-v95-is-now-available","content":"Dear Academic Partners, The Aria Mobile Companion App v95 for Android is now available for download. Find it in the Aria Web Portal by visiting it directly from your Android internet browser. Here are the updates this new version brings. "},{"title":"NEW FEATURES​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-features-2","content":"Backend Update for User Accounts​ Over the next few weeks, we will be changing the way we create user accounts to log into the Aria Mobile App on Android. There should be no perceived difference after this change, even with older versions of the app. If you run into an issue, please contact us immediately. Mobile App Update Prompt​ When you launch the Mobile Companion app, it should trigger a notification prompting you to download the latest version from Aria Web Portal. If dismissed, the prompt will not show up again within the same day unless you log out and back in. Otherwise, the same notification prompt will show up the following day when launching the app. "},{"title":"BUG FIXES​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-8","content":"Various bug fixes "},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-8","content":"Contact Aria User Support by posting in Project Aria Academic Partner Feedback &amp; Support or emailing AriaOps@meta.com. "},{"title":"December 16, 2022 Major Feature Release 🚀(Dec 2022)​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#december-16-2022-major-feature-release-dec-2022","content":""},{"title":"New Feature Summary​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-feature-summary","content":"1) New Machine Perception Services (i.e. MPS): These features are available through the updated version of Desktop App. Improved Trajectory - We will now provide additional Trajectory output, including 1 khz open loop trajectory (instead of low frequency 10 hz previously), 1 khz closed loop trajectory, online calibration at camera frame rate and more robust error messaging for scenarios where Trajectory processing fails.Local Eye Gaze - Provides unit vectors and associated uncertainties for each ET frame. The gaze vectors are expressed in central pupil frame (CPF). We also provide CPF to device frame 6DOF transformation. 2) Aria Data Tools: Open Source tools that provide C++ and Python3 tools to interact with Project Aria data.Read and visualize Project Aria sequences and sensor dataRetrieve calibration data and interact with Aria camera modelsRead and visualize machine perception output from Project Aria sequences (6DoF Trajectory, Local Eye Gaze) 3) Usability &amp; Bug Fixes:Various usability improvements across Desktop, Mobile &amp; Aria Data ToolsImproved documentation, including a “Troubleshooting &amp; Known Issues” sectionWe will now be logging high-level anonymous usage data to better understand how we can improve your experience "},{"title":"ACCESSING NEW FEATURES​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#accessing-new-features","content":"To gain access to new features, you will need to: Download the most recent version of the Mobile (v90) and Desktop (v34) Apps from the Portal (projectaria.com). Additionally, we will prompt Mobile App users to update their app.Access Aria Data Tools from Github Thank you, and again please feel free to provide feedback. We want to hear the good, the bad and the ugly! "},{"title":"Detailed Release Notes​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#detailed-release-notes","content":""},{"title":"ARIA MOBILE APP V90​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-mobile-app-v90","content":"New features and bug fixes include: “Task ID” and “Client Tag” fields have been renamed “Name” and “Notes”.Access Denied screen no longer shows when external users start the app with no internet connection.Wi-Fi can be configured while glasses are not connected to power. It does not allow uploading without plugging in the device.Profile selection screen now shows sensor details for each profile. Note: If your current version is under v85 it will not update automatically. Please delete and install v90 by signing into the Portal (projectaria.com) with your Android device. "},{"title":"ARIA DESKTOP APP V34​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-desktop-app-v34","content":"New features and bug fixes include: New improved trajectory with open/close loop poses and online calibration.Eye Gaze vectors with uncertainty.Various usability improvements and bug fixes "},{"title":"DEVICE SW (BUILD 4958601.360.70)​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#device-sw-build-495860136070","content":"Please make sure your device is charged and connected to wifi via Aria Mobile App to receive this build. New features and bug fixes include: Enabled USB streaming support on Mac OS (not available on AriaHub yet).Added telemetry logging for a subset of device events with an anonymized location.Added new recording profile21.Companion App shows details about a streaming session started from AriaHub.Updated security patch level to November 2022.Added the ability to run RGB in RAW mode at high frame rate (not exposed in a profile yet).Removed profile17 from the list of recording profiles.   "},{"title":"November 11, 2022, Android Aria App v85 is now available](https://my.workplace.com/groups/1137227200340269/permalink/1215336389196016/)​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#november-11-2022-android-aria-app-v85-is-now-availablehttpsmyworkplacecomgroups1137227200340269permalink1215336389196016","content":"Dear Academic Partners,The Android Aria App v85 has now been released. Here is what you need to know.### NEW FEATURES AND IMPROVEMENTS Thumbnails showing what was recorded appear in each Recording Details page under the Recordings tab immediately after that recording is completed and saved (this feature should become available after the next Project Aria OS update around November 15th)If the Bluetooth and/or Location services need to be enabled, the app will now show a prompt to do so upon launching the app.During an ongoing recording, the Profile and Sensors used are now mentioned on the active recording page.Through the app, users can now connect a Project Aria device to an EAP-PWD Wi-Fi network (using username and password for authentication - does not support certificates).The Android app will now show a banner on top of the screen indicating that the user has not selected a default Profile for the paired Project Aria device, which can only be done via the Desktop app. "},{"title":"IF YOUR CURRENT VERSION IS V80​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#if-your-current-version-is-v80","content":"As long as you're currently using v80 (check the app version in the app Settings page), launching the app should trigger a notification prompting you to download the latest version from the Partner Portal. "},{"title":"IF YOUR CURRENT VERSION IS OLDER THAN V80​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#if-your-current-version-is-older-than-v80","content":"App versions older than v80 will not receive any notifications, as this is a new feature (as announced in this post). In this case, you will need to uninstall the app and install v85 from the Partner Portal. "},{"title":"WHAT IF YOU DISMISSED THE PROMPT?​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#what-if-you-dismissed-the-prompt","content":"If you dismissed the notification prompting you to update, it should appear again when you launch the Android Aria App the following day. If you don't want to wait that long, you can directly install the latest version from the Partner Portal. "},{"title":"VERIFY V85 WAS INSTALLED​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#verify-v85-was-installed","content":"Once the Aria app is installed, login and tap the Settings tab at the bottom right corner of the Dashboard page. On that Settings page, you'll find the app version. "},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-9","content":"Contact Aria User Support by posting here in Project Aria Academic Partner Feedback &amp; Support or emailing AriaOps@meta.com.   "},{"title":"October 10, 2022 ARIA DESKTOP APP V32​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#october-10-2022-aria-desktop-app-v32","content":"With v32 a single universal Mac application is now available, supporting both Intel and Apple Silicon architecture "},{"title":"New Features​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-features-3","content":"Single universal Mac application supporting both Intel and Apple Silicon architectureLocal Notifications for both Mac &amp; WindowsMerging both Uploads &amp; Local recordings in the same viewAbility to select a default profile when using the HW recording buttonResizable columns for the Local &amp; Uploads tablesOverall app speed and performance improved "},{"title":"Deprecated Features​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#deprecated-features","content":"OS update &amp; Wireless connection (you may now use the latest Companion App for that) "},{"title":"Bug Fixes​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-9","content":"Frequent inability to select a recording profileInability to visualize the Aria Stream windowVarious issues when using the extracting features via the &quot;More&quot; toolbar buttonWindows app freezing randomlyOn Mac, when in fullscreen, the top toolbar covering the top of the window As always, please make sure to update your glasses to the latest version using the Companion App before starting to use Aria For Mac and Aria for Windows V32   "},{"title":"ARIA Companion App APP V80​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-companion-app-app-v80","content":"With v80, updating the companion app is now easier. "},{"title":"New Features​","type":1,"pageTitle":"Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-features-4","content":"Starting with v80, the Android app will show a notification prompting you to update it when a newer version becomes available. The on-screen prompt will take you to the location (Aria Web Portal) of the new version, allowing you to update the app faster. "},{"title":"Aria Research Kit User Support","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/troubleshooting/get_support","content":"Aria Research Kit User Support If you need further support, have feedback or feature requests we encourage you to: Post to the Academic Partners Workplace groupEmail AriaOps@meta.com","keywords":""},{"title":"How to Reduce VRS File Size","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/troubleshooting/reduce_vrs_file_size","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"How to Reduce VRS File Size","url":"/projectaria_tools/docs/ARK/troubleshooting/reduce_vrs_file_size#overview","content":"If uploading data to MPS for processing times out, you may need to reduce the size of the VRS file. Finding a faster internet connection may also help. Because VRS files store each sensor stream separately, you can use VRS tooling to create copies of VRS files that do not include specific sensor streams. The MPS output can then be used in combination with the original VRS file (with all sensor streams). MPS does not need the following sensor streams that contain a lot of data: RGB Sensor Streams (214-1) Should more than halve your file sizeJust removing this stream may be sufficient Microphone Sensor Streams (231-1), contains 8 audio channels "},{"title":"Instructions​","type":1,"pageTitle":"How to Reduce VRS File Size","url":"/projectaria_tools/docs/ARK/troubleshooting/reduce_vrs_file_size#instructions","content":""},{"title":"Prerequisites​","type":1,"pageTitle":"How to Reduce VRS File Size","url":"/projectaria_tools/docs/ARK/troubleshooting/reduce_vrs_file_size#prerequisites","content":"These instructions use the VRS tools to create a VRS file without a specific sensor stream. Install and Build VRS "},{"title":"Command​","type":1,"pageTitle":"How to Reduce VRS File Size","url":"/projectaria_tools/docs/ARK/troubleshooting/reduce_vrs_file_size#command","content":"In this example, a copy of the VRS file is created that does not include RGB Sensor Streams (214-1) vrs copy &lt;path/to/recording.vrs&gt; --to &lt;path/to/recording_norgb.vrs&gt; - 214-1 "},{"title":"Troubleshooting & Known Issues","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#overview","content":"This page provides troubleshooting information for issues you may encounter while using the ARK. It covers: Device and Recording IssuesDesktop App IssuesMachine Perception Services Issues and Error Messages Go to the Glasses Manual for information about LED States, button configuration, how to factory reset and power cycle your device. If you need further support, have feedback or feature requests, please email AriaOps@meta.com or post to Project Aria Academic Partner Announcements, Feedback &amp; Support. "},{"title":"Device and Recording Issues​","type":1,"pageTitle":"Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#device-and-recording-issues","content":""},{"title":"I haven't received my Project Aria glasses​","type":1,"pageTitle":"Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#i-havent-received-my-project-aria-glasses","content":"Once your Aria glasses request has been approved, please contact AriaOps@meta.com if you project hasn't received the requested devices in: 7 days when shipping to continental USA14 days when shipping internationally "},{"title":"I can't pair/detect my glasses with the Mobile Companion App!​","type":1,"pageTitle":"Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#i-cant-pairdetect-my-glasses-with-the-mobile-companion-app","content":"Make sure your Aria glasses are plugged into power and that the privacy switch is not turned on (the switch should be pushed forwards (towards the lenses of your device) to be able to pair. "},{"title":"I can't start recording​","type":1,"pageTitle":"Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#i-cant-start-recording","content":"Check that the privacy switch is not engaged. The switch should be pushed forward (towards the lenses of your device) for recording to be possible.  "},{"title":"Where's my recording?​","type":1,"pageTitle":"Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#wheres-my-recording","content":"It is possible you may have accidentally discarded your recording. Do not use the privacy switch to stop recording, as it prevents recording and deletes any current recording. "},{"title":"Stop recording does not work/ has a long delay​","type":1,"pageTitle":"Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#stop-recording-does-not-work-has-a-long-delay","content":"The longer a device records for, the longer it takes for a recording to stop. This is because the larger the VRS file, the longer it takes a recording to finish indexing. This can be particularly apparent in the Desktop App, as you can press the Stop button and it looks like nothing has happened. note Recording has not fully stopped until the Recording LEDs have turned off "},{"title":"Desktop App Issues​","type":1,"pageTitle":"Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#desktop-app-issues","content":""},{"title":"Aria device flickers on and off in the Desktop app​","type":1,"pageTitle":"Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#aria-device-flickers-on-and-off-in-the-desktop-app","content":"You most likely need to update your Aria device's OS. Pair your device with the mobile companion app. Your device should update automatically once paired. If the issue continues to occur: In the Aria Mobile Companion App, select the settings icon next to your Paired GlassesScroll down to Device ModeIf you have the correct device, the device mode should say “Partner” If your device mode says anything else, please contact Aria User Support to get a replacement device If the device mode correctly shows “Partner”, scroll up to Glasses OS and select Check for Updates "},{"title":"Copying files locally takes a long time!​","type":1,"pageTitle":"Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#copying-files-locally-takes-a-long-time","content":"We recommend copying data from your Aria device using ADB or MTP (Windows automatic files transfer) rather than the Desktop App, for a faster experience. On a Windows machine, your Aria device will be automatically detected as a USB drive when plugged in and the Windows File Explorer will automatically open a new window showing Aria and its internal device storage. Go to the Quickstart Guide for ADB commands and MTP instructions. "},{"title":"Desktop app/computer can't detect my Aria device​","type":1,"pageTitle":"Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#desktop-appcomputer-cant-detect-my-aria-device","content":"It may be that your Aria device's battery is drained, make sure it is correctly charging (there should be a blue LED on the right arm) and wait ten minutes. On Linux, this also may be due to USB driver issues. Fix USB Driver Issues in Linux "},{"title":"Machine Perception Services (MPS)​","type":1,"pageTitle":"Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#machine-perception-services-mps","content":""},{"title":"How do I find out what recording profile was used?​","type":1,"pageTitle":"Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#how-do-i-find-out-what-recording-profile-was-used","content":"There are several ways to check what recording profile was used. Method 1: Check the vrs.json file​ Every VRS recording comes with a .vrs.json file that contains metadata about that recording. Open the file with any text editor and check the recording_profile value. Method 2: Get metadata from the VRS file​ Go to Recordings in the Desktop AppSelect ToolsSelect More &gt; VRS Data Layout &gt; JSON MetadataSelect a VRS recording from your directorySearch the opened .vrs-description.txt file for “profile” "},{"title":"Can’t upload data to MPS/Upload keeps failing​","type":1,"pageTitle":"Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#cant-upload-data-to-mpsupload-keeps-failing","content":"If you cannot upload your data to MPS it could be that your VRS file is timing out during upload. Finding a faster internet connection may help or you can reduce your VRS file’s size. How to reduce VRS file size "},{"title":"Machine Perception Services (MPS) Error Messages​","type":1,"pageTitle":"Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#machine-perception-services-mps-error-messages","content":""},{"title":"Unsupported Format​","type":1,"pageTitle":"Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#unsupported-format","content":"You may encounter this error message when trying to unzip a downloaded MPS output file, especially large trajectory files. “Unsupported Format” occurs when the zip file has not fully downloaded. Please wait and try again later. "},{"title":"Unsupported MPS Profile​","type":1,"pageTitle":"Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#unsupported-mps-profile","content":"This recording profile does not support trajectory generation. This recording profile does not support eye gaze generation. Trajectory and Eye Gaze derived data can only be generated if they have the necessary sensor data. Go to the Recording Profiles page for information about supported profiles. "},{"title":"Recording duration not supported​","type":1,"pageTitle":"Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#recording-duration-not-supported","content":"This recording duration is not supported by trajectory generation. If you experience this error message, it is because the recording is either too long or too short. Recordings need to be longer than 5 seconds and shorter than one hour. "},{"title":"Health checks failed (trajectory)​","type":1,"pageTitle":"Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#health-checks-failed-trajectory","content":"The health checks for this recording failed. When trajectory data is derived, it goes through a series of health checks that are recorded in the summary.json file that's included with in every trajectory output. If your trajectory data fails, you can download a zip file that contains the summary.json file with further information. Go to Recordings &gt; Uploads to download the summary.json file. If you need to reach out to Support, please include the Transaction ID (found in the Tools tab, MPS Uploads) as well as the summary.json file. "},{"title":"Fix USB Driver Issues in Linux","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/troubleshooting/linux_usb_driver","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Fix USB Driver Issues in Linux","url":"/projectaria_tools/docs/ARK/troubleshooting/linux_usb_driver#overview","content":"If the Aria Desktop app or computer can't detect an Aria device, it may be that your Aria device's battery is drained, or in Linux it may be because of your USB driver. Use the following instructions to resolve USB driver issues in Linux. "},{"title":"Prerequisites​","type":1,"pageTitle":"Fix USB Driver Issues in Linux","url":"/projectaria_tools/docs/ARK/troubleshooting/linux_usb_driver#prerequisites","content":"Android Device Bridge (ADB) To install ADB use sudo apt-get android-tools "},{"title":"Instructions​","type":1,"pageTitle":"Fix USB Driver Issues in Linux","url":"/projectaria_tools/docs/ARK/troubleshooting/linux_usb_driver#instructions","content":""},{"title":"Look for Aria device​","type":1,"pageTitle":"Fix USB Driver Issues in Linux","url":"/projectaria_tools/docs/ARK/troubleshooting/linux_usb_driver#look-for-aria-device","content":"With your Aria device plugged into your computer, use the command adb devices. If your device can be found, you'll get an output like: List of devices attached 1820dc10 device  If you see no permissions: List of devices attached 1820dc10 no permissions  you likely need to change your udev. "},{"title":"Change udev​","type":1,"pageTitle":"Fix USB Driver Issues in Linux","url":"/projectaria_tools/docs/ARK/troubleshooting/linux_usb_driver#change-udev","content":"The following instructions were taken from Arch Linux's Android Debug Bridge instructions and Janos Gyerik's Adding udev rules: Step 1: Get VENDOR_ID and PRODUCT_ID​ Use list devices to find the [VENDOR_ID] and [PRODUCT_ID] of your Aria device. The command lsusb  should show something like: Bus 002 Device 002: ID 2833:0086 Facebook, Inc. Aria  In the example above, [VENDOR_ID] = 2833 and [PRODUCT_ID]=0086 Step 2: Modify 51-android.rules​ Using lsusb will create a new file /etc/udev/rules.d/51-android.rules Modify 51-android.rules using the following commands or script. Make sure you create a group called adbusers and $USER, so that you have the correct permissions. Commands $ cat /etc/udev/rules.d/51-android.rules SUBSYSTEM==&quot;usb&quot;, ATTR{idVendor}==&quot;2833&quot;, MODE=&quot;0660&quot;, GROUP=&quot;adbusers&quot;, TAG+=&quot;uaccess&quot; SUBSYSTEM==&quot;usb&quot;, ATTR{idVendor}==&quot;2833&quot;, ATTR{idProduct}==&quot;0086&quot;, MODE=&quot;0660&quot;, GROUP=&quot;adbusers&quot;, SYMLINK+=&quot;android_adb&quot; SUBSYSTEM==&quot;usb&quot;, ATTR{idVendor}==&quot;2833&quot;, ATTR{idProduct}==&quot;0086&quot;, MODE=&quot;0660&quot;, GROUP=&quot;adbusers&quot;, SYMLINK+=&quot;android_fastboot&quot;  Reboot your workstation to ensure the changes are applied. Script This script will will apply the previous commands and reboot your workstation. IDs=$(lsusb | grep Facebook) if [[ &quot;$?&quot; -ne 0 ]]; then echo &quot;Make sure you have your VROS device connected to your workstation&quot; exit fi IDs=$(echo $IDs | cut -d &quot; &quot; -f 6) VID=$(echo $IDs | cut -d &quot;:&quot; -f 1) PID=$(echo $IDs | cut -d &quot;:&quot; -f 2) conf_f=/etc/udev/rules.d/51-android.rules sudo touch ${conf_f} echo &quot;SUBSYSTEM==\\&quot;usb\\&quot;, ATTR{idVendor}==\\&quot;$VID\\&quot;, MODE=\\&quot;0660\\&quot;, GROUP=\\&quot;adbusers\\&quot;, TAG+=\\&quot;uaccess\\&quot;&quot; &gt;&gt; $conf_f echo &quot;SUBSYSTEM==\\&quot;usb\\&quot;, ATTR{idVendor}==\\&quot;$VID\\&quot;, ATTR{idProduct}==\\&quot;$PID\\&quot;, MODE=\\&quot;0660\\&quot;, GROUP=\\&quot;adbusers\\&quot;, SYMLINK+=\\&quot;android_adb\\&quot;&quot; &gt;&gt; $conf_f echo &quot;SUBSYSTEM==&quot;usb&quot;, ATTR{idVendor}==\\&quot;$VID\\&quot;, ATTR{idProduct}==\\&quot;$PID\\&quot;, MODE=\\&quot;0660\\&quot;, GROUP=\\&quot;adbusers\\&quot;, SYMLINK+=\\&quot;android_fastboot\\&quot;&quot; &gt;&gt; $conf_f sudo groupadd adbusers sudo usermod -aG adbusers $USER  "},{"title":"How to Update Your Aria Glasses OS","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/troubleshooting/update_glasses_os","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"How to Update Your Aria Glasses OS","url":"/projectaria_tools/docs/ARK/troubleshooting/update_glasses_os#overview","content":"This page is for Project Aria glasses users that wish to manually update their device's Operating System. Normally, your glasses' OS will automatically update when it is plugged into power and connected to Wi-Fi. "},{"title":"To Identify Your OS Build​","type":1,"pageTitle":"How to Update Your Aria Glasses OS","url":"/projectaria_tools/docs/ARK/troubleshooting/update_glasses_os#to-identify-your-os-build","content":" In the Mobile Companion App, select Device SettingsScroll down to view your Aria Glasses OS Version "},{"title":"To Update the OS​","type":1,"pageTitle":"How to Update Your Aria Glasses OS","url":"/projectaria_tools/docs/ARK/troubleshooting/update_glasses_os#to-update-the-os","content":"Plug in your glasses into power and ensure they are connected to Wi-Fi.In the Mobile Companion App, select Device SettingsScroll down to view the OS VersionSelect Check for UpdatesOnce your glasses have finished updating, it will reboot your glasses, and the update will be complete "},{"title":"Attribution and Contributing","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/attribution_citation","content":"","keywords":""},{"title":"Citation​","type":1,"pageTitle":"Attribution and Contributing","url":"/projectaria_tools/docs/attribution_citation#citation","content":""},{"title":"Project Aria Tools​","type":1,"pageTitle":"Attribution and Contributing","url":"/projectaria_tools/docs/attribution_citation#project-aria-tools","content":"If you use Project Aria tools or data in your research, please consider starring ⭐ our github repository, and citing the Project Aria Whitepaper. @misc{ariawhitepaper23, title={Project Aria}, author={Project Aria team}, year={2023}, howpublished={\\url{https://drive.google.com/file/d/1eAgYMXbI6zNtTC6c9eEOctMGG8u43rJS/view}}, }  "},{"title":"Aria Digital Twin Dataset​","type":1,"pageTitle":"Attribution and Contributing","url":"/projectaria_tools/docs/attribution_citation#aria-digital-twin-dataset","content":"If you use Aria Digital Twin dataset and its tools, please cite the Aria Digital Twin dataset paper: @misc{pan2023aria, title={Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception}, author={Xiaqing Pan and Nicholas Charron and Yongqian Yang and Scott Peters and Thomas Whelan and Chen Kong and Omkar Parkhi and Richard Newcombe and Carl Yuheng Ren}, year={2023}, eprint={2306.06362}, archivePrefix={arXiv}, primaryClass={cs.CV} }  "},{"title":"Aria Synthetic Environments Dataset​","type":1,"pageTitle":"Attribution and Contributing","url":"/projectaria_tools/docs/attribution_citation#aria-synthetic-environments-dataset","content":"If you use Aria Synthetic Environments Dataset and its tools, please cite the Aria Synthetic Environments dataset paper (coming soon). "},{"title":"Aria Pilot Dataset​","type":1,"pageTitle":"Attribution and Contributing","url":"/projectaria_tools/docs/attribution_citation#aria-pilot-dataset","content":"If you use the Aria Pilot Dataset in GitHub, please cite @misc{aria_pilot_dataset, title = {Aria Pilot Dataset}, author = {Zhaoyang Lv and Edward Miller and Jeff Meissner and Luis Pesqueira and Chris Sweeney and Jing Dong and Lingni Ma and Pratik Patel and Pierre Moulon and Kiran Somasundaram and Omkar Parkhi and Yuyang Zou and Nikhil Raina and Steve Saarinen and Yusuf M Mansour and Po-Kang Huang and Zijian Wang and Anton Troynikov and Raul Mur Artal and Daniel DeTone and Daniel Barnes and Elizabeth Argall and Andrey Lobanovskiy and David Jaeyun Kim and Philippe Bouttefroy and Julian Straub and Jakob Julian Engel and Prince Gupta and Mingfei Yan and Renzo De Nardi and Richard Newcombe}, howpublished = {\\url{https://about.facebook.com/realitylabs/projectaria/datasets}}, year = {2022} }  "},{"title":"Contributing​","type":1,"pageTitle":"Attribution and Contributing","url":"/projectaria_tools/docs/attribution_citation#contributing","content":"We welcome contributions! See CONTRIBUTING for details about how to get started, and our code of conduct. "},{"title":"How to Join the Academic Partners Workplace Group","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/workplacegroup","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"How to Join the Academic Partners Workplace Group","url":"/projectaria_tools/docs/ARK/workplacegroup#overview","content":"As part of the Aria Research Kit, you'll get access to theProject Aria Academic Partner Announcements Feedback and Support Workplace Group. This group is Aria's research community space where researchers and engineers at Meta and all our Academic partners can connect, ask questions, share ideas and provide support. In addition to being the place where we make announcements, we want to support researchers to: Provide feedback about their Aria experiencesParticipate in general discussions about AriaAsk our community of researchers and engineers questions (support related or exploring an idea)Engage with other academic researchers and perhaps even set challenges for each other Once you've joined this group you can post support queries in the group or directly message our user support team. This group is specifically for people who are working with Aria Devices. "},{"title":"How to Join​","type":1,"pageTitle":"How to Join the Academic Partners Workplace Group","url":"/projectaria_tools/docs/ARK/workplacegroup#how-to-join","content":"Please do not add extra people to this group If someone who is approved for the ARK and has received Aria glasses is missing, please let your Meta point of contact know. When you first onboard with Project Aria you will receive two emails. One will contain account credentials that you'll use with the Partner Portal and the Companion App. The second email will invite you to join the Project Aria Workplace Group. The email will be from notification@fbworkmail.com and have the subject “Join [person] in Project Aria Academic Partner Announcements, Feedback and Support”. "},{"title":"To join the WP group:​","type":1,"pageTitle":"How to Join the Academic Partners Workplace Group","url":"/projectaria_tools/docs/ARK/workplacegroup#to-join-the-wp-group","content":"Find the email invitation with the subject Join [Person] in Project Aria Academic Partner Announcements, Feedback &amp; Support and select “Join [person]” If you don't have this email invitation, please email AriaOps@meta.com In your browser, follow the prompts to create a new Workplace To set up a workplace group you just need to answer a few questions: your name, type of role, organization and organization size. Once you've set up your workplace, you should see a prompt saying that you've been invited into a closed multi-company group.Select Accept InviteYou should then see “Welcome [Your Name]!”Select Skip or Next If you can't see Skip or Next, scroll down the page. The next button might not be immediately visible on some mobile devices Follow the prompts until you get to the workplace group You may need to scroll further down the page if you do not see the Next button "},{"title":"Accessing the Workplace Group​","type":1,"pageTitle":"How to Join the Academic Partners Workplace Group","url":"/projectaria_tools/docs/ARK/workplacegroup#accessing-the-workplace-group","content":"You should be able to continue from your workplace group creation. In addition, once you've created your workplace group you should get an email fromnotification@fbworkmail.com with the subject “[Your Workplace Group name] via Workplace”. Select Log into Workplace and enter your credentialsYou should see your groups displayed in the Home section On Mobile, select the menu icon on the top right of your screen to view Home "},{"title":"Data Formats","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats","content":"Data Formats Aria's sensor data is stored in VRS format. We can also provide derived data from our machine perception algorithms. In this section, we describe the format of raw sensor data in VRS format, and Machine Perception Service (MPS) data. We also describe the coordinate system convention we use.","keywords":""},{"title":"Timestamps in Aria VRS","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs","content":"","keywords":""},{"title":"Device timestamps​","type":1,"pageTitle":"Timestamps in Aria VRS","url":"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs#device-timestamps","content":"Each piece of data captured by Aria glasses is associated with a device timestamp (also called capture timestamp in the VRS file format). All sensors on the same pair of Aria glasses share the same device time domain issued from a single clock. We strongly recommend always working with the device timestamp when dealing with single-device Aria data. "},{"title":"Timecode timestamp​","type":1,"pageTitle":"Timestamps in Aria VRS","url":"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs#timecode-timestamp","content":"In the case of multi-device data capture, we use time sync servers to record pairs of timestamps between the server’s local timestamp and the Aria device’s device timestamp. This generates a mapping between the server’s local time and Aria’s device time. The server’s local time serves as a unified time domain shared by the multiple devices. These are called Timecode timestamps. Timecode time refers to the same “capture” event as the device time, but differs by the clock assigning the timestamps. Thus we can convert between timecode time and device time using the timestamp samples corrected by the time sync servers. "},{"title":"Record and Host(Arrival) timestamps​","type":1,"pageTitle":"Timestamps in Aria VRS","url":"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs#record-and-hostarrival-timestamps","content":"When working with Aria data you might encounter timestamps with other time domains. For instance, we call record timestamps the timestamps stored in the index of VRS files. For Project Aria glasses, these are equal to the device timestamp converted to a double-precision floating point representation. Additionally, some of the data record might have a host or arrival timestamp, their meaning is unspecified and should not be needed for any purpose. "},{"title":"Aria VRS format","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format","content":"","keywords":""},{"title":"Aria data streams​","type":1,"pageTitle":"Aria VRS format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#aria-data-streams","content":"In VRS, data is organized by streams. Each stream stores the data measured by a specific sensor1. The streams are identified by their stream ID. Each stream ID is composed of two parts, a recordable type ID to categorize the type of the sensor and a stream ID for identify the specific sensor instance. E.g. the first SLAM (aka Mono Scene) camera is identified as 1201-1 where 1201 is the numerical ID for SLAM camera data type, and 1 identifies the cameras as the first instance. 1 Two exceptions are Aria Eye Tracking(ET) cameras and microphones. The two ET cameras share a single data stream. All seven microphones also share a single data stream. Alternatively, the streams are identified by a shorter form of label. Labels are used to identify sensors in calibration. The following table lists the streamID and type ID as well as their label. Note GPS, Wi-Fi and Bluetooth are not calibrated but they do have a label. If you are using loaders in projectaria tools, you do not have to memorize this mapping. Projectaria tools has an API that converts between Stream ID and labels.  Sensor Stream ID Recordable type ID label ET camera 211-1 EyeCameraRecordableClass camera-et RGB camera 214-1 RgbCameraRecordableClass camera-rgb Microphone 231-1 StereoAudioRecordableClass mic Barometer 247-1 BarometerRecordableClass baro GPS 281-1 GpsRecordableClass gps Wi-Fi 282-1 WifiBeaconRecordableClass\twps Bluetooth 283-1 BluetoothBeaconRecordableClass\tbluetooth SLAM/Mono Scene camera left 1201-1 SlamCameraData camera-slam-left SLAM/Mono Scene camera right\t1201-2 SlamCameraData camera-slam-right IMU (1kHz) 1202-1 SlamImuData imu-right IMU (800Hz) 1202-2 SlamImuData imu-left Magnetometer 1203-1 SlamMagnetometerData mag  Each stream also contains a configuration blob that stores sensor-specific information such as image resolution, nominal frame rate, etc. All data in VRS is timestamped. Go to Timestamps in Aria VRS for more details. "},{"title":"Aria sensor data and configuration​","type":1,"pageTitle":"Aria VRS format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#aria-sensor-data-and-configuration","content":"Sensor data includes: Sensor readoutTimestampsAcquisition parameters (exposure and gain settings)Conditions (e.g. temperature) during data collection Most sensor data of a single stream and at a specific timestamp is stored as a single piece, except for image and audio. "},{"title":"How data is stored for image recordings​","type":1,"pageTitle":"Aria VRS format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#how-data-is-stored-for-image-recordings","content":"Each camera stores a single image frame at a time, with the exception of the ET camera. ET cameras pair share a single image frame by concatenating horizontally. The image frame contains two parts, the image itself and the image record. The image record stores timestamps, frame id, and acquisition parameters such as exposure and gain. This avoids having to read image data to get the information in the record. "},{"title":"How data is stored for audio recordings​","type":1,"pageTitle":"Aria VRS format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#how-data-is-stored-for-audio-recordings","content":"The audio data is grouped into data chunks of 4096 audio samples from all 7 microphones.Each chunk contains two parts, the data part for the audio signal, and the report part for the timestamps of each audio signal. "},{"title":"Sensor configuration blob​","type":1,"pageTitle":"Aria VRS format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#sensor-configuration-blob","content":"The sensor configuration blob stores static information of a stream. Common sensor configuration stores information such as sensor model, sensor serial (if available) as well as frame rate. Stream-specific information, such as image resolution, are also stored in configurations. See source code for the detailed implementation of sensor data and configurations. See here for example sensor data and how to accessing our sensor data using our Python data utilities. "},{"title":"Useful VRS tools​","type":1,"pageTitle":"Aria VRS format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#useful-vrs-tools","content":"The most intuitive way to access Aria data is via our loaders and visualizers. We provide Python and C++ interface to easily access VRS data. You may also want to use vrs tools to extract or inspect VRS data. Below we provide instruction for some most common use cases. "},{"title":"Check the VRS file’s validity and integrity​","type":1,"pageTitle":"Aria VRS format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#check-the-vrs-files-validity-and-integrity","content":"The check command decodes every record in the VRS file and prints how many records were decoded successfully. It proves that the VRS file is correct at the VRS level. You can also compute a checksum to ensure you have valid VRS files. For more information see VRS File Validation. $ vrs check &lt;file.vrs&gt; $ vrs checksum &lt;file.vrs&gt;  If the file is not valid, it is normally because there is missing data that could lead to invalid behavior with the tooling. All files in the Aria Pilot Dataset are valid, so if you encounter that issue with this dataset, re-downloading the file should resolve this issue. "},{"title":"Extract image or audio content to folders​","type":1,"pageTitle":"Aria VRS format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#extract-image-or-audio-content-to-folders","content":"Use the following commands to extract JPEG or WAV files. Use the --to &lt;folder_path&gt; to specify a destination folder where the data will be extracted, or it will be added to the current working directory. $ vrs extract-images &lt;file.vrs&gt; --to &lt;image_folder&gt; $ vrs extract-audio &lt;file.vrs&gt; --to &lt;audio_folder&gt;  To extract RAW image files, use: vrs extract-images &lt;file.vrs&gt; --raw-images --to &lt;image_folder&gt;  "},{"title":"Extract all content to folders and JSONs​","type":1,"pageTitle":"Aria VRS format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#extract-all-content-to-folders-and-jsons","content":"This command lets you extract all images, audio, and metadata into files: vrs extract-all &lt;file.vrs&gt; --to &lt;folder&gt;  The metadata is extracted into a single .jsons file that contains a succession of json messages, one per line. Each line corresponds to a single record, in timestamp order, so it is possible to parse it even if the number of records is huge. Saving all the data in a single file prevents saturating your disk with possibly millions of small files. Once extracted, your file will look like this:  ├── file.vrs ├── all_data * `NNNN-MM` folders: image folders, one folder per stream containing images. ├── 1201-1 # SLAM Left images ├── *.jpg ├── 1201-2 # SLAM Right images ├── *.jpg ├── 211-1 # Eye Tracking images ├── *.jpg ├── 214-1 # RGB (Color) Camera images ├── *.jpg ├── metadata.jsons └── ReadMe.md  For more information, see VRS Data Extraction. "},{"title":"Inspect how many data recordings there are by type​","type":1,"pageTitle":"Aria VRS format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#inspect-how-many-data-recordings-there-are-by-type","content":"vrs &lt;file.vrs&gt; | grep &quot;] records.&quot;  Will get you a return like this: 623 Eye Camera Class #1 - device/aria [211-1] records. 1244 RGB Camera Class #1 - device/aria [214-1] records. 729 Stereo Audio Class #1 - device/aria [231-1] records. 3101 Barometer Data Class #1 - device/aria [247-1] records. 65 Time Domain Mapping Class #1 - device/aria [285-1] records. 623 Camera Data (SLAM) #1 - device/aria [1201-1] records. 623 Camera Data (SLAM) #2 - device/aria [1201-2] records. 61965 IMU Data (SLAM) #1 - device/aria [1202-1] records. 50002 IMU Data (SLAM) #2 - device/aria [1202-2] records. 619 Magnetometer Data (SLAM) #1 - device/aria [1203-1] records.  Each line reports how many data records are stored in each data stream as well as the stream ID, e.g. 623 Camera Data (SLAM) #2 - device/aria [1201-2] records.  indicates the second slam camera recorded 623 frames. The stream name is Camera Data (SLAM) #2 and identified by numerical ID [1201-2]. "},{"title":"Aria Glasses 2D Image Coordinate System Conventions","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/coordinate_convention/2d_image_coordinate_system_convention","content":"Aria Glasses 2D Image Coordinate System Conventions For any provided camera intrinsic calibration value we use the convention that the color value of a pixel with integer coordinates (u,v)(u,v)(u,v) is the average color of the square spanning from (u−0.5,v−0.5)(u-0.5,v-0.5)(u−0.5,v−0.5) to (u+0.5,v+0.5)(u+0.5,v+0.5)(u+0.5,v+0.5) in continuous coordinates. This is visualized in the Figure 1, and has the following important consequences: Checking in bound: A pixel (u,v)(u,v)(u,v) is considered to be in bound if −0.5≤u&lt;W−0.5-0.5\\leq u&lt;W-0.5−0.5≤u&lt;W−0.5 and −0.5≤v&lt;H−0.5-0.5\\leq v&lt;H-0.5−0.5≤v&lt;H−0.5.Interpolation: In bilinear interpolation, a point (u,v) can be interpolated of all four neighboring integer-valued pixel coordinates are in-bound. That requires 0≤u≤W−10 \\leq u \\leq W-10≤u≤W−1 and 0≤v≤H−10 \\leq v \\leq H-10≤v≤H−1.Image down-sampling: When downsampling images by a factor of sss, every s×ss \\times ss×s pixel are squeezed into a single pixel. For example, the intensity at pixel s×ss \\times ss×s in the scaled image accounts for all the photons collected in the area [−0.5,s−0.5]×[−0.5,s−0.5][-0.5,s-0.5]\\times[-0.5,s-0.5][−0.5,s−0.5]×[−0.5,s−0.5] (i.e. column 000 to s−1s - 1s−1, and row 000 to s−1s-1s−1 in the discrete coordinate) in the original image. In order to keep this assumption valid, the re-scaled point pscaledp_\\text{scaled}pscaled​ not only needs to scale from the corresponding point in the original image poriginalp_\\text{original}poriginal​ but also accounts for the (0.5,0.5)(0.5,0.5)(0.5,0.5) translation accordingly by pscaled=s(poriginal+0.5)−0.5p_\\text{scaled} =s (p_\\text{original}+0.5)-0.5pscaled​=s(poriginal​+0.5)−0.5 Figure 1: 2D Image Coordinate System Conventions","keywords":""},{"title":"3D Coordinate Frame Conventions for Aria Glasses","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention","content":"","keywords":""},{"title":"SE(3) Lie groups​","type":1,"pageTitle":"3D Coordinate Frame Conventions for Aria Glasses","url":"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention#se3-lie-groups","content":"The 6-DoF poses are represented by SE(3) Lie group. The quaternion part of SE(3) uses Hamilton convention following the Eigen library, in which the exact formula to convert a quaternion to a rotation matrix of the SE(3) can be found here. We use the SE3d class in the Sophus Library to represent SE(3) Lie groups, and provide a minimal pybind for the class. PythonC++ transform_a_b represents a SE(3) rigid transformation from b coordinate frame to a coordinate frame. p_a represents an R^3 point (or vector) in the coordinate system of a. Easy mnemonics of the chaining principle (a, b, c are coordinate frames): transform_a_c = transform_a_b * transform_b_c; p_a = transform_a_b.matrix() @ numpy.append(p_b, 1) If you want to get quaternion from the SE3d, please notice the order is consistent to numpy quaternion_a_b = transform_a_b.quaternion() # order is w, x, y, z  3D Coordinate frame conventions Every sensor on Aria glasses has their own local coordinate system. We represent the 6DoF pose of each sensor as the relative pose (rotation and translation) with regard to the “Device frame&quot;. The device frame is by-default the local frame of the left SLAM camera.  "},{"title":"Camera coordinate system convention​","type":1,"pageTitle":"3D Coordinate Frame Conventions for Aria Glasses","url":"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention#camera-coordinate-system-convention","content":"A camera's local frame has its origin at the camera's optical center. Coarsely, when the camera is placed up-right, the camera coordinate frame's axes points to left, up and forward. More rigorously, we define a camera's local frame based on the optical axis and the entrance pupil of its lens. Both are uniquely defined for each camera according to the camera's lens prescription. The origin of a camera's local frame is at center of the camera's entrance pupil. The frame's Z axis is aligned with the optical axis. The camera's X axis are aligned with the projection of the image plane's X axis on the entrance pupil plane. The cross-product of the X and Z axis defines the system's Y axis.  "},{"title":"Non-visual sensor coordinate system​","type":1,"pageTitle":"3D Coordinate Frame Conventions for Aria Glasses","url":"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention#non-visual-sensor-coordinate-system","content":"We choose the IMU coordinate systems to have their origins at the position of the accelerometer, oriented along the direction of the accelerometer sensitive axis, eventually orthogonalized to compensate for sensor orthogonalities error. We do similarly for magnetometer.  "},{"title":"The nominal central pupil frame (CPF)​","type":1,"pageTitle":"3D Coordinate Frame Conventions for Aria Glasses","url":"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention#the-nominal-central-pupil-frame-cpf","content":"The CPF frame is placed at the midpoint between the eye boxes of the left and right eye. CPF's X-axis points left, Y-axis points up and the Z-axis points forward, from the person's perspective. Aria's ET gaze is defined as a vector in the CPF space originating at (0,0,0)(0, 0, 0)(0,0,0) of the CPF frame.  "},{"title":"MPS output - Eye gaze","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze","content":"","keywords":""},{"title":"Eye Gaze Data Format​","type":1,"pageTitle":"MPS output - Eye gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#eye-gaze-data-format","content":"MPS uses Aria's eye tracking camera images to estimate the direction in which the user is looking. This eye gaze estimation is in Central Pupil Frame. The output is generated in csv format and consists of the following fields Column\tType\tDescriptiontracking_timestamp_us\tint\tThis is the timestamp, in microseconds, of the eye tracking camera frame in device time domain. The MPS location output also contains pose estimations in the same time domain and these timestamps can be directly used to infer the device pose from the MPS location output. yaw_rads_cpf\tfloat\tThis is the eye gaze yaw angle in radians in CPF frame. The yaw angle is the angle between the projection of the eye gaze vector (originating at CPF) on XZ plane and the Z axis in the CPF frame. pitch_rads_cpf\tfloat\tThis is the eye gaze pitch angle in radians in CPF frame. The pitch angle is the angle between the projection of the eye gaze vector (originating at CPF) on YZ plane and the Z axis in the CPF frame. depth_m\tfloat\tThis is the absolute depth in meters of the 3D gaze point in CPF frame. This value is currently not available as part of MPS output. yaw_low_rads_cpf\tfloat\tThis value represents the lower bound of the confidence interval for the yaw estimation. pitch_low_rads_cpf\tfloat\tThis value represents the lower bound of the confidence interval for the pitch estimation. yaw_high_rads_cpf\tfloat\tThis value represents the upper bound of the confidence interval for the yaw estimation. pitch_high_rads_cpf\tfloat\tThis value represents the upper bound of the confidence interval for the pitch estimation. The confidence intervals represent the models uncertainty estimation. A smaller interval represents higher confidence and a wider interval represents lower confidence. The confidence interval angles are in radians and in CPF frame. Some common factors that impact uncertainty include: BlinkingHair occluding the eye tracking camerasRe-adjusting glasses or taking them off to clean them For utility function to load the eye gaze in Python and C++, please check the code examples "},{"title":"Yaw/Pitch to 3D vector conversion​","type":1,"pageTitle":"MPS output - Eye gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#yawpitch-to-3d-vector-conversion","content":"A common use case is to convert the gaze angles into 3D vectors. To convert a gaze measurement (yaw/pitch) into a 3D gaze vector originating at the origin of CPF use the operation here. "},{"title":"Basics","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/mps/mps_summary","content":"","keywords":""},{"title":"Common terminologies​","type":1,"pageTitle":"Basics","url":"/projectaria_tools/docs/data_formats/mps/mps_summary#common-terminologies","content":""},{"title":"graph_uid​","type":1,"pageTitle":"Basics","url":"/projectaria_tools/docs/data_formats/mps/mps_summary#graph_uid","content":"graph_uid is a unique identifier for the world coordinate frame. For all the 3D geometric instances like pose and points in the world frames (having _world in the suffix), when they have the same graph_uid, they are in the same coordinate frame. For simulation (such as Aria Synthetic Environments) and Aria Digital Twin(ADT) datasets we use the same random value for one space, e.g. the same graph_uid for one ADT/simulation space. "},{"title":"tracking_timestamp_us​","type":1,"pageTitle":"Basics","url":"/projectaria_tools/docs/data_formats/mps/mps_summary#tracking_timestamp_us","content":"tracking_timestamp_us's values are shaped by whether it is real world or synthetic data. For real world data, tracking_timestamp_us provides the Device timestamps from your Aria glasses. Go to Timestamps in Aria VRS for a definition of the device timestamps. In simulation datasets, this will be the timestamp in the simulator. In tracking_timestamp_us This clock has arbitrary starting points, which are not synchronized between recording sessions or devices.This clock is strictly monotonic, has stable clock speed, and is accurate in duration If you want to compute the time duration between two timestamps (especially when touching dynamics, e.g. integrating acceleration to velocity over time), you should use this timestamp. "},{"title":"utc_timestamp_ns​","type":1,"pageTitle":"Basics","url":"/projectaria_tools/docs/data_formats/mps/mps_summary#utc_timestamp_ns","content":"utc_timestamp_ns is the timestamp from Aria real-time clock (RTC). This time is synchronized to the cell phone time via the Aria Mobile Companion app to get UTC time at the beginning of the recording which is a rough estimate of the external standard clock. This clock is not available in the simulation datasets.This clock provides rough synchronization between sessions and devices.This clock is not guaranteed to be monotonic, or have stable clock speed, due to synchronization with NTP. So do not compute duration between two UTC timestamps. "},{"title":"Operator summary​","type":1,"pageTitle":"Basics","url":"/projectaria_tools/docs/data_formats/mps/mps_summary#operator-summary","content":"The operator summary includes individual operator’s status (except for eye tracking that does not have a summary today), whether the operation is successful. There is three possible status flag: SUCCESS: the operator is successfully finished without known issues.WARN: the operator is finished, but internally it detects problem(s) which may affect results quality. The operator still outputs the results, but we don’t have enough confidence in the quality of the results, so consume the results with caution.ERROR: the operator is not finished, or finished with major error, or the quality of the results are too bad to be consumed. Results may or may not be generated, and should not be consumed even if there are results. Other than the status flag, extra information (or warning/error reasons if known) messages are included as part of the summary. Here’s an example summary JSON output: { &quot;SLAM&quot;: { &quot;status&quot;: &quot;SUCCESS&quot;, &quot;info&quot;: [ &quot;Recording total time: 1104.00s; Trajectory total length: 155.42m&quot;, &quot;Total Vision Translational Correction (mm): p50: 0.048; p99: 0.451&quot;, &quot;Rotational Correction (deg): p50: 0.001; p99: 0.007&quot; ], &quot;warnings&quot;: [], &quot;errors&quot;: [] }, ...  "},{"title":"MPS Output - Semi-Dense Point Cloud","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/mps/mps_pointcloud","content":"","keywords":""},{"title":"What are semi-dense points?​","type":1,"pageTitle":"MPS Output - Semi-Dense Point Cloud","url":"/projectaria_tools/docs/data_formats/mps/mps_pointcloud#what-are-semi-dense-points","content":"Semi-dense points are the 3D points associated with tracks from our semi-dense tracking pipeline. Semi-dense tracks are continually created in pixel locations of input frames that lie in regions of high image gradient, and are then successively tracked in the following frames. Associated with each track is a 3D point, parameterized as an inverse distance along a ray originating from the track's first initial observation, as well as its uncertainty in inverse distance and distance. These points are transformed from their original camera coordinate spaces to the same coordinate frame associated with the closed loop trajectory of the sequence. "},{"title":"User needs to define how to enforce quality​","type":1,"pageTitle":"MPS Output - Semi-Dense Point Cloud","url":"/projectaria_tools/docs/data_formats/mps/mps_pointcloud#user-needs-to-define-how-to-enforce-quality","content":"To support user flexibility the tool outputs the associated points of all tracks regardless of quality. This means the data will contain a number of points whose positions have high uncertainty and are geometrically less accurate. Users will either need to threshold the pointcloud by setting a maximum allowed inverse distance / distance certainty or correctly weight points by their certainty when using them in downstream tasks. Nominal threshold values are a maximum inv_dist_std of 0.005 and a maximum dist_std of 0.01. "},{"title":"Points in the world coordinate frame​","type":1,"pageTitle":"MPS Output - Semi-Dense Point Cloud","url":"/projectaria_tools/docs/data_formats/mps/mps_pointcloud#points-in-the-world-coordinate-frame","content":"This file is the gzip compressed semi-dense points in the world coordinate system. The world coordinate frame is the same frame of the closed loop trajectory. For utility function to load the points in Python and C++, please check the code examples Column\tType\tDescriptionuid\tint\tA unique identifier of this point within this map graph_uid\tstring\tUnique identifier of the world coordinate frame. Associated with an equivalent graph_uid found in close_loop_trajectory.csv, depending on the frame this point was first observed in. p{x,y,z}_world\tfloat\tPoint location in the world coordinate frame p_world. inv_dist_std\tfloat\tStandard deviation of the inverse distance estimate, in meter^-1. Could be used for determining the quality of the 3D point position estimate dist_std\tfloat\tStandard deviation of the distance estimate, in meters. Could be used for determining the quality of the 3D point position estimate "},{"title":"Point observations​","type":1,"pageTitle":"MPS Output - Semi-Dense Point Cloud","url":"/projectaria_tools/docs/data_formats/mps/mps_pointcloud#point-observations","content":"The observation file is the gzip compressed semi-dense 2D observations, described in image pixel 2D coordinate frame. For utility function to load the observations in Python and C++, please check the code examples Column\tType\tDescriptionuid\tint\tA unique identifier integer of this point within this map. frame_tracking_timestamp_us\tint\tAria device timestamp of the host frame’s center of exposure, in microsecond camera_serial\tstring\tThe serial number of the camera which observes this point u\tfloat\tThe sub-pixel-accuracy observed measurement of the point in pixels, in the observing frame’s camera. v\tfloat\tThe sub-pixel-accuracy observed measurement of the point in pixels, in the observing frame’s camera. "},{"title":"MPS output - Trajectory","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/mps/mps_trajectory","content":"","keywords":""},{"title":"Open loop trajectory​","type":1,"pageTitle":"MPS output - Trajectory","url":"/projectaria_tools/docs/data_formats/mps/mps_trajectory#open-loop-trajectory","content":"Open loop trajectory is the high frequency (IMU rate, which is 1kHz) odometry estimation output by the visual-inertial odometry (VIO), in an arbitrary odometry coordinate frame. The estimation includes pose and dynamics (translational and angular velocities). The open loop trajectory has good “relative” and “local” accuracy: the relative transformation between two poses is accurate when the time span between two frames is short (within a few minutes). However, the open loop trajectory has increased drift error accumulated over time spent and travel distance. Consider using closed loop trajectory if you are looking for trajectory without drift error. For the utility function to load the open loop trajectory in Python and C++, please check the code examples Column\tType\tDescriptiontracking_timestamp_us\tint\tAria device timestamp in microseconds utc_timestamp_ns\tint\tWall clock UTC time in nanoseconds. If not available, the value will be -1 session_uid\tstring\tUnique identifier of the odometry coordinate frame. When the session_uid is the same, poses and velocities are defined in the same coordinate frame. {tx,ty,tz,qx,qy,qz,qw}_odometry_device\tfloat\tPose of the device coordinate frame in odometry frame T_odometry_device, include translation (tx, ty, tz) in meters and rotation quaternion (qx, qy, qz, qw) device_linear_velocity_{x,y,z}_odometry\tfloat\tVelocity of device coordinate frame in odometry frame, (x, y, z) in meter/s angular_velocity_{x,y,z}_device\tfloat\tAngular velocity of device coordinate frame in device frame, (x, y, z) in rad/s gravity_{x,y,z}_odometry\tfloat\tEarth gravity vector in odometry frame, (x, y, z) in meter/s^2. This vector is pointing toward the ground, and includes gravitation and centrifugal forces from earth rotation. quality_score\tfloat\tA quality score between 0.0 to 1.0. The larger the score is, the higher confidence the estimation has higher quality "},{"title":"Closed loop trajectory​","type":1,"pageTitle":"MPS output - Trajectory","url":"/projectaria_tools/docs/data_formats/mps/mps_trajectory#closed-loop-trajectory","content":"Closed loop trajectory is the high frequency (IMU rate, which is 1kHz) pose estimation output by our mapping process, in an arbitrary gravity aligned world coordinate frame. The estimation includes pose and dynamics (translational and angular velocities). Closed loop trajectories are fully bundle adjusted with detected loop closures, reducing the VIO drift which is present in the open loop trajectories. However, due to the loop closure correction, the “relative” and “local” trajectory accuracy within a short time span (i.e. seconds) might be worse compared to open loop trajectories. In some open datasets we also share and use this format for trajectory pose ground truth from simulation or Optitrack, and the files will be called in a different file name aria_gt_trajectory.csv. For the utility function to load the closed loop trajectory in Python and C++, please check the code examples Column\tType\tDescriptiongraph_uid\tstring\tUnique identifier of the world coordinate frame tracking_timestamp_us\tint\tAria device timestamp in microsecond utc_timestamp_ns\tint\tWall clock UTC time in nanosecond. If not available, the value will be -1 {tx,ty,tz,qx,qy,qz,qw}_world_device\tfloat\tPose of the device coordinate frame in world frame T_world_device, translation (tx, ty, tz) in meters and rotation quaternion (qx, qy, qz, qw) device_linear_velocity_{x,y,z}_device\tfloat\tVelocity of device coordinate frame in device frame, (x, y, z) in meter/s. angular_velocity_{x,y,z}_device\tfloat\tAngular velocity of device coordinate frame in device frame, (x, y, z) in rad/s gravity_{x,y,z}_world\tfloat\tGravity vector (x, y, z) in the world frame, in meter/s^2. MPS output will all have fixed value` [0, 0, -9.81]’, while other source (e.g. simulation or Optitrack ground truth) may give different values quality_score\tfloat\tA quality score between 0.0 to 1.0. The larger the score is, the higher confidence the estimation has higher quality` "},{"title":"Online calibration​","type":1,"pageTitle":"MPS output - Trajectory","url":"/projectaria_tools/docs/data_formats/mps/mps_trajectory#online-calibration","content":"JSON files contain one json online calibration record per line. Each record is a json dict object that contains timestamp metadata and the result of online calibration for the cameras and IMUs. The calibration parameters contain intrinsics) and extrinsics parameters for each sensor as well as a time offsets which best temporally align their data. For how to load and read online calibrations in Python and C++, please checkout the code examples "},{"title":"Static camera calibration​","type":1,"pageTitle":"MPS output - Trajectory","url":"/projectaria_tools/docs/data_formats/mps/mps_trajectory#static-camera-calibration","content":"Poses and intrinsic calibration of a set of stationary cameras. For the utility function to load the static cameras in Python and C++, please check the code examples Column\tType\tDescriptioncam_uid\tstring\tUnique identifier of camera graph_uid\tstring\tUnique identifier of the world coordinate frame {tx,ty,tz,qx,qy,qz,qw}_world_cam\tfloat\tPose of the camera coordinate frame in world frame T_world_cam, translation (tx, ty, tz) in meters and rotation quaternion (qx, qy, qz, qw) image_width\tint\tImage size in pixels image_height\tint\tImage size in pixels intrinsics_type\tstring\tCamera intrinsics calibration type. Currently support types: KANNALABRANDTK3: KB3 model intrinsics_{0-7}\tfloat\tCamera intrinsics parameters start_frame_idx\tint\tThe start frame number of the video is stationary, and camera pose and intrinsic calibration results are applicable. Both will be -1 if the pose and intrinsic calibration are applicable to the whole video. end_frame_idx\tint\tThe end frame number of the video is stationary, and camera pose and intrinsic calibration results are applicable. Both will be -1 if the pose and intrinsic calibration are applicable to the whole video. "},{"title":"Overview","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities","content":"Overview Project Aria Tools provides Python and C++ APIs. This section provides a detailed walk through of installation, visualization, code tutorials and snippets. Getting started A quickstart to install Project Aria Tools using a Python package followed by tutorials in Jupyter notebook. It contains: Dataprovider quickstart tutorial: a walk-through of accessing sensor data from VRS file, obtaining sensor calibrations and accessing project/unproject functionalities, undistorting an image, etc.Machine Perception Services (MPS) quickstart tutorial: How to visualize MPS derived data (gaze, trajectory, and point cloud) Visualizers In this tutorial we introduce binaries for visualizing Aria data Aria viewer: Visualize raw Aria dataMPS Static scene visualizer: Visualize Aria data with trajectories, global point cloud, and static camera poses Installation guide Various installation processes for Project Aria Tools API in Python and C++How to add CMake to your projectsInstall and Build Troubleshooting Core Code Snippets Python and C++ code snippets for Project Aria Tools core functionality Data Provider: open and load Aria raw data (VRS files)Image: Access and manage Aria imagesCalibration: Access device, 6DoF and sensor calibrationMPS: How to work with Aria derived data generated by Project Aria's Machine Perception Services Advanced Code Snippets Plotting Sensor Data (Python) Save images as PNGPlot the raw sensor data of a VRS file and store the plots in PDF files Image Utilities (Python and C++)","keywords":""},{"title":"Image Utilities (Python and C++)","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/image_utilities","content":"","keywords":""},{"title":"Image debayer​","type":1,"pageTitle":"Image Utilities (Python and C++)","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/image_utilities#image-debayer","content":"Some recording profiles outputs raw RGB images (Profile 7 in Recording Profile). We provide functionalities to debayer them and perform white-balancing to get RGB images. PythonC++ from projectaria_tools.core import data_provider, image stream_id = provider.get_stream_id_from_label(&quot;camera-rgb&quot;) image_data = provider.get_image_data_by_index(stream_id, 0) image_data_array = image_data[0].to_numpy_array() debayered_array = image.debayer(image_data_array)   See projectaria_tools/core/image/utility/Debayer.cpp for implementation "},{"title":"Image undistortion​","type":1,"pageTitle":"Image Utilities (Python and C++)","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/image_utilities#image-undistortion","content":"In this example, we remove distortions in raw sensor data so that straight 3D lines appear straight in the undistorted images. There is existing C++ implementation and python wrapper of this helper function in the data utilities. PythonC++ from projectaria_tools.core import data_provider, calibration camera_label = &quot;camera-slam-left&quot; stream_id = provider.get_stream_id_from_label(camera_label) calib = provider.get_device_calibration().get_camera_calib(camera_name) pinhole = calibration.get_linear_camera_calibration(512, 512, 150) raw_image = provider.get_image_data_by_index(stream_id, 0)[0].to_numpy_array() undistorted_image = calibration.distort_by_calibration(raw_image, pinhole, calib)   See projectaria_tools/core/calibration/utility/Distort.cpp for implementation. "},{"title":"Calibration","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration","content":"","keywords":""},{"title":"Accessing device calibration​","type":1,"pageTitle":"Calibration","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#accessing-device-calibration","content":"Device calibration stores: The device's CAD model, which contains the 6DoF poses of sensors of the device as designed.The calibration of all sensors on a single Aria device. See the Accessing sensor calibration section for details.The device's sub-type (DVT-S or DVT-L to indicate small or large) PythonC++ from projectaria_tools.core import data_provider, calibration from projectaria_tools.core.stream_id import StreamId vrsfile = &quot;example.vrs&quot; provider = data_provider.create_vrs_data_provider(vrsfile) # returns None if vrs does not have a calibration device_calib = provider.get_device_calibration() print(device_calib.get_device_subtype())  "},{"title":"Accessing 6DoF poses of sensors​","type":1,"pageTitle":"Calibration","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#accessing-6dof-poses-of-sensors","content":"All 6DoF poses (a.k.a. extrinsic parameters) are represented as relative to the device frame. The device frame is a specific sensor frame, identified by the sensor's label. Aria device frame is by default camera-slam-left. We also provide the pose of the central-pupil-frame in the device frame or as relative to a sensor frame. PythonC++ label = &quot;camera-slam-right&quot; transform_device_sensor = device_calib.get_transform_device_sensor(label) transform_device_cpf = device_calib.get_transform_device_cpf() transform_cpf_sensor = device_calib.get_transform_cpf_sensor(label)  "},{"title":"Accessing sensor calibration​","type":1,"pageTitle":"Calibration","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#accessing-sensor-calibration","content":"Each sensor on the device may have a corresponding stream in the vrs and may have a corresponding calibration. However, some types of sensors may not have calibration defined for them (e.g. GPS, WPS, bluetooth), and some sensors may not record stream in a specific vrs. For sensor streams where calibration is available, they can be accessed by labels: PythonC++ # returns None if vrs does not have a calibration device_calib = provider.get_device_calibration() sensor_calib = device_calib.get_sensor_calib(label) More conveniently, you can just do stream_id = StreamId(&quot;1201-1&quot;) calib = provider.get_sensor_calibration(stream_id) If you know the calibration type, you can also do # returns None if the calibration label does not exist cam_calib = device_calib.get_camera_calib(&quot;camera-rgb&quot;); imu_calib = device_calib.get_imu_calib(&quot;imu-left&quot;);  Accessing ET and Microphone calibration​ Note Aria's ET camera stream and audio are special types: Aria's ET stream switches the stream for left and right ET together, thus its calibration is a pair of CameraCalibration.Aria's Audio stream has 7 channels, thus its calibration is an array of seven microphone calib. PythonC++ # returns None if the calibration label does not exist et_calib = device_calib.get_aria_et_camera_calib() print(et_calib[0].get_label()) mic_calib = device_calib.get_aria_microphone_calib() print(mic_calib[0].get_label())  "},{"title":"Data Provider","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider","content":"","keywords":""},{"title":"Open a VRS file​","type":1,"pageTitle":"Data Provider","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider#open-a-vrs-file","content":"PythonC++ from projectaria_tools.core import data_provider from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions from projectaria_tools.core.stream_id import RecordableTypeId, StreamId vrsfile = &quot;example.vrs&quot; provider = data_provider.create_vrs_data_provider(vrsfile) assert provider is not None, &quot;Cannot open file&quot;  "},{"title":"Mapping between labels and stream ids​","type":1,"pageTitle":"Data Provider","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider#mapping-between-labels-and-stream-ids","content":"PythonC++ Stream ids can be mapped from labels by using get_stream_id_from_label stream_id = provider.get_stream_id_from_label(&quot;camera-slam-left&quot;) Inversely, you can retrieve a label from stream id by using get_stream_id_from_label label = provider.get_label_from_stream_id(StreamId(&quot;1201-1&quot;))  "},{"title":"Random access data by index​","type":1,"pageTitle":"Data Provider","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider#random-access-data-by-index","content":"PythonC++ for stream_id in provider.get_all_streams(): for i in range(0, provider.get_num_data(stream_id)): sensor_data = provider.get_sensor_data_by_index(stream_id, i)  "},{"title":"Random access data by timestamp​","type":1,"pageTitle":"Data Provider","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider#random-access-data-by-timestamp","content":"PythonC++ We support four time domains: TimeDomain.RECORD_TIMETimeDomain.DEVICE_TIMETimeDomain.HOST_TIMETimeDomain.TIME_CODE You can also search using three different time query options: TimeQueryOptions.BEFORE (default): last data with t &lt;= t_queryTimeQueryOptions.AFTER : first data with t &gt;= t_queryTimeQueryOptions.CLOSEST : the data where |t - t_query| is smallest for stream_id in provider.get_all_streams(): t_first = provider.get_first_time_ns(stream_id, TimeDomain.DEVICE_TIME) t_last = provider.get_last_time_ns(stream_id, TimeDomain.DEVICE_TIME) query_timestamp = (t_first + t_last) // 2 # example query timestamp sensor_data = provider.get_sensor_data_by_time_ns(stream_id, query_timestamp, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST)  "},{"title":"Deliver all sensor data in VRS​","type":1,"pageTitle":"Data Provider","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider#deliver-all-sensor-data-in-vrs","content":"PythonC++ Async iterator to deliver sensor data for all streams in device time order. for data in provider.deliver_queued_sensor_data(): print(data.get_time_ns(TimeDomain.DEVICE_TIME)) Alternatively can use iterator-type syntax seq = provider.deliver_queued_sensor_data() iter_data = iter(seq) while True: print(iter_data.get_time_ns(TimeDomain.DEVICE_TIME)) try: next(iter_data) except StopIteration: break Deliver with sub-stream selection, time truncation, and frame rate subsampling: # Starts by default options which activates all sensors deliver_option = provider.get_default_deliver_queued_options() # Only play data from two cameras, also reduce framerate to half of vrs deliver_option.deactivate_stream_all() for label in [&quot;camera-slam-left&quot;, &quot;camera-slam-right&quot;]: streamId = provider.get_stream_id_from_label(label) deliver_option.activate_stream(streamId) deliver_option.set_subsample_rate(streamId, 2) # skip first 100ns deliver_option.set_truncate_first_device_time_ns(100) for data in provider.deliver_queued_sensor_data() : print(data.get_time_ns(TimeDomain.DEVICE_TIME))  "},{"title":"Image","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/image","content":"","keywords":""},{"title":"Raw sensor data​","type":1,"pageTitle":"Image","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/image#raw-sensor-data","content":"Raw image data is stored in ImageData. ImageData is a type alias of an std::pair. The two components of that pair are: The image frame stored in vrs::PixelFrame class (potentially compressed) We recommend that users do not directly use PixelFrame Image data records Image acquisition information such as timestamps, exposure and gain PythonC++ from projectaria_tools.core import data_provider, image from projectaria_tools.core.stream_id import StreamId vrsfile = &quot;example.vrs&quot; provider = data_provider.create_vrs_data_provider(vrsfile) stream_id = provider.get_stream_id_from_label(&quot;camera-slam-left&quot;) image_data = provider.get_image_data_by_index(stream_id, 0) pixel_frame = image_data[0].pixel_frame  "},{"title":"Manipulating images​","type":1,"pageTitle":"Image","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/image#manipulating-images","content":"PythonC++ In Python, we provide an interface for converting from ImageData into numpy arrays. image_array = image_data[0].to_numpy_array()  "},{"title":"MPS","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps","content":"","keywords":""},{"title":"Load MPS output​","type":1,"pageTitle":"MPS","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps#load-mps-output","content":"The loaders for MPS output (projectaria_tools/main/core/mps) provide a convenient way to quickly load the MPS output in a few lines of code into data structures that can then be used downstream. Please refer to the MPS data schema wiki page to learn more about the specifics of what each MPS output consists of. Here, we will focus only on the loading APIs in python and C++. "},{"title":"Open loop/Closed loop trajectory​","type":1,"pageTitle":"MPS","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps#open-loopclosed-loop-trajectory","content":"PythonC++ import projectaria_tools.core.mps as mps open_loop_path = &quot;/path/to/mps/output/trajectory/open_loop_trajectory.csv&quot; open_loop_traj = mps.read_open_loop_trajectory(open_loop_path) closed_loop_path = &quot;/path/to/mps/output/trajectory/closed_loop_trajectory.csv&quot; closed_loop_traj = mps.read_closed_loop_trajectory(closed_loop_path) # example: get transformation from this device to world coordinate frame for closed_loop_pose in closed_loop_traj: transform_world_device = closed_loop_pose.transform_world_device  "},{"title":"Point cloud​","type":1,"pageTitle":"MPS","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps#point-cloud","content":"Always filter global point clouds in 3D Post-filtering the point cloud using inverse distance and distance certainty is required to get point cloud accurate in 3D space. There are points cannot be accurately estimated in 3D space due to low parallax, but those points are well tracked in 2D images, and produce valid 2D observations. We choose to output all the points, include those have poor 3D estimations, in case researchers need. Check semi-dense point cloud page for more information. Loading observations could be slow When the Aria recording is long, loading point observations could be memory and time consuming (&gt; 1 minute). A typical 20 minutes long Aria recording will have roughly total 10+ millions of 3D points with total 100+ millions of 2D observations. PythonC++ import projectaria_tools.core.mps as mps global_points_path = &quot;/path/to/mps/output/trajectory/global_points.csv.gz&quot; points = mps.read_global_point_cloud(global_points_path, mps.StreamCompressionMode.GZIP) # filter the point cloud by inverse depth and depth filtered_points = [] for point in points: if (point.inverse_distance_std &lt; 0.001 and point.distance_std &lt; 0.15): filtered_points.append(point) # example: get position of this point in the world coordinate frame for point in filtered_points: position_world = point.position_world observations_path = &quot;/path/to/mps/output/trajectory/semidense_observations.csv.gz&quot; observations = mps.read_point_observations(observations_path, mps.StreamCompressionMode.GZIP)  "},{"title":"Online calibration​","type":1,"pageTitle":"MPS","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps#online-calibration","content":"PythonC++ import projectaria_tools.core.mps as mps online_calib_path = &quot;/path/to/mps/output/trajectory/online_calibration.jsonl&quot; online_calibs = mps.read_online_calibration(online_calib_path) for calib in online_calibs: # example: get left IMU's online calibration for imuCalib in calib.imu_calibs: if imuCalib.get_label() == &quot;imu-left&quot;: leftImuCalib = imuCalib # example: get left SLAM camera's online calibration for camCalib in calib.camera_calibs: if camCalib.get_label() == &quot;camera-slam-left&quot;: leftCamCalib = camCalib  "},{"title":"Eye gaze​","type":1,"pageTitle":"MPS","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps#eye-gaze","content":"PythonC++ import projectaria_tools.core.mps as mps gaze_path = &quot;/path/to/mps/output/eye_gaze/eyegaze.csv&quot; gaze_cpf = mps.read_eye_gaze(eye_gaze_path) # project the 3D gaze point assume depth is 1.0 meter depth_m = 1.0 gaze_Point_cpf = mps.get_eyegaze_point_at_depth(gaze_cpf[0].yaw, gaze_cpf[0].pitch, depth_m)  "},{"title":"Static camera calibration​","type":1,"pageTitle":"MPS","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps#static-camera-calibration","content":"PythonC++ import projectaria_tools.core.mps as mps static_cameras_path = &quot;/path/to/mps/output/trajectory/static_cam_calibs.csv&quot; static_cameras = mps.read_static_camera_calibrations(static_cameras_path)  "},{"title":"Plotting sensor data (Python)","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data","content":"","keywords":""},{"title":"Save Images as PNGs​","type":1,"pageTitle":"Plotting sensor data (Python)","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#save-images-as-pngs","content":"Because we support converting image data to numpy arrays, images can be converted to PIL images and saved as PNG files. from PIL import Image stream_mappings = { &quot;camera-slam-left&quot;: StreamId(&quot;1201-1&quot;), &quot;camera-slam-right&quot;: StreamId(&quot;1201-2&quot;), &quot;camera-rgb&quot;: StreamId(&quot;214-1&quot;), &quot;camera-eyetracking&quot;: StreamId(&quot;211-1&quot;), } for [stream_name, stream_id] in stream_mappings.items(): image = provider.get_image_data_by_index(stream_id, index) Image.fromarray(image[0].to_numpy_array()).save(f'{stream_name}.png')  The above snippets will save the following images to the local folder: SLAM images Eye Tracking images RGB images\t "},{"title":"Plotting IMU​","type":1,"pageTitle":"Plotting sensor data (Python)","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#plotting-imu","content":"Organize the data into 6 lists. Each list stores one axis of a specific IMU. stream_id = provider.get_stream_id_from_label(&quot;imu-left&quot;) accel_x = [] accel_y = [] accel_z = [] gyro_x = [] gyro_y = [] gyro_z = [] timestamps = [] for index in range(0, provider.get_num_data(stream_id)): imu_data = provider.get_imu_data_by_index(stream_id, index) accel_x.append(imu_data.accel_msec2[0]) accel_y.append(imu_data.accel_msec2[1]) accel_z.append(imu_data.accel_msec2[2]) gyro_x.append(imu_data.gyro_radsec[0]) gyro_y.append(imu_data.gyro_radsec[1]) gyro_z.append(imu_data.gyro_radsec[2]) timestamps.append(imu_data.capture_timestamp_ns * 1e-9)  Plot the data with matplotlib plt.figure() fig, axes = plt.subplots(1, 2, figsize=(12, 5)) fig.suptitle(f&quot;{stream_id.get_name()}&quot;) axes[0].plot(timestamps, accel_x, 'r-', label=&quot;x&quot;) axes[0].plot(timestamps, accel_y, 'g-', label=&quot;y&quot;) axes[0].plot(timestamps, accel_z, 'b-', label=&quot;z&quot;) axes[0].legend(loc='upper left') axes[0].grid('on') axes[0].set_xlabel('timestamps (s)') axes[0].set_ylabel('accelerometer readout (m/sec2)') axes[1].plot(timestamps, gyro_x, 'r-', label=&quot;x&quot;) axes[1].plot(timestamps, gyro_y, 'g-', label=&quot;y&quot;) axes[1].plot(timestamps, gyro_z, 'b-', label=&quot;z&quot;) axes[1].legend(loc='upper left') axes[1].grid('on') axes[1].set_xlabel('timestamps (s)') axes[1].set_ylabel('gyroscope readout (rad/sec)')  The plotted image looks like this: Save the plot to PDF plt.savefig(&quot;imu.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)  "},{"title":"Magnetometer​","type":1,"pageTitle":"Plotting sensor data (Python)","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#magnetometer","content":"Plotting magnetometer is similar to plotting IMU. Organize the data into 3 lists. Each list stores one axis of magnetometer data. stream_id = provider.get_stream_id_from_label(&quot;mag0&quot;) mag_x = [] mag_y = [] mag_z = [] timestamps = [] for index in range(0, provider.get_num_data(stream_id)): mag_data = provider.get_magnetometer_data_by_index(stream_id, index) mag_x.append(mag_data.mag_tesla[0] * 1e6) mag_y.append(mag_data.mag_tesla[1] * 1e6) mag_z.append(mag_data.mag_tesla[2] * 1e6) timestamps.append(mag_data.capture_timestamp_ns * 1e-9)  Plot the data with matplotlib plt.figure() fig, axes = plt.subplots(1, 1, figsize=(12, 5)) fig.suptitle(f&quot;Magnetometer signal&quot;) axes.plot(timestamps, mag_x, 'r-', label=&quot;x&quot;) axes.plot(timestamps, mag_y, 'g-', label=&quot;y&quot;) axes.plot(timestamps, mag_z, 'b-', label=&quot;z&quot;) axes.legend(loc='upper left') axes.grid('on') axes.set_xlabel('timestamps (s)') axes.set_ylabel('magnetometer readout (uT)') plt.savefig(&quot;mag.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)   "},{"title":"Audio​","type":1,"pageTitle":"Plotting sensor data (Python)","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#audio","content":"Audio data is interesting since each data is in fact a 7x4096 chunk Load the audio data stream_id = provider.get_stream_id_from_label(&quot;mic&quot;) timestamps = [] audio = [[] for c in range(0, 7)] for index in range(0, 2): audio_data_i = provider.get_audio_data_by_index(stream_id, index) audio_signal_block = audio_data_i[0].data timestamps_block = [t * 1e-9 for t in audio_data_i[1].capture_timestamps_ns]; timestamps += timestamps_block for c in range(0, 7): audio[c] += audio_signal_block[c::7]  Plot the data with matplotlib plt.figure() fig, axes = plt.subplots(1, 1, figsize=(12, 5)) fig.suptitle(f&quot;Microphone signal&quot;) for c in range(0, 7): plt.plot(timestamps, audio[c], '-', label = f&quot;channel {c}&quot;) axes.legend(loc='upper left') axes.grid('on') axes.set_xlabel('timestamps (s)') axes.set_ylabel('audio readout') plt.savefig(&quot;audio.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)   "},{"title":"Barometer​","type":1,"pageTitle":"Plotting sensor data (Python)","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#barometer","content":"Load and plot the data using the following commands plt.figure() fig, axes = plt.subplots(1, 2, figsize=(12, 5)) fig.suptitle(f&quot;Barometer signal&quot;) stream_id = provider.get_stream_id_from_label(&quot;baro0&quot;) pressure = [] temperature = [] timestamps = [] for index in range(0, provider.get_num_data(stream_id)): baro_data = provider.get_barometer_data_by_index(stream_id, index) pressure.append(baro_data.pressure * 1e-3) temperature.append(baro_data.temperature) timestamps.append(baro_data.capture_timestamp_ns * 1e-9) axes[0].plot(timestamps, pressure, 'r-') axes[0].grid('on') axes[0].set_xlabel('timestamps (s)') axes[0].set_ylabel('pressure readout (kPascal)') axes[1].plot(timestamps, temperature, 'r-') axes[1].grid('on') axes[1].set_xlabel('timestamps (s)') axes[1].set_ylabel('temperature readout (C)') plt.savefig(&quot;baro.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)   "},{"title":"GPS​","type":1,"pageTitle":"Plotting sensor data (Python)","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#gps","content":"GPS data can be visualized with 2D or 3D plots. 2D plots​ plt.figure() fig, axes = plt.subplots(1, 3, figsize=(12, 3)) fig.suptitle(f&quot;GPS signal&quot;) stream_id = provider.get_stream_id_from_label(&quot;gnss&quot;) latitude = [] longitude = [] altitude = [] timestamps = [] for index in range(100, 300): gps_data = provider.get_gps_data_by_index(stream_id, index) latitude.append(gps_data.latitude) longitude.append(gps_data.longitude) altitude.append(gps_data.altitude) timestamps.append(gps_data.capture_timestamp_ns * 1e-9) ax = axes[0] ax.plot(timestamps, latitude, 'r-') ax.grid('on') ax.set_xlabel('timestamps (s)') ax.set_ylabel('latitude') ax.yaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True, useOffset=False)) ax = axes[1] ax.plot(timestamps, longitude, 'r-') ax.grid('on') ax.set_xlabel('timestamps (s)') ax.set_ylabel('longitude') ax.yaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True, useOffset=False)) ax = axes[2] ax.plot(timestamps, altitude, 'r-') ax.grid('on') ax.set_xlabel('timestamps (s)') ax.set_ylabel('altitude') ax.yaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True, useOffset=False)) fig.tight_layout() plt.savefig(&quot;gps.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)   3D plots​ plt.figure() fig = plt.figure() axes = fig.add_subplot(projection='3d') axes.plot(latitude, longitude, altitude) axes.view_init(elev=20., azim=-35, roll=0) plt.savefig(&quot;gps3d.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)   "},{"title":"Wi-Fi beacon​","type":1,"pageTitle":"Plotting sensor data (Python)","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#wi-fi-beacon","content":"Group the Wi-Fi beacon data by mac bssid stream_id = provider.get_stream_id_from_label(&quot;wps&quot;) rssi = {} timestamps = {} print(provider.get_num_data(stream_id)) for index in range(0, provider.get_num_data(stream_id)): wps_data = provider.get_wps_data_by_index(stream_id, index) if wps_data.bssid_mac not in rssi: rssi[wps_data.bssid_mac] = [] timestamps[wps_data.bssid_mac] = [] rssi[wps_data.bssid_mac].append(wps_data.rssi) timestamps[wps_data.bssid_mac].append(wps_data.board_timestamp_ns * 1e-9)  Plot the mac address This example has &gt; 15 samples plt.figure() fig, ax = plt.subplots(1, 1, figsize=(6, 5)) fig.suptitle(f&quot;Wi-Fi beacon signal&quot;) for ssid in list(timestamps.keys()): if len(timestamps[ssid]) &lt; 15: continue ax.scatter(timestamps[ssid], rssi[ssid], label=ssid) ax.grid('on') ax.set_xlabel('timestamps (s)') ax.set_ylabel('Wi-Fi RSSI(dBm)') plt.legend(loc='upper left') plt.savefig(&quot;wifi.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)   "},{"title":"Bluetooth beacon​","type":1,"pageTitle":"Plotting sensor data (Python)","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#bluetooth-beacon","content":"Group data by unique_id (similar to Wi-Fi grouping) stream_id = provider.get_stream_id_from_label(&quot;bluetooth&quot;) rssi = {} timestamps = {} for index in range(0, provider.get_num_data(stream_id)): bluetooth_data = provider.get_bluetooth_data_by_index(stream_id, index) if bluetooth_data.unique_id not in rssi: rssi[bluetooth_data.unique_id] = [] timestamps[bluetooth_data.unique_id] = [] rssi[bluetooth_data.unique_id].append(bluetooth_data.rssi) timestamps[bluetooth_data.unique_id].append(bluetooth_data.board_timestamp_ns * 1e-9)  Plot the data per unique_id plt.figure() fig, ax = plt.subplots(1, 1, figsize=(6, 5)) fig.suptitle(f&quot;Bluetooth beacon signal&quot;) for ssid in list(timestamps.keys()): ax.plot(timestamps[ssid], rssi[ssid], '.') ax.grid('on') ax.set_xlabel('timestamps (s)') ax.set_ylabel('bluetooth RSSI(dBm') fig.tight_layout() plt.savefig(&quot;ble.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)   "},{"title":"Getting Started with Aria Data Utilities","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/getting_started","content":"","keywords":""},{"title":"Step 0 : Check system requirements and download codebase​","type":1,"pageTitle":"Getting Started with Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-0--check-system-requirements-and-download-codebase","content":"Go to the Download Codebase page to: Check your system is supportedDownload projectaria_tools codebase from the github "},{"title":"Step 1 : Install/Update Python 3​","type":1,"pageTitle":"Getting Started with Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-1--installupdate-python-3","content":"To use the Jupyter notebooks in this tutorial you'll need Python 3.9 or above. To ensure all utilities work effectively, we recommend keeping Python 3 up to date. Python 3 download pageTo check what what version of Python 3 you have use python3 --version "},{"title":"Step 2 : Create a virtual environment​","type":1,"pageTitle":"Getting Started with Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-2--create-a-virtual-environment","content":"rm -rf $HOME/projectaria_tools_python_env python3 -m venv $HOME/projectaria_tools_python_env source $HOME/projectaria_tools_python_env/bin/activate  "},{"title":"Step 3 : Install projectaria_tools from pypi​","type":1,"pageTitle":"Getting Started with Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-3--install-projectaria_tools-from-pypi","content":"pip3 install --upgrade pip pip3 install projectaria-tools'[all]'  "},{"title":"Step 4: Run Dataprovider quickstart tutorial​","type":1,"pageTitle":"Getting Started with Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-4-run-dataprovider-quickstart-tutorial","content":"cd $HOME/Documents/projectaria_sandbox jupyter notebook projectaria_tools/core/examples/dataprovider_quickstart_tutorial.ipynb  "},{"title":"Step 5: Run Machine Perception Services (MPS) quickstart tutorial​","type":1,"pageTitle":"Getting Started with Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-5-run-machine-perception-services-mps-quickstart-tutorial","content":"In the MPS tutorial, the notebook walks through how to visualize gaze, trajectory, and point cloud from MPS data. cd $HOME/Documents/projectaria_sandbox jupyter notebook projectaria_tools/core/examples/mps_quickstart_tutorial.ipynb  "},{"title":"Troubleshooting​","type":1,"pageTitle":"Getting Started with Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#troubleshooting","content":"Check the Troubleshooting Guide if you encounter issues using this tutorial. "},{"title":"Other Useful Links​","type":1,"pageTitle":"Getting Started with Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#other-useful-links","content":"TroubleshootingInstallation guideVisualizers "},{"title":"CMake for your projects","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/installation/build_with_cmake","content":"","keywords":""},{"title":"Example code​","type":1,"pageTitle":"CMake for your projects","url":"/projectaria_tools/docs/data_utilities/installation/build_with_cmake#example-code","content":"Please refer to the sample project for a full example. Install targets are coming soon! So that you can install projectaria_tools and access it using find_package() in your CMakeLists.txt "},{"title":"Download Codebase","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/installation/download_codebase","content":"","keywords":""},{"title":"Supported Platforms​","type":1,"pageTitle":"Download Codebase","url":"/projectaria_tools/docs/data_utilities/installation/download_codebase#supported-platforms","content":"The codebase has been tested on the following platforms. Fedora 36,37,38 recommendedMac Intel / Mac ARM-based (M1)Ubuntu focal (20.04 LTS) and jammy (22.04 LTS) "},{"title":"Download codebase​","type":1,"pageTitle":"Download Codebase","url":"/projectaria_tools/docs/data_utilities/installation/download_codebase#download-codebase","content":"mkdir -p $HOME/Documents/projectaria_sandbox cd $HOME/Documents/projectaria_sandbox git clone https://github.com/facebookresearch/projectaria_tools.git  "},{"title":"Python Package Installation","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/installation/installation_python","content":"","keywords":""},{"title":"Install via virtual environment​","type":1,"pageTitle":"Python Package Installation","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#install-via-virtual-environment","content":""},{"title":"Step 1 : Install Python​","type":1,"pageTitle":"Python Package Installation","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#step-1--install-python","content":"To use the Jupyter notebooks featured in our documentation you'll need Python 3.9 or above. To ensure all utilities work effectively, we recommend keeping Python 3 up to date. Python 3 download pageTo check what what version of Python 3 you have use python3 --version "},{"title":"Step 2 : Create a virtual environment​","type":1,"pageTitle":"Python Package Installation","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#step-2--create-a-virtual-environment","content":"rm -rf $HOME/projectaria_tools_python_env python3 -m venv $HOME/projectaria_tools_python_env source $HOME/projectaria_tools_python_env/bin/activate  "},{"title":"Step 3 : Install the required Python packages​","type":1,"pageTitle":"Python Package Installation","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#step-3--install-the-required-python-packages","content":"pip3 install --upgrade pip pip3 install projectaria-tools'[all]'  "},{"title":"Step 4 : Verify installation​","type":1,"pageTitle":"Python Package Installation","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#step-4--verify-installation","content":"Verify the Python package installed correctly by running through the two example tutorials in Jupyter notebook. cd $HOME/Documents/projectaria_sandbox # Basic VRS data access tutorial jupyter notebook projectaria_tools/core/examples/dataprovider_quickstart_tutorial.ipynb # Machine Perception Service (MPS) tutorial jupyter notebook projectaria_tools/core/examples/mps_quickstart_tutorial.ipynb  Python Module Error? Check the Python module import error section of Data Utilities Troubleshooting Guide if you encounter this issue Building Python bindings from source (advanced user) You'll need to install C++ dependencies to build Python bindings from source. Go to the C++ Installation page and follow the instructions to install dependenciesGo to the projectaria_tools code folderEnter the following commands cd $HOME/Documents/projectaria_sandbox/projectaria_tools pip3 install --upgrade pip pip3 install --global-option=build_ext --config-settings=compile-args=&quot;-j2&quot; .;  "},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting","content":"","keywords":""},{"title":"Jupyter notebook error​","type":1,"pageTitle":"Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#jupyter-notebook-error","content":"Jupyter notebook works with Python 3.9 or above. If you have problems using Jupyter examples, please upgrade Python 3 to the latest version. If you are using a virtual environment you'll need to recreate it to bring in the update. "},{"title":"Python module import error​","type":1,"pageTitle":"Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#python-module-import-error","content":"There are several things that could cause this error message. Python version mismatch​ When running Jupyter notebook, it might use a Python 3 version that's not in the virtual environment. There are two ways you can resolve this issue. Remove the Jupyter notebook from outside of the virtual environmentDirectly start the Jupyter notebook from the virtual environment bin folder. If the virtual environment was created using python3 -m venv $HOME/projectaria_tools_python_env, you can directly call Jupyter from the virtual env as $HOME/projectaria_tools_python_env/bin/jupyter notebook notebook_example.ipynb  Old version of projectaria_tools​ You may also encounter a Python module import error if you are running an old version of projectaria_tools. Make sure you've installed the latest version of projectaria_tools. "},{"title":"Visualizer issues​","type":1,"pageTitle":"Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#visualizer-issues","content":""},{"title":"Visualizer does not build​","type":1,"pageTitle":"Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#visualizer-does-not-build","content":"If the visualizer does not build it may be because of missing Pangolin functions. Aria Digital Twin (ADT) dataset depends on very recent changes to Pangolin's master branch. If ADT depends on Pangolin functions that are not available on your installed version of Pangolin, please reinstall using the most recent master. "},{"title":"Runtime errors/missing libraries​","type":1,"pageTitle":"Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#runtime-errorsmissing-libraries","content":"Runtime errors can be caused by missing libraries. The following commands may resolve the issue. # Missing libpango_geometry.so, libpango_windowing.so, etc sudo ldconfig LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib/ export LD_LIBRARY_PATH  "},{"title":"Visualizer Window freezes - X11 known issue​","type":1,"pageTitle":"Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#visualizer-window-freezes---x11-known-issue","content":"If you are running a platform that uses X11 the Visualizer window may freeze. This is most likely because of a graphics driver bug in X11.Pangolin has a discussion on the issue. If the issue is triggered by Pangolin Plotter, the fix is swap from X11 to EGL. Step 1: Check the cause​ Test to see if it's a display driver issue triggered by Pangolin Plotter. Build the latest version of Pangolin cd /tmp/Pangolin_Build cmake -DCMAKE_BUILD_TYPE=Release -DBUILD_TOOLS=OFF -DBUILD_PANGOLIN_PYTHON=OFF \\ -DBUILD_EXAMPLES=ON ../Pangolin/ make -j2  Run the following example, it should work without issues ./examples/BasicOpenGl/tutorial_3_gl_intro_classic_triangle_vbo_shader  Run the following example, if it shows a black window and the machine freezes, this may be the graphics driver issue. Move on to Step 2. ./examples/SimplePlot/SimplePlot  Step 2: Checkout the fix rebase onto master​ Use the following commands to checkout the fix on github and rebase onto master cd /tmp/Pangolin git fetch origin pull/389/head:x11_to_egl git checkout x11_to_egl git rebase master # rebuild pangolin cd /tmp/Pangolin_Build cmake -DCMAKE_BUILD_TYPE=Release -DBUILD_TOOLS=OFF -DBUILD_PANGOLIN_PYTHON=OFF \\ -DBUILD_EXAMPLES=ON ../Pangolin/ make -j2 sudo make install  Confirm this is the correct fix by rebuilding and retesting SimplePlot from Step 1. Step 3: Patch CMake for Visualizers​ Update the AriaViewer CMakeLists in$HOME/Documents/projectaria_sandbox/projectaria_tools/tools/CMakeLists.txtUpdate AriaDigitalTwinViewer CMakeLists in$HOME/Documents/projectaria_sandbox/projectaria_tools/projects/AriaDigitalTwinDatasetTools/visualization/CMakeLists.txt By adding the following line: find_package(OpenGL QUIET COMPONENTS EGL)  Step 4. Rebuild Aria Viewer and validate that it works​ cd $HOME/Documents/projectaria_sandbox/build cmake ../projectaria_tools -DPROJECTARIA_TOOLS_BUILD_TOOLS=ON make -j2 ./tools/visualization/aria_viewer \\ --vrs ../projectaria_tools/data/mps_sample/sample.vrs  "},{"title":"C++ Installation","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp","content":"","keywords":""},{"title":"Build from source without visualization​","type":1,"pageTitle":"C++ Installation","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#build-from-source-without-visualization","content":""},{"title":"Step 1 : Install dependencies​","type":1,"pageTitle":"C++ Installation","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-1--install-dependencies","content":"UbuntuFedoraMacOS # Install build essentials sudo apt install build-essential git cmake # Install VRS/Pangolin dependencies sudo apt install libgtest-dev libgmock-dev libgoogle-glog-dev libfmt-dev \\ liblz4-dev libzstd-dev libxxhash-dev libboost-all-dev libpng-dev \\ libjpeg-turbo8-dev libturbojpeg0-dev libglew-dev libgl1-mesa-dev libeigen3-dev  "},{"title":"Step 2 : Compile C++ source code​","type":1,"pageTitle":"C++ Installation","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-2--compile-c-source-code","content":"cd $HOME/Documents/projectaria_sandbox mkdir -p build &amp;&amp; cd build # compile the C++ API cmake ../projectaria_tools/ make -j2   "},{"title":"Build from source with visualization​","type":1,"pageTitle":"C++ Installation","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#build-from-source-with-visualization","content":""},{"title":"Step 1 : Install dependencies​","type":1,"pageTitle":"C++ Installation","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-1--install-dependencies-1","content":"Follow the above steps to install dependencies build from source "},{"title":"Step 2 : Compile Pangolin​","type":1,"pageTitle":"C++ Installation","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-2---compile-pangolin","content":"The viewers are built using Pangolin. # compile &amp; install Pangolin cd /tmp git clone --recursive https://github.com/stevenlovegrove/Pangolin.git mkdir -p Pangolin_Build &amp;&amp; cd Pangolin_Build cmake -DCMAKE_BUILD_TYPE=Release -DBUILD_TOOLS=OFF -DBUILD_PANGOLIN_PYTHON=OFF \\ -DBUILD_EXAMPLES=OFF ../Pangolin/ make -j2 sudo make install  "},{"title":"Step 3 : Build projectaria_tools with visualization​","type":1,"pageTitle":"C++ Installation","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-3--build-projectaria_tools-with-visualization","content":"cd $HOME/Documents/projectaria_sandbox mkdir -p build &amp;&amp; cd build # Build C++ Aria Viewer cmake ../projectaria_tools -DPROJECTARIA_TOOLS_BUILD_TOOLS=ON make -j2  "},{"title":"Step 4 : Verify installation by running the viewer​","type":1,"pageTitle":"C++ Installation","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-4--verify-installation-by-running-the-viewer","content":"cd $HOME/Documents/projectaria_sandbox/build # Running the Aria Viewer with default example data ./tools/visualization/aria_viewer \\ --vrs ../projectaria_tools/data/mps_sample/sample.vrs  "},{"title":"Troubleshooting​","type":1,"pageTitle":"C++ Installation","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#troubleshooting","content":"Check the Troubleshooting Guide if you encounter any issues. "},{"title":"Visualizers","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/visualization_guide","content":"","keywords":""},{"title":"Step 0 : Check system requirements and download codebase​","type":1,"pageTitle":"Visualizers","url":"/projectaria_tools/docs/data_utilities/visualization_guide#step-0--check-system-requirements-and-download-codebase","content":"Go to the Download Codebase page to: Check your system is supportedDownload projectaria_tools codebase from the github "},{"title":"Step 1 : Build and install visualizers​","type":1,"pageTitle":"Visualizers","url":"/projectaria_tools/docs/data_utilities/visualization_guide#step-1--build-and-install-visualizers","content":"The visualizers need the C++ version of Project Aria Tools to run. In the C++ Installation Guide, follow the instructions to build from source with visualization "},{"title":"Step 2 : Run Aria Viewer​","type":1,"pageTitle":"Visualizers","url":"/projectaria_tools/docs/data_utilities/visualization_guide#step-2--run-aria-viewer","content":"Aria Viewer enable you to to visualize Aria device recorded VRS files. It shows all sensor data including: Camera imagesIMUAudio (visualization of waveform, sound is not available) cd $HOME/Documents/projectaria_sandbox/build ./tools/visualization/aria_viewer --vrs ../projectaria_tools/data/mps_sample/sample.vrs   "},{"title":"Step 3 : Run MPS 3D Scene Viewer​","type":1,"pageTitle":"Visualizers","url":"/projectaria_tools/docs/data_utilities/visualization_guide#step-3--run-mps-3d-scene-viewer","content":"The MPS 3D Scene Viewer renders a static scene using location MPS output. Through this tool you can create visualizations using: Closed loop trajectoriesGlobal point cloudStatic camera posesOpen loop trajectories Because open loop is in odometry frame of reference, it shouldn’t be visualized with closed loop trajectories, global points or static camera poses This tutorial generates a visualization containing: Closed loop trajectoriesGlobal point cloud cd $HOME/Documents/projectaria_sandbox/build ./tools/mps_visualization/mps_3d_scene_viewer \\ --closed-loop-traj \\ ../projectaria_tools/data/mps_sample/trajectory/closed_loop_trajectory.csv \\ --global-point-cloud \\ ../projectaria_tools/data/mps_sample/trajectory/global_points.csv.gz   info Because the sample dataset doesn't have static cameras you won't be able to interact with the static camera settings "},{"title":"Step 4: MPS Eye Gaze visualizer​","type":1,"pageTitle":"Visualizers","url":"/projectaria_tools/docs/data_utilities/visualization_guide#step-4-mps-eye-gaze-visualizer","content":"The MPS Eye Gaze visualizer renders the computed eye gaze and vrs data side by side. The visualizer contains: Eye Tracking camera streamRGB, Mono Scene (SLAM) left and right camera streams A red dot shows the projection of the eye gaze onto the imageThe projection is computed using a fixed depth of 1m 2D graph plot of the gaze yaw and pitch angles in radians2D radar plot of the eye gaze yaw and pitch angles cd $HOME/Documents/projectaria_sandbox/build ./tools/mps_visualization/mps_eyegaze_viewer \\ --vrs ../projectaria_tools/data/mps_sample/sample.vrs \\ --eyegaze ../projectaria_tools/data/mps_sample/eye_gaze/eyegaze.csv   "},{"title":"Troubleshooting​","type":1,"pageTitle":"Visualizers","url":"/projectaria_tools/docs/data_utilities/visualization_guide#troubleshooting","content":"Check the Troubleshooting Guide if you encounter issues using this tutorial. "},{"title":"Project Aria Tools","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/intro","content":"","keywords":""},{"title":"Further resources​","type":1,"pageTitle":"Project Aria Tools","url":"/projectaria_tools/docs/intro#further-resources","content":"projectaria.com - about the project, how to partner with Project Aria, learn about Grand Challenges and download datasetsProject Aria: A New Data Platform for Egocentric Multi-modal AI Research - Project Aria Whitepaper "},{"title":"Overview of sections​","type":1,"pageTitle":"Project Aria Tools","url":"/projectaria_tools/docs/intro#overview-of-sections","content":"Technical Specifications: hardware specifications, the different configurations Aria glasses can use for recording and an overview of extrinsics and intrinsics calibration. Data Formats: information about data formatting conventions used with Aria raw sensor data (stored in VRS files) as well as Machine Perception Services (MPS) data. Data Utilities: our opensource C++/Python library provides the ability to work with Aria raw sensor data as well as MPS data. We also provide binaries implemented in C++ to visualize the data. If you want to immediately dive in with the code, go to the Python Getting Started guide for a quick tour of the library in a Jupyter notebook. Aria Research Kit: how to use an Aria glasses, the companion app or request Machine Perception Services. Open Datasets: the Aria Digital Twin dataset and the Aria synthetic environments dataset. How to download the data and use our opensource tooling to visualize and access the data. For documentation about the Aria Pilot Dataset , go to our previous wiki in Aria Data Tools. Please note, some of the file formatting will be different. Tech Insights: technical deeper dives on domain-specific topics. Attribution and Contributing: Citation information and how to contribute to Project Aria Tools. "},{"title":"Overview","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset","content":"Overview Project Aria Tools provides Python and C++ APIs to access the Aria Digital Twin (ADT) dataset (paper). This section provides a detailed walk through of: Getting started A quickstart to install Project Aria Tools python package and run Aria Digital Twin tutorial notebook to access and visualize ADT ground-truth data. Dataset download A walkthrough of using adt_benchmark_dataset_downloader to download the published ADT dataset. Visualizers Compile and run our visualizer using an example that accesses ADT data in C++. Advanced tutorial A guide to learn how device synchronization works in ADT and run through a Jupyter notebook. Data loader APIs to load ADT data with handy code snippets Data format How ADT data is organized and stored","keywords":""},{"title":"Open Datasets","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets","content":"Open Datasets This section provides information about how to use Project Aria's open data. To download the datasets go to https://www.projectaria.com/datasets/","keywords":""},{"title":"Advanced Tutorials","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials","content":"","keywords":""},{"title":"Multi-person Synchronization​","type":1,"pageTitle":"Advanced Tutorials","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials#multi-person-synchronization","content":"This tutorial will walk you through the steps to get synchronized ground truth data in a multi-person sequence. "},{"title":"How does time synchronization work in ADT​","type":1,"pageTitle":"Advanced Tutorials","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials#how-does-time-synchronization-work-in-adt","content":"In a single-person ADT sequence, all ground truth data is stored in the device capture time of the Aria used by the person. For a multi-person ADT sequence, we store ground truth files in two separate folders, each representing a person's Aria recording. Each subsequence is self-contained such that all ground truth data is also stored in the device capture time domain of the associated Aria device. To synchronize the two Aria devices, we store a mapping between timecodetimestamps and device capture timestamps in each Aria data. "},{"title":"Step 0 : Install project_aria_tools package and create venv if not already done​","type":1,"pageTitle":"Advanced Tutorials","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials#step-0--install-project_aria_tools-package-and-create-venv-if-not-already-done","content":"Follow Step 0 to Step 3 in Getting Started. "},{"title":"Step 1 : Prepare download​","type":1,"pageTitle":"Advanced Tutorials","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials#step-1--prepare-download","content":"Follow Step 1 to Step 3 in Download the sample Aria Digital Twin (ADT) sequence "},{"title":"Step 2 : Download sample multi-person sequence​","type":1,"pageTitle":"Advanced Tutorials","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials#step-2--download-sample-multi-person-sequence","content":"From your python virtual environment, run: adt_benchmark_dataset_downloader -c $HOME/Documents/projectaria_tools_adt_data/aria_digital_twin_dataset_download_urls.json \\ -o $HOME/Documents/projectaria_tools_adt_data/ -l Apartment_release_multiskeleton_party_seq114 \\ -d 0  "},{"title":"Step 3 : Run the tutorial notebook​","type":1,"pageTitle":"Advanced Tutorials","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials#step-3--run-the-tutorial-notebook","content":"From your python virtual environment, run: cd $HOME/Documents/projectaria_sandbox jupyter notebook projectaria_tools/projects/AriaDigitalTwinDatasetTools/examples/adt_multiperson_tutorial.ipynb  "},{"title":"Troubleshooting​","type":1,"pageTitle":"Advanced Tutorials","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials#troubleshooting","content":"Check the troubleshooting if you are having issues in this guide. "},{"title":"Data Format","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format","content":"","keywords":""},{"title":"Sequence and Subsequence​","type":1,"pageTitle":"Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#sequence-and-subsequence","content":"A sequence in ADT represents a data recording in a scene. It can be either a multi-person activity, which may include multiple Aria devices recording at the same time, or a single-person activity, which includes only one Aria device. Inside a sequence, we use subsequences to represent the recording of each Aria device and its associated ground truth data. So far, an ADT sequence contains at most 2 subsequences. Each sequence has the following folder structure: |SequenceName| -----|Subsequence1Name| -----|Subsequence2Name| [Optional] # Omitted if a sequence is a single person activity -----|metadata.json  The metadata.json file contains the high-level sequence information such as the included Aria's serial number, the scene name, etc, which can be loaded and queried by AriaDigitalTwinDataPathProvider. Note that prior to dataset v1.1, this was named gt-metadata. Please see release_note "},{"title":"Ground Truth Data​","type":1,"pageTitle":"Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#ground-truth-data","content":"You can use the AriaDigitalTwinDataPathProvider to load a sequence and select a subsequence. AriaDigitalTwinDataPathProvider will manage all the ground truth files (see below) in a subsequence folder. |SubsequenceName| ----video.vrs # Aria recording data ----instances.json # metadata of all instances in a sequence. An instance can be an object or a skeleton ----aria_trajectory.csv # 6DoF Aria trajectory ----2d_bounding_box.csv # 2D bounding box data for instances in three Aria sensors: RGB camera, left SLAM camera, right SLAM camera ----3d_bounding_box.csv # 3D AABB of each object ----scene_objects.csv # 6 DoF poses of objects ----eyegaze.csv # Eye gaze ----synthetic_video.vrs # Synthetic rendering of video.vrs ----depth_images.vrs # Depth images of video.vrs ----segmentations.vrs # Instance segmentations of video.vrs ----skeleton_aria_association.json [optional] # File showing association between Aria devices and skeletons, if they exist. Omitted if a sequence does not have skeleton ground truth. ----Skeleton_*.json [optional] # Body skeleton data. * is the skeleton name. Omitted if a sequence does not have skeleton ground truth ----2d_bounding_box_with_skeleton.csv [optional] # 2D bounding box data with body mesh occlusions. Omitted if a sequence does not have skeleton ground truth ----depth_images_with_skeleton.vrs [optional] # Depth images with body mesh occlusions. Omitted if a sequence does not have skeleton ground truth ----segmentations_with_skeleton.vrs [optional] # Segmentations with body mesh occlusions. Omitted if a sequence does not have skeleton ground truth  Note that prior to dataset v1.1, skeleton_aria_association.json was named SkeletonMetaData.json. Please see release note "},{"title":"Skeleton Data and Availability​","type":1,"pageTitle":"Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#skeleton-data-and-availability","content":"Note that not all ADT sequences have skeleton tracking. For those sequences with skeleton tracking enabled, we use the marker measurements from the bodysuit to generate a 3D mesh estimate of the wearer which is then used in our ground truth generation pipeline to calculate 2D bounding boxes, segmentation images and depth images. In these cases, ADT provide two sets of ground truth data: one with skeleton occlusion, one without. segmentations.vrs vs. segmentations_with_skeleton.vrsdepth_images.vrs vs. depth_images_with_skeleton.vrs2d_bounding_box_with_skeleton.csv You can use AriaDigitalTwinDataPathsProvider to easily switch between these two sets. "},{"title":"Format of Ground Truth Data​","type":1,"pageTitle":"Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#format-of-ground-truth-data","content":"Our data loader loads all this data into a single class with useful tools for accessing data. For more information on the data classes returned by the loader, see this section. Below you will find more information on the above csv and json files. "},{"title":"2d_bounding_box.csv (or 2d_bounding_box_with_skeleton).csv​","type":1,"pageTitle":"Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#2d_bounding_boxcsv-or-2d_bounding_box_with_skeletoncsv","content":"Column\tType\tDescriptionstream_id\tstring\tcamera stream id associated with the bounding box image object_uid\tuint64_t\tid of the instance (object or skeleton) timestamp[ns]\tint64_t\ttimestamp of the image in nanoseconds x_min[pixel]\tint\tminimum dimension in the x axis x_max[pixel]\tint\tmaximum dimension in the x axis y_min[pixel]\tint\tminimum dimension in the y axis y_max[pixel]\tint\tmaximum dimension in the y axis visibility_ratio[%]\tdouble\tpercentage of the object that is visible (0: not visible, 1: fully visible) "},{"title":"3d_bounding_box.csv​","type":1,"pageTitle":"Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#3d_bounding_boxcsv","content":"Column\tType\tDescriptionobject_uid\tuint64_t\tid of the instance (object or skeleton) timestamp[ns]\tint64_t\ttimestamp of the image in nanoseconds. -1 means the instance is static p_local_obj_xmin[m]\tdouble\tminimum dimension in the x axis (in meters) of the bounding box p_local_obj_xmax[m]\tdouble\tmaximum dimension in the x axis (in meters) of the bounding box p_local_obj_ymin[m]\tdouble\tminimum dimension in the y axis (in meters) of the bounding box p_local_obj_ymax[m]\tdouble\tmaximum dimension in the y axis (in meters) of the bounding box p_local_obj_zmin[m]\tdouble\tminimum dimension in the z axis (in meters) of the bounding box p_local_obj_zmax[m]\tdouble\tmaximum dimension in the z axis (in meters) of the bounding box "},{"title":"aria_trajectory.csv​","type":1,"pageTitle":"Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#aria_trajectorycsv","content":"We are using the same trajectory format as the closed loop trajectory in MPS. "},{"title":"eyegaze.csv​","type":1,"pageTitle":"Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#eyegazecsv","content":"We are using the same eye gaze format as MPS. "},{"title":"scene_objects.csv​","type":1,"pageTitle":"Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#scene_objectscsv","content":"Column\tType\tDescriptionobject_uid\tuint64_t\tid of the instance (object or skeleton) timestamp[ns]\tint64_t\ttimestamp of the image in nanoseconds. -1 means the instance is static t_wo_x[m]\tdouble\tx translation from object frame to world (scene) frame (in meters) t_wo_y[m]\tdouble\ty translation from object frame to world (scene) frame (in meters) t_wo_z[m]\tdouble\tz translation from object frame to world (scene) frame (in meters) q_wo_w\tdouble\tw component of quaternion from object frame to world (scene) frame q_wo_x\tdouble\tx component of quaternion from object frame to world (scene) frame q_wo_y\tdouble\ty component of quaternion from object frame to world (scene) frame q_wo_z\tdouble\tz component of quaternion from object frame to world (scene) frame "},{"title":"instances.json​","type":1,"pageTitle":"Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#instancesjson","content":"{ &quot;IID1&quot;: { &quot;instance_id&quot;: IID1, &quot;instance_name&quot;: &quot;XXXX&quot;, &quot;prototype_name&quot;: &quot;XXXX&quot;, &quot;category&quot;: &quot;XXXX&quot;, &quot;category_uid&quot;: XXXX, &quot;motion_type&quot;: &quot;static/dynamic&quot;, &quot;instance_type&quot;: &quot;object/human&quot;, &quot;rigidity&quot;: &quot;rigid/deformable&quot;, &quot;rotational_symmetry&quot;: { &quot;is_annotated&quot;: true/false }, &quot;canonical_pose&quot;: { &quot;up_vector&quot;: [ x, y, z ], &quot;front_vector&quot;: [ x, y, z ] } }, ... }  "},{"title":"Skeleton_T.json or Skeleton_C.json​","type":1,"pageTitle":"Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#skeleton_tjson-or-skeleton_cjson","content":"{ &quot;dt_optitrack_minus_device_ns&quot;: { &quot;1WM103600M1292&quot;: XXXXX }, &quot;frames&quot;: [ { &quot;markers&quot;: [ [ mx1 my1 mz1 ], ... ], &quot;joints&quot;: [ [ jx1 jy1 jz1 ], ... ], &quot;timestamp_ns&quot;: tsns1 }, ... ] }  "},{"title":"SkeletonMetaData.json​","type":1,"pageTitle":"Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#skeletonmetadatajson","content":"This file shows the skeleton info including name, Id, and associated Aria device for each human in the sequence. Since it's possible to have a human wearing a bodysuit that does not have an Aria, it's possible to have a skeleton with no associated Aria. Conversely, it's also possible to have an Aria wearer with no bodysuit, which means there may be an empty skeleton Id and name associated with an Aria. { &quot;SkeletonMetadata&quot;: [ { &quot;AssociatedDeviceSerial&quot;: &quot;AriaSerial1/NONE&quot;, &quot;SkeletonId&quot;: ID1, &quot;SkeletonName&quot;: &quot;SkeletonName1/NONE&quot; }, ... ] }  "},{"title":"How to Download the ADT Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download","content":"","keywords":""},{"title":"Download the sample Aria Digital Twin (ADT) sequence​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#download-the-sample-aria-digital-twin-adt-sequence","content":""},{"title":"Step 0: install project_aria_tools package and create venv if not done before​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#step-0-install-project_aria_tools-package-and-create-venv-if-not-done-before","content":"Follow Step 0 to Step 3 in Getting Started. "},{"title":"Step 1 : Visit ADT website sign up.​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#step-1--visit-adt-website-sign-up","content":"Scroll down to the bottom of the page. Enter you email and select Access the Datasets.  "},{"title":"Step 2 : Download the download-links file​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#step-2--download-the-download-links-file","content":"Once you've selected Access the Datasets you'll be taken back to the top of the ADT page. Scroll down the page to select Aria Digital Twin Download Links and download the file to the folder $HOME/Downloads.  The download-links file will expire in 14 days You can redownload the download links whenever they expire "},{"title":"Step 3 : Set up a folder for ADT data​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#step-3--set-up-a-folder-for-adt-data","content":"mkdir -p $HOME/Documents/projectaria_tools_adt_data mv $HOME/Downloads/aria_digital_twin_dataset_download_urls.json $HOME/Documents/projectaria_tools_adt_data/  "},{"title":"Step 4 : Download the sample sequence (~500MB) via CLI:​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#step-4--download-the-sample-sequence-500mb-via-cli","content":"From your Python virtual environment, run: adt_benchmark_dataset_downloader -c $HOME/Documents/projectaria_tools_adt_data/aria_digital_twin_dataset_download_urls.json \\ -o $HOME/Documents/projectaria_tools_adt_data/ \\ -d 0 1 2 3 -e  "},{"title":"Download the Aria Digital Twin (ADT) benchmark dataset​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#download-the-aria-digital-twin-adt-benchmark-dataset","content":""},{"title":"Data size​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#data-size","content":"Aria Digital Twin dataset consists of 222 sequences in total. The total size of the dataset is about 3.5TB. The dataset is split into 4 data types that can be downloaded individually. The size of each data type is below. Data type\tWhat's included\tPer sequence size\tTotal size for all sequencesmain\tAria raw data, 2D bounding box, 3D object poses and bounding box, skeleton data, eye gaze data\t3 - 6 GB\t~700 GB segmentation\tInstance segmentation data\t2 - 4 GB\t~750 GB depth\tDepth map data\t4 - 8 GB\t~1.5 TB synthetic\tSynthetic rendering data\t2 - 4 GB\t500 GB "},{"title":"Download via CLI​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#download-via-cli","content":"Follow the ADT Getting Started Guide to download the example data. This section will introduce how to download the dataset using the adt_benchmark_dataset_downloader. Resumable download​ The adt_benchmark_dataset_downloader checks the previous download status of the sequences in the --output_folder. If the downloading breaks in the middle, relaunch the CLI and it will continue the downloading. Detailed arguments​ Arguments\tType\tDescription--cdn_file\tstr\tThe download-urls file you downloaded from the ADT website page after signing up --output_folder\tstr\tA local path where the downloaded files and metadata will be stored --metadata_only\tflag\tOnly download the metadata --data_types\tlist of int\t0→main, 1→segmentation, 2→depth, 3→synthetic --example_only\tflag\tOnly download example data --overwrite\tflag\tDisable resumable download. Force download and overwrite existing data --sequence_names\tlist of str\tlist of sequence names. If not specified, download all sequences "},{"title":"Download Examples​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#download-examples","content":"Note that all these commands must be run from your Python virtual environment that has the projectaria-tools package and dependencies installed. Download metadata for ADT datasets​ adt_benchmark_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --metadata_only  Download main data for all sequences​ adt_benchmark_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0  Download all data for all sequences​ adt_benchmark_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 1 2 3  Download main data for 2 specific sequences​ adt_benchmark_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 --sequence_names Lite_release_recognition_BambooPlate_seq031 Lite_release_recognition_BirdHouseToy_seq030  Download main data for all sequences and overwrite​ adt_benchmark_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 --overwrite  "},{"title":"Select specific sequences​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#select-specific-sequences","content":"The dataset metadata JSON “aria_digital_twin_benchmark_metadata.json”, which can be downloaded using adt_benchmark_dataset_downloader, contains metadata for each ADT sequence. The metadata fields of each sequence are: Field Name\tDescriptionscenes\tThe scene that a sequence is captured at, Apartment or LiteOffice, in the current ADT release, there will only be one element in the list is_multi_person\tWhether the sequence is a single person activity or a multiperson activity num_skeleton\tnumber of persons whose body skeleton is tracked aria_digital_twin_dataset_searcher.py is an example Python script for filtering sequences via different criteria. "},{"title":"Troubleshooting​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#troubleshooting","content":"Check the troubleshooting if you are having issues in this guide. "},{"title":"Data Loader","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader","content":"","keywords":""},{"title":"AriaDigitalTwinDataPathsProvider​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#ariadigitaltwindatapathsprovider","content":"The main goal of this loader is to give the user an easy way to load an ADT sequence and its metadata, to select a specific subsequence, and to select specific annotations to load (e.g., with or without skeleton). AriaDigitalTwinDataPathsProvider manages all ground truth file paths that can be used to load ground truth data in AriaDigitalTwinDataProvider. The following shows an example code snippet which loads an ADT sequence and select a subsequence to be passed to the AriaDigitalTwinDataProvider. PythonC++ from projectaria_tools.projects.adt import AriaDigitalTwinDataPathsProvider # define the sequence path you want to load sequence_path = &quot;PATH/TO/An_ADT_sequence&quot; # create path provider paths_provider = AriaDigitalTwinDataPathsProvider(sequence_path) # list all subsequences for this sequence all_device_serials = paths_provider.get_device_serial_numbers() # print the Aria device serial number used in each subsequence for idx, device_serial in enumerate(all_device_serials): print(&quot;device number - &quot;, idx, &quot;: &quot;, device_serial) # load the set of ground truth data files without skeleton occlusion of the first Aria device data_paths_without_skeleton_occlusion = paths_provider.get_datapaths_by_device_num(0, False) # load the set of ground truth data files with skeleton occlusion of the first Aria device data_paths_with_skeleton_occlusion = paths_provider.get_datapaths_by_device_num(0, True)  "},{"title":"AriaDigitalTwinDataProvider​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#ariadigitaltwindataprovider","content":"This is the core data loader that takes an instance of the AriaDigitalTwinDataPaths class (generated by the AriaDigitalTwinDataPathsProvider) and provides you will query functions to access all ADT data. The following shows an example snippet to load ground truth data with the AriaDigitalTwinDataProvider: PythonC++ from projectaria_tools.projects.adt import AriaDigitalTwinDataPathsProvider, AriaDigitalTwinDataProvider # define the sequence path you want to load sequence_path = &quot;PATH/TO/An_ADT_sequence&quot; # create path provider paths_provider = AriaDigitalTwinDataPathsProvider(sequence_path) # load the set of ground truth data files with skeleton occlusion of the first Aria device data_paths_with_skeleton_occlusion = paths_provider.get_datapaths_by_device_num(0, True) # create data provider gt_provider = AriaDigitalTwinDataProvider(data_paths_with_skeleton_occlusion)  "},{"title":"Skip Data loading​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#skip-data-loading","content":"All data loaders are designed to allow the user to skip the loading of specific data types. You can do this by setting the path to an empty string in your AriaDigitalTwinDataPathsinstance prior to constructing the AriaDigitalTwinDataProvider. "},{"title":"Check Data Existence​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#check-data-existence","content":"Since we allow users to skip specific data type loading as explained above, we also provide functions in in AriaDigitalTwinDataProviderto check if data exists by calling their appropriate functions before calling the corresponding getter functions. E.g. hasObject3dBoundingboxes() "},{"title":"Ground Truth Data Getter Functions​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#ground-truth-data-getter-functions","content":"For a full example of the python getters, please refer to the notebook in the Getting Started. For a full example of the C++ getters, please refer to the visualizer example. Getting Instance Information​ In ADT, we define an instance to be either a human or an object. The attributes of an instance is defined in class InstanceInfo in AriaDigitalTwinDataTypes. We use instanceType to differentiate a human and an object. Time Query Options​ You may have also noticed the timeQueryOptions parameter in the above getter functions. Same as dataprovider, all getter functions for timestamped data allow you to specify how to query the timestamps. The options are defined in TimeTypes Accessing Timestamped Data​ All timestamped data query APIs return a templated DataWithDt class. For example, BoundingBox2dDataWithDt defined in AriaDigitalTwinDataTypes as: using BoundingBox2dDataWithDt = DataWithDt&lt;TypeBoundingBox2dMap&gt;;  The goal of wrapping all data in a DataWithDt class is to ensure all returned timestamped data has two fields: isValid, and dtNs. Where isValid defined whether or not the returned data is valid, since all timestamp queries may be invalid times, and dtNs to ensure the user always knows the time difference between the returned data and the query time. Interpolation Function​ We provide interpolation functions for 6DoF Aria poses and Object 3d bounding boxes called &quot;getInterpolatedAria3dPoseAtTimestampNs&quot; and &quot;getInterpolatedObject3dBoundingBoxesAtTimestampNs&quot; in AriaDigitalTwinDataProvider "},{"title":"Time Synchronization Between Subsequences​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#time-synchronization-between-subsequences","content":"Please refer to this tutorial to find out how to synchronize subsequences in an ADT sequence. "},{"title":"Skeleton Data​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#skeleton-data","content":"Separate from the 2D skeleton data, we also have skeleton frames as measured by Optitrack. This data can be accessed directly from the AriaDigitalTwinDataProvider, or using the AriaDigitalTwinSkeletonProvider which can be extracted from AriaDigitalTwinDataProvider. Motive, the software that runs the Optitrack system, generates two types of skeleton data: Skeleton Markers: a set of 3D marker positions of all visible markers that are attached to the bodysuit. If markers are occluded, they are set to [0,0,0]. We provide a helper function to get the labels: getMarkerLabels() in AriaDigitalTwinSkeletonProvider. For more information see motive’s Biomech57 template Skeleton Joints: a set of estimated 3D joint positions. We provide a list of these joint positions for each timestamp, as well as the joint labels getJointConnections(), and connections getJointLabels() in in AriaDigitalTwinSkeletonProvider Note that both the markers and the joints are provided in the ADT Scene frame to be consistent with all other ground truth data. "},{"title":"Visualizers","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers","content":"","keywords":""},{"title":"AriaDigitalTwinViewer​","type":1,"pageTitle":"Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#ariadigitaltwinviewer","content":"AriaDigitalTwinViewer is a C++ binary written to visualize ADT data with toggles for each ground truth data type and a slider bar for frame selection. The image below shows an example screenshot of the viewer.  "},{"title":"Step 1 : Download Sample Sequence:​","type":1,"pageTitle":"Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#step-1--download-sample-sequence","content":"Download the sample Aria Digital Twin (ADT) sequencefollow this guide. "},{"title":"Step 2 : Build projectaria_tools C++ libraries​","type":1,"pageTitle":"Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#step-2--build-projectaria_tools-c-libraries","content":"Follow the entire C++ installation to build projectaria_tools C++ libraries with visualization. "},{"title":"Step 3 : Build AriaDigitalTwinViewer​","type":1,"pageTitle":"Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#step-3--build-ariadigitaltwinviewer","content":"cd $HOME/Documents/projectaria_sandbox/build cmake ../projectaria_tools -DPROJECTARIA_TOOLS_BUILD_PROJECTS=ON -DPROJECTARIA_TOOLS_BUILD_PROJECTS_ADT=ON make -j2  "},{"title":"Step 4 : Run AriaDigitalTwinViewer​","type":1,"pageTitle":"Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#step-4--run-ariadigitaltwinviewer","content":"cd $HOME/Documents/projectaria_sandbox/build ./projects/AriaDigitalTwinDatasetTools/visualization/AriaDigitalTwinViewer \\ --sequence-path $HOME/Documents/projectaria_tools_adt_data/Apartment_release_golden_skeleton_seq100_10s_sample/ \\ --device-num 0 --skeleton-flag 0  "},{"title":"Troubleshooting​","type":1,"pageTitle":"Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#troubleshooting","content":"Check the troubleshooting if you are having issues in this guide. "},{"title":"Getting Started","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started","content":"","keywords":""},{"title":"Step 0 : Check system requirements and download codebase​","type":1,"pageTitle":"Getting Started","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#step-0--check-system-requirements-and-download-codebase","content":"Ensure your system is supported and then download projectaria_tools codebase from the github "},{"title":"Step 1 : Install Python​","type":1,"pageTitle":"Getting Started","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#step-1--install-python","content":"If you have already installed projectaria-tools using Python Package Installation, you can skip to Step 4. The ADT Python code is part of the main projectaria-tools package. Jupyter notebook error : If you have problems using Jupyter examples, please upgrade python3 to the latest version. "},{"title":"Step 2 : Create a virtual environment​","type":1,"pageTitle":"Getting Started","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#step-2--create-a-virtual-environment","content":"rm -rf $HOME/projectaria_tools_python_env python3 -m venv $HOME/projectaria_tools_python_env source $HOME/projectaria_tools_python_env/bin/activate  "},{"title":"Step 3 : Install projectaria_tools from pypi​","type":1,"pageTitle":"Getting Started","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#step-3--install-projectaria_tools-from-pypi","content":"pip3 install --upgrade pip pip3 install projectaria-tools'[all]'  "},{"title":"Step 4 : Download Sample Sequence:​","type":1,"pageTitle":"Getting Started","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#step-4--download-sample-sequence","content":"Download the sample Aria Digital Twin (ADT) sequence by following steps 0 to 4 inthis guide. "},{"title":"Step 5 : Run Tutorial​","type":1,"pageTitle":"Getting Started","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#step-5--run-tutorial","content":"From your projectaria_tools python virtual environment, run: cd $HOME/Documents/projectaria_sandbox jupyter notebook projectaria_tools/projects/AriaDigitalTwinDatasetTools/examples/adt_quickstart_tutorial.ipynb  "},{"title":"Troubleshooting​","type":1,"pageTitle":"Getting Started","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#troubleshooting","content":"Go to Data Utilities Troubleshooting if you have issues implementing this guide. "},{"title":"Other Useful Links​","type":1,"pageTitle":"Getting Started","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#other-useful-links","content":"Dataset downloadVisualizersAdvanced tutorialUse projectaria_tools with CMake "},{"title":"ASE Data Format","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_format","content":"","keywords":""},{"title":"Overall Data Organization​","type":1,"pageTitle":"ASE Data Format","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_format#overall-data-organization","content":"Each scene has its own subdirectory with a unique ID (0-100K)Each scene directory contains separate files and directories for each type of data &lt;sceneID&gt; ├── rgb │ └── vignette0000000.jpg │ └── vignette0000001.jpg │ ... │ └── vignette0xxn.jpg ├── depth │ └── depth0000000.jpg │ └── depth0000001.jpg │ ... │ └── depth0xxn.jpg ├── instances │ └── instance0000000.jpg │ └── instance0000001.jpg │ ... │ └── instance0xxn.jpg ├── ase_scene_language.txt ├── trajectory.txt ├── semidense_points.csv.gz └── semidense_observations.csv.gz  rgb - 2D RGB fisheye images Aria RGB sensor at 10FPSRGB JPEG format depth - 2D depth maps (16 bit) Aria RGB Sensor at 10 FPSPNG format instances - 2D segmentation maps (16 bit) Aria RGB sensor at 10FPSPNG format ase_scene_language.txt - 3D floor plan definition ASE Language format trajectory.txt - Ground-truth trajectorysemidense_points.csv.gz - Semi-dense Map points From MPS run on Aria SLAM cameras sensor data semidense_observations.csv.gz - Semi-dense Map observations From MPS run on Aria SLAM cameras sensor data "},{"title":"Aria RGB Sensor - Image, Depth and Instance Segmentation​","type":1,"pageTitle":"ASE Data Format","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_format#aria-rgb-sensor---image-depth-and-instance-segmentation","content":"For each frame from the RGB sensor we provide: A vignetted sensor imageSimulated 16 bit metric depth (mm) in PNG image formatA segmentation image (16 bit PNG) The images in each folder are in sync. This means there will be same number of images in each folder. We also provide example data visualizers to load these images and/or associate them. "},{"title":"ASE Scene Language Format​","type":1,"pageTitle":"ASE Data Format","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_format#ase-scene-language-format","content":"The ASE Scene Language format is set of hand-designed procedural commands in pure text form. To handle commonly encountered static indoor layout elements, we use three commands: make_wall - the full set of parameters specifies a gravity-aligned oriented boxmake_door - specify box-based cutouts from wallsmake_window - specify box-based cutouts from wall Each command includes its own set of parameters, as described below. Given the command’s full set of parameters, a geometry is completely specified. A single scene is described via a sequence of multiple commands. The sequence length is arbitrary and follows no specific ordering. The interpretation of the command and its arguments is carried out by a customized interpreter responsible for parsing the sequence and generating a 3D mesh of the scene.  "},{"title":"Trajectory and Semi-Dense Map Points​","type":1,"pageTitle":"ASE Data Format","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_format#trajectory-and-semi-dense-map-points","content":"Ground-truth trajectory data provides poses for each frame generated from a simulation at 10 FPS. We are follow the same trajectory format as the closed loop trajectory used by Machine Perception Services (MPS). For semi-dense map point clouds and their observations, we follow the same point cloud points and observations format as MPS. The semi-dense map point cloud is generated using same algorithm as MPS, with the addition of ground-truth trajectory and simulated SLAM camera images. "},{"title":"Synthetic Environments Data Tools and Visualization","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools","content":"","keywords":""},{"title":"Data Helper Tools​","type":1,"pageTitle":"Synthetic Environments Data Tools and Visualization","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools#data-helper-tools","content":"These helper functions are broadly categorized into the following types: Data interpreter: interpreter.py Provides an interpreter for the ASE Scene Language to convert them into a 3D model in the form of bounding boxes Data readers: readers.py Provide readers for the: ASE Scene Language,Ground-truth trajectory andSemi-dense Map points. Data Plotters: plotters.py Provide simple plotting functions for the: 3D scene from ASE Scene Language,Ground-truth trajectory andSemi Dense Map points. "},{"title":"Visualization Notebook​","type":1,"pageTitle":"Synthetic Environments Data Tools and Visualization","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools#visualization-notebook","content":"We also provide Jupyter notebooks to visualize the data for each sequence. To get started download ASE data following steps from Dataset Download cd /path_to/projectaria_tools jupyter notebook projects/AriaSyntheticEnvironment/tutorial/ase_tutorial_notebook.ipynb  "},{"title":"Part 1: 3D visualization of the scene​","type":1,"pageTitle":"Synthetic Environments Data Tools and Visualization","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools#part-1-3d-visualization-of-the-scene","content":"This section will introduce the dataset’s 3D components as well as code snippets to help users get familiar with them. You will be taken through examples of how to load the 3D dataset annotations namely: the ground-truth trajectory, the ASE Scene Language, and the Semi-dense Map point cloud. In addition, we provide examples of how they can each be plotted. At the end of the section you should see 3D plots containing: The Semi-dense Map point cloud,The layout annotations, visualized as 3D box wireframes,The trajectory plotted as a dotted line in 3D. Example scene visualization: "},{"title":"Part 2: Loading and Plotting Images and Image Annotations​","type":1,"pageTitle":"Synthetic Environments Data Tools and Visualization","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools#part-2-loading-and-plotting-images-and-image-annotations","content":"Since the file structure and format are straightforward, the code consists of very simple PIL and matplotlib code to show the 3 images (RGB, depth and instance maps) side-by-side: "},{"title":"Part 3: Projecting Points into Images​","type":1,"pageTitle":"Synthetic Environments Data Tools and Visualization","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools#part-3-projecting-points-into-images","content":"Running the final part of the notebook will load the camera calibration, as well as the pointcloud, trajectory and select a random frame. Then given the device pose from the trajectory, we project the points into the frame. Points that project outside of the valid radius, should not be plotted  "},{"title":"Dataset Download","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_download_dataset","content":"","keywords":""},{"title":"Download via CLI​","type":1,"pageTitle":"Dataset Download","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_download_dataset#download-via-cli","content":"Follow the ASE quickstart guide to get the system ready to download data/example data. This section will introduce how to download the dataset using the aria_synthetic_environments_downloader python script. "},{"title":"Detailed arguments​","type":1,"pageTitle":"Dataset Download","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_download_dataset#detailed-arguments","content":"Arguments\tType\tDescription--cdn_file\tstr\tThe download-urls file you downloaded from the ASE website page after signing up --output-dir\tstr\tA local path where the downloaded files will be stored --set\tstr\tDownload either train / test data. All the 100K data is for training data and comes with GT ASE language. At a later point we will add test data without GT ASE language, which will be used for evaluation --scene-ids\tstr\tRange of scene ids to download --unzip\tbool\tAllows the user to unzip in the output directory or keep it as a zip "},{"title":"Examples​","type":1,"pageTitle":"Dataset Download","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_download_dataset#examples","content":""},{"title":"Download ASE datasets​","type":1,"pageTitle":"Dataset Download","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_download_dataset#download-ase-datasets","content":"cd $HOME/Documents/projectaria_sandbox/projectaria_tools  Download first 10 scenes​ python3 projects/AriaSyntheticEnvironment/aria_synthetic_environments_downloader.py --set train --scene-ids 0-9 --cdn-file aria_synthetic_environments_dataset_download_urls.json --output-dir $HOME/projectaria_tools_ase_data --unzip True  Download a large number of scenes: This downloads all 100 scenes (10 chunks)​ python3 projects/AriaSyntheticEnvironment/aria_synthetic_environments_downloader.py --set train --scene-ids 0-99 --cdn-file aria_synthetic_environments_dataset_download_urls.json --output-dir $HOME/projectaria_tools_ase_data --unzip True  Download specific scenes: 560-569​ python3 projects/AriaSyntheticEnvironment/aria_synthetic_environments_downloader.py --set train --scene-ids 560-569 --cdn-file aria_synthetic_environments_dataset_download_urls.json --output-dir $HOME/projectaria_tools_ase_data --unzip True  "},{"title":"Getting Started With the Synthetic Environments Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started","content":"","keywords":""},{"title":"Quickstart Tutorial - Python​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#quickstart-tutorial---python","content":""},{"title":"Step 0 : Check system requirements and download codebase​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#step-0--check-system-requirements-and-download-codebase","content":"Ensure your system is supported and then download projectaria_tools codebase from the github "},{"title":"Step 1 : Install Python​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#step-1--install-python","content":"Ensure python3 is installed on the system (check with python3 --version) "},{"title":"Step 2 : Create a virtual environment​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#step-2--create-a-virtual-environment","content":"rm -rf $HOME/projectaria_tools_python_env python3 -m venv $HOME/projectaria_tools_python_env source $HOME/projectaria_tools_python_env/bin/activate  "},{"title":"Step 3 : Install projectaria_tools from pypi​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#step-3--install-projectaria_tools-from-pypi","content":"pip3 install --upgrade pip pip3 install projectaria-tools'[all]'  The ASE python tooling for projection of 3D points to RGB images is included in the projectaria_tools package, so no further steps are needed. The following packages used in this tutorial are standard python packages that are also included in project_aria_tools build. plotlynumpyscipypandasmatplotlibrequeststqdmjupyter "},{"title":"Step 4 : Download sample data​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#step-4--download-sample-data","content":"Navigate to the ASE page on the Project Aria Website and follow the instructions to download the download-urls file. This same download-urls file can be used for any dataset download until the link expiresSetup ASE local folder and move download-urls file: mkdir -p $HOME/Documents/projectaria_sandbox/projectaria_tools_ase_data mv $HOME/Downloads/aria_synthetic_environments_dataset_download_urls.json $HOME/Documents/projectaria_sandbox/projectaria_tools_ase_data/  Download sample dataset using the download tool: cd $HOME/Documents/projectaria_sandbox/projectaria_tools python3 projects/AriaSyntheticEnvironment/aria_synthetic_environments_downloader.py --set train --scene-ids 0-10 --cdn-file $HOME/Documents/projectaria_sandbox/projectaria_tools_ase_data/aria_synthetic_environments_dataset_download_urls.json --output-dir $HOME/Documents/projectaria_sandbox/projectaria_tools_ase_data --unzip True  "},{"title":"Step 5 : Run the visualization notebooks​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#step-5--run-the-visualization-notebooks","content":"jupyter notebook projects/AriaSyntheticEnvironment/tutorial/ase_tutorial_notebook.ipynb  "},{"title":"Aria Pilot Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/pilot_dataset","content":"Aria Pilot Dataset The Aria Pilot Dataset was launched at CVPR in 2022. It uses a different file structure compared to our other datasets. Our previous wiki in Aria Data Tools contains documentation for Aria Pilot Dataset.","keywords":""},{"title":"Tech Insights","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights","content":"Tech Insights Technical deeper dives on domain-specific topics. You don't need to read this section to use Aria data or glasses, but you may find it interesting to understand how Aria glasses works.","keywords":""},{"title":"Camera Photometric and Noise Model","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights/camera_photometric_and_noise_model","content":"","keywords":""},{"title":"Photometric Models​","type":1,"pageTitle":"Camera Photometric and Noise Model","url":"/projectaria_tools/docs/tech_insights/camera_photometric_and_noise_model#photometric-models","content":"In their working distance range, Aria camera lenses are well-focused, i.e. their point spread function is at sub-pixel level. Thus, we can establish a simplified photometric model where each camera pixel collects the photon emitted from a tiny surface area around a corresponding world point. The irradiance of each pixel is attenuated by vignetting. The vignetting of Aria cameras are dominated by two factors (1) cos^4 fall-off (2) mechanical cropping of the lens barrel. Points that falls out of the camera's FOV are not visible, and cannot be applied by the above intrinsic model. Then each pixel takes the time integral of the irradiance, as the sensors collect the arriving photon over the exposure time. The pixel intensity of a non-linear function of the amount of received photons as the ADC transform is non-linear and saturated. "},{"title":"Noise Models​","type":1,"pageTitle":"Camera Photometric and Noise Model","url":"/projectaria_tools/docs/tech_insights/camera_photometric_and_noise_model#noise-models","content":"The two sources of noise dominating Aria camera sensors are: Shot noise, which accounts for the noise generated due to arrival of photons. Shot noise follows the Poisson distribution.Read noise, which accounts for the noise generated due to ADC conversion, etc. Read noise can be modeled by a zero-mean Gaussian random variable. "},{"title":"Device timestamping","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights/device_timestamping","content":"Device timestamping The figure below illustrates the various hardware components in the Aria device and how they are connected electrically. The device consists of a microcontroller unit (MCU) that interfaces with most of the sensors for configuring and controlling them. The MCU is responsible for timestamping the data from these sensors, which enables capturing the multi-modal data with common timestamps across the motion sensors, microphones and camera sensors. The device also has an Application Processor (AP) that runs Android High Level OS. The device timestamp is ideally assigned, by the embedded micro-controller (MCU), to the measurement as close as possible to the time the measurement is captured. However, the meaning of the event effectively timestamped and the way the timestamp is obtained differs significantly depending on the sensor: SLAM and ET cameras have an electronic global shutter sensors. They are triggered at regular rate. Their image timestamps marks the center of the exposure window and are derived from the value of a MCU counter. The timestamping error is expected to be upper-bounded by 19us.The RGB camera has a electronic rolling shutter. It is triggered at regular rate, often a divider of the SLAM camera rate. The timestamp marks the center of the exposure of the middle row and is obtained similarly to the SLAM camera timestamp.The two IMUs, the barometer and the magnetometer sensors operate respectively at 800Hz, 1000Hz, 50Hz and 10Hz in free-running mode. We timestamp their data-ready signal on the MCU. Because of on-chip signal processing operations, those timestamps correspond to a time point after the instant for which the measurement is valid. The next section details how to finely align the data with the images.The GNSS data are timestamped on the AP at their time of arrival from the receiver of and converted to a device timestamp. Conversion of timestamp is based on a bidirectional communication between SoC and MCU and is expected to introduce less than 100us of error.For the audio stream, each samples is individually timestamped with an accuracy expected to be better than one audio sample. This synchronization relies on the MCU periodically injecting an encoded version of the current device timestamp into an unused microphone channel; the AP decodes it on reception.The BLUETOOTH and WIFI scan data is received and timestamped on the AP using a time estimate of the MCU time. Conversion of timestamp is based on the protocol between SoC and MCU and is expected to introduce less than 100us of error.","keywords":""},{"title":"IMU Noise Model","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights/imu_noise_model","content":"IMU Noise Model In our visual-inertial fusion algorithm, we model, as traditionally done, the stochastic part of the IMU error as including three components: The noise along any of the axis of the inertial sensor is written described as the sum of those contributions: xksampled=xktrue+nk0turn-on bias+nkbias random walk+nkwhite noisex^{\\text{sampled}}_k = x^{\\text{true}}_k + n^{\\text{turn-on bias}}_{k0} + n^{\\text{bias random walk}}_k + n^{\\text{white noise}}_kxksampled​=xktrue​+nk0turn-on bias​+nkbias random walk​+nkwhite noise​ Where xktruex^{\\text{true}}_kxktrue​ is the real value of the quantity measured projected on the sensor sensitive axis. xksampledx^{\\text{sampled}}_kxksampled​ is the sampled value seen as a random variable. nk0turn-on biasn^{\\text{turn-on bias}}_{k0}nk0turn-on bias​ is a random variable draw from a Gaussian when the device is turned-on (denoted here as step k0k_0k0​). nkwhite noisen^{\\text{white noise}}_{k}nkwhite noise​ is the time-independent noise components and draw from a 0-centered Gaussian at each step kkk. The latter noise is sometime \\emph{Angle Random walk in rad/s/Hzrad/s/\\sqrt{Hz}rad/s/Hz​} for the gyrometer and the \\emph{Velocity Random Walk in m/s/Hzm/s/\\sqrt{Hz}m/s/Hz​} for the accelerometer. The covariance of the Gaussian is usually characterized by the continuous noise σc\\sigma_cσc​ strength (in the same unit), which needs to be multiplied by the sampling period Δt\\Delta tΔt to get the distribution of the sample noise: σc2Δt\\sigma^2_c \\Delta tσc2​Δt. Finally, nkbias random walkn^{\\text{bias random walk}}_knkbias random walk​ is draw from a random walk process. The parameter describing those noises are derived from an Allan Variance plot computed from data collected over a period of 24 hours in a temperature stable environment. See Table below for the white noise and bias instability parameters of Aria IMU sensors. accel-left\taccel-right\tgyro-left\tgyro-rightwhite noise\t0.9×10−60.9\\times 10^-60.9×10−6\t0.8×10−60.8\\times 10^-60.8×10−6\t5×10−35\\times 10^-35×10−3\t10−210^-210−2 bias instability\t280×10−6280\\times 10^-6280×10−6\t350×10−6350\\times 10^-6350×10−6\t1.3×10−31.3\\times 10^-31.3×10−3\t0.6−30.6^-30.6−3 Also see the following Figure for the Allan Variance plot supporting this measurement, note that the data duration used for Allan Variance was not enough to capture the bias random walk confidently. In practice, we tune the parameter of the bias random walk used for sensor-fusion starting from the bias instability measured on the Allan Variance of the sensor (the floor of the curve) and inflating it by a tuning factor. This is because real MEMS sensors are not that well modelled by this stochastic model.","keywords":""},{"title":"Sensor Measurement Model","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights/sensor_measurement_model","content":"Sensor Measurement Model For IMUs, we employ an affine model where the value from the readout of accelerometer sas_asa​ or gyroscope sgs_gsg​, is compensated to obtain a &quot;real&quot; acceleration aaa and angular velocity ω\\omegaω by a=Ma−1(sa−ba)ω=Mg−1(sg−bg)a = M_a^{-1}(s_a - b_a) \\qquad \\omega = M_g^{-1}(s_g - b_g)a=Ma−1​(sa​−ba​)ω=Mg−1​(sg​−bg​) MaM_aMa​ and MgM_gMg​ are assumed to be upper triangular so that there is no global rotation from the imu body frame to the accelerometer frame. Inversely, we can simulate the sensor read-out from acceleration or angular velocity by sa=Maa+basg=Mgω+bgs_a = M_a a + b_a \\qquad s_g = M_g \\omega + b_gsa​=Ma​a+ba​sg​=Mg​ω+bg​ When the read-out signal exceeds a threshold, the signal saturates. Saturation limits are sensor dependent and referenced in the following table for accelerometer and gyrometers. accel-left\taccel-right\tgyro-left\tgyro-rightsaturation\t4g\t8g\t5000\t1000 Similar to the IMU rectification model, the sensor readout of magnetometer, barometer, and audio data are modeled as linear to the real rrr (magnetic field, air pressure and sound intensity). Audio specifically is bias only.","keywords":""},{"title":"Temporal Alignment of Sensor Data","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights/temporal_alignment_of_sensor_data","content":"","keywords":""},{"title":"Per sensor time offset : dtDeviceSensor\\text{dt}_{\\text{Device}}^{\\text{Sensor}}dtDeviceSensor​​","type":1,"pageTitle":"Temporal Alignment of Sensor Data","url":"/projectaria_tools/docs/tech_insights/temporal_alignment_of_sensor_data#per-sensor-time-offset--textdt_textdevicetextsensor","content":"Since the Slam camera timestamps are direct measurements of a well defined instant (the center of exposure of our image sensor), we choose the Slam camera timestamps as the reference timestamp and define dtDeviceSlam=0\\text{dt}_{\\text{Device}}^{\\text{Slam}} = 0dtDeviceSlam​=0. The same goes for the RGB camera : dtDeviceRGB=0\\text{dt}_{\\text{Device}}^{\\text{RGB}} = 0dtDeviceRGB​=0. Note that the next subsection describes in more details the temporal aspect of the image formation model. For IMUs, the time offset is estimated in the factory by our camera-imu calibration process and is so that the following relation approximately holds (after IMU intrinsics compensation): a^tDevice=a~(tDevice+dtDeviceAccel+0.5Δt)\\hat{a}_{t_{\\text{Device}}} = \\tilde{a}\\left(t_{\\text{Device}} + \\text{dt}_{\\text{Device}}^{\\text{Accel}} + 0.5\\Delta_t \\right)a^tDevice​​=a~(tDevice​+dtDeviceAccel​+0.5Δt​) where tDevicet_{\\text{Device}}tDevice​ is a timestamp in the device time domain, a^tDevice\\hat{a}_{t_{\\text{Device}}}a^tDevice​​ is an estimate of the true acceleration at the instant represented by the timestamp tDevicet_{\\text{Device}}tDevice​, dtDeviceAccel\\text{dt}_{\\text{Device}}^{\\text{Accel}}dtDeviceAccel​ is the estimate of the time offset, Δt\\Delta_tΔt​ is the sampling period of the sensor. Finally, the operator t→a~(t)t\\xrightarrow{}\\tilde{a}(t)t​a~(t) is a temporal interpolation of the compensated imu sample time series (tk,a~k)(t_k, \\tilde{a}_k)(tk​,a~k​). Note the appearance of 0.5Δt0.5\\Delta_t0.5Δt​ in previous equation, which stems from internal implementation choice. The same relation exists for the gyrometer. For magnetometer, we estimate dtDeviceMag\\text{dt}_{\\text{Device}}^{\\text{Mag}}dtDeviceMag​ to be around +5ms+5ms+5ms with the following relation: B^tDevice=B~(tDevice+dtDeviceMag)\\hat{B}_{t_{\\text{Device}}} = \\tilde{B}\\left(t_{\\text{Device}} + \\text{dt}_{\\text{Device}}^{\\text{Mag}} \\right)B^tDevice​​=B~(tDevice​+dtDeviceMag​) Where notation are similar as above with B^\\hat{B}B^ representing the magnetic field vector. For barometer, audio signal and GPS data, such an offset is undetermined. "},{"title":"Images formation temporal model: rolling shutter and PLS artifact​","type":1,"pageTitle":"Temporal Alignment of Sensor Data","url":"/projectaria_tools/docs/tech_insights/temporal_alignment_of_sensor_data#images-formation-temporal-model-rolling-shutter-and-pls-artifact","content":"In practice, the images obtained from our environment facing camera are not well described as captured at a unique timestamp for the most demanding applications. First, the regular exposure duration window can range from 19us up to 14ms, second the RGB sensor has a rolling shutter, where each row is being captured at different time per design, finally the slam cameras, even if specified as global shutter sensors are impacted by a row-dependent parasitic light sensitivity: a proportion of the photons forming the image are captured outside of the regular exposure window. This proportion depends on the row. For the RGB sensor, we characterize the rolling shutter behavior through the read-out time Δdtreadout\\Delta \\text{dt}_{\\text{readout}}Δdtreadout​, we define is as the time between the readout of the first row and the readout of the last row. This time depends on the sensor binning configuration, that can differ on a recording basis. The readout time is specified to be 16.26ms16.26ms16.26ms for the full resolution (2880×28802880\\times28802880×2880) and 5ms5ms5ms for the binned/cropped configuration (1408×14081408\\times14081408×1408). We can account for rolling shutter by assigning a center of exposure timestamp to each pixel with the following formulae: tDevice(p)=tDeviceImage+(row(p)H−0.5)⋅Δdtreadoutt_{\\text{Device}}(\\boldsymbol{p}) = t_{\\text{Device}}^{\\text{Image}} + \\left(\\frac{\\text{row}(\\boldsymbol{p})}{H} - 0.5\\right) \\cdot \\Delta \\text{dt}_{\\text{readout}}tDevice​(p)=tDeviceImage​+(Hrow(p)​−0.5)⋅Δdtreadout​ Where tDevice(p)t_{\\text{Device}}(\\boldsymbol{p})tDevice​(p) is the timestamp of the observation at pixel p\\boldsymbol{p}p, tDeviceImaget_{\\text{Device}}^{\\text{Image}}tDeviceImage​ is the device timestamp of the image assigned by the MCU, p→row(p)\\boldsymbol{p}\\xrightarrow{}\\text{row}(\\boldsymbol{p})p​row(p) is the projection of the pixel coordinate on the image dimension aligned with sensor rows, HHH is the size of the image along this dimension, Δdtreadout\\Delta \\text{dt}_{\\text{readout}}Δdtreadout​ is the readout time value mentioned above. For the even more demanding applications, one might need to compensate timestamp of pixels observation for Slam cameras too. On Aria camera sensors, readout of the image is done row by row. Each row can still accumulate charges after the regular exposure time and until it is fully read out, an effect sometime called Parasitic light sensitivity (PLS). The readout time of the last row Δdtpls\\Delta \\text{dt}_{\\text{pls}}Δdtpls​, i.e. the time a pixel on the last row still accumulates electrons before being discharged is specified by the manufacturer as being 9.12ms. The ratio SplsS_{\\text{pls}}Spls​ of the sensitivity during readout over the sensitivity during regular exposure was estimated to be ~0.01, instead of the ideal 0 value. From this, it results that when dealing with pixel observation time, it might be necessary to take effect into account by assigning to each pixel their effective center of exposure, we suggest the the following formulae: tDevice(p)=tDeviceImage+12⋅r(p)⋅Spls⋅1+re(p)1+re(p)⋅Splst_{\\text{Device}}(\\boldsymbol{p}) = t_{\\text{Device}}^{\\text{Image}} + \\frac{1}{2} \\cdot r\\left(\\boldsymbol{p}\\right) \\cdot S_{\\text{pls}} \\cdot \\frac{1 + r_e\\left({\\boldsymbol{p}}\\right)}{1 + r_e\\left({\\boldsymbol{p}}\\right) \\cdot S_{\\text{pls}}}tDevice​(p)=tDeviceImage​+21​⋅r(p)⋅Spls​⋅1+re​(p)⋅Spls​1+re​(p)​ Where tDevice(p)t_{\\text{Device}}(\\boldsymbol{p})tDevice​(p) represents the time of the pixel observation (effective center of exposure), r(p)r\\left({\\boldsymbol{p}}\\right)r(p) represents the readout time of the current pixel p\\boldsymbol{p}p and is computed as Δdtplsrow(p)H\\Delta \\text{dt}_{\\text{pls}} \\frac{\\text{row}(\\boldsymbol{p})}{H}Δdtpls​Hrow(p)​, $S_{\\text{pls}} $ represents the sensitivity ratio of the readout phase over the exposure phase, $r_e\\left({\\boldsymbol{p}}\\right) $ represents the ratio of the current row readout time over the regular exposure duration. "},{"title":"Technical specifications","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_spec","content":"Technical specifications The Technical Specifications section provides information about Aria glasses hardware, the different configurations Aria glasses can use when recording and how Aria glasses are calibrated.","keywords":""},{"title":"Device Calibration","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_spec/device_calibration","content":"","keywords":""},{"title":"Further resources​","type":1,"pageTitle":"Device Calibration","url":"/projectaria_tools/docs/tech_spec/device_calibration#further-resources","content":"Go to Calibration in Device Utilities to find out how to access device and sensor calibration. Go to Camera intrinsic models and Sensor measurement model for how we model sensors mathematically in calibration. "},{"title":"Aria Hardware specifications","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_spec/hardware_spec","content":"","keywords":""},{"title":"Sensor specifications​","type":1,"pageTitle":"Aria Hardware specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#sensor-specifications","content":"The following table summarizes the specs for the five cameras on Aria glasses. Camera\tHFOV (deg)\tVFOV (deg)\tIFOV (deg/pix)\tMaximum resolution (pix)\tDownsampled resolution (pix)\tMax frame rate (FPS)\tNominal frame rate (FPS)\tShutterMono Scene (x2)\t150\t120\t0.26\t640x480\t-\t30\t10\tglobal RGB (x1)\t110\t110\t0.038\t2880x2880\t1408x1408\t30\t10\trolling ET (x2)\t64\t48\t0.2\t640x480\t320x240\t90\t10\tglobal The non-visual sensors in Aria glasses are: Two IMUs operating at 1000Hz and 800Hz respectivelyOne Magnetometer operating at 10HzOne barometer operating at 50HzSeven-channel spatial microphone array with a sampling rate of 48kHz The microphone also has a stereo mode where only two channels record One GPS receiver, Wi-Fi beacon, and Bluetooth beacon. All cameras, as well as the IMU, magnetometer, barometer and microphone are calibrated and all sensor measurements are timestamped on a common clock at nanosecond resolution. The SLAM and RGB cameras have fisheye lenses to maximize the visible field of view. "},{"title":"Other hardware specifications​","type":1,"pageTitle":"Aria Hardware specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#other-hardware-specifications","content":""},{"title":"Compute​","type":1,"pageTitle":"Aria Hardware specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#compute","content":"Qualcomm SD835, 4GB RAM, 128GB storageFlash memory (UFS)Android 7.1SW configurable user button and switch "},{"title":"Weight & Size​","type":1,"pageTitle":"Aria Hardware specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#weight--size","content":"75g in two sizes 147mm and 152mm frame width, with adjustable nose pads and temple arms (87% fit coverage). "},{"title":"Visual Correction​","type":1,"pageTitle":"Aria Hardware specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#visual-correction","content":"Removable lenses with plano (Non-Rx) or single vision Rx correction [-4.5D to +3.5D]. "},{"title":"Battery Life​","type":1,"pageTitle":"Aria Hardware specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#battery-life","content":"Capacity is 2.5Wh. Operating time depends on the recording profile. Battery life is 1.5 hours of continuous recording + 30 hours standby when using profile 0: 10 FPS ET x 210 FPS SLAM x 21 FPS RGB 8MPIMU, Wi-Fi + GPS and 7-channel audio on at nominal FPS Aria glasses connects to USB via a magnetic connector on the right temple arm. "},{"title":"Recording Profiles","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_spec/recording_profiles","content":"Recording Profiles Aria glasses have multiple recording profiles that enable users to choose what sensors to record with and what settings to use. Project Aria currently supports 17 different recording profiles, which vary by: Sub-selection of sensor streamsRGB and ET (Eye Tracking) camera resolutionMono Scene (often, but not exclusively used for SLAM), RGB and ET camera frame rate and auto-exposure settingsImage stream formatNumber of audio channels: all (7) v.s. stereo(2) The default profile is Profile0. Its settings are: All sensors are turned onMono Scene cameras at 10 frame per second (FPS), RGB camera at 1 FPS and ET at 10 FPSFull resolution (2880x2880) RGB and downsampled (320x240) ETAll images in JPEG7-channel audio See the following table for a detailed spec of each profile we currently support. We add new profiles when necessary. To get the most up to date list of profiles, please browse them using the companion apps. If you would like a quick guide on how some recording profiles are used, as well as which ones are compatible with Machine Perception Services, go to the Recording Profile Guide. Microphones\tET Cameras\tRGB Cameras Mono Scene Cameras GPS IMU 1 IMU 2 Magnetometer Barometer Wi-Fi Profile\tChannels Sample Rate (kHz)\tResolution FPS\tAuto Exposure\tImage Format\tResolution FPS\tAuto Exposure\tImage Format\tResolution FPS\tAuto Exposure\tImage Format\tData Rate (Hz)\tData Rate (Hz)\tData Rate (Hz)\tData Rate (Hz)\tData Rate (Hz)\tScan Duration(s)0\t7 48 320x240 10 OFF JPEG 2880x2880 1 ON JPEG 640x480 10 ON JPEG 1 1000 800 10 50 10 2\t- - 1408x1408 20 ON JPEG 640x480 20 ON JPEG - - - - 1 1000 800 10 50 10 4\t7 48 - - - - 1408x1408 10 ON JPEG - - - - 1 1000 800 10 50 - 5\t- - 640x480 20 OFF JPEG 1408x1408 20 ON JPEG - - - - - 1000 800 - - - 7\t7 48 - - - - 1408x1408 10 ON RAW - - - - 1 1000 800 10 50 - 8\t7 48 320x240 30 OFF JPEG 1408x1408 5 ON JPEG 640x480 15 ON JPEG - 1000 800 10 50 - 9\t7 48 320x240 10 OFF JPEG 1408x1408 20 ON JPEG 640x480 10 ON JPEG - 1000 800 10 50 - 10\t7 48 320x240 10 OFF JPEG 1408x1408 10 ON JPEG 640x480 10 ON JPEG 1 1000 800 10 50 10 12\t- - 320x240 10 OFF JPEG 1408x1408 10 ON JPEG 640x480 10 ON JPEG - 1000 800 10 50 - 14\t- - 320x240 10 OFF JPEG 1408x1408 10 ON JPEG 640x480 30 ON JPEG 1 1000 800 10 50 - 15\t7 48 320x240 10 OFF JPEG 1408x1408 30 ON JPEG 640x480 30 ON JPEG - 1000 800 10 50 - 16\t2 48 640x480 90 OFF JPEG 1408x1408 10 ON JPEG - - - - - 1000 800 10 50 - 17\t- - 320x240 10 OFF VIDEO 1408x1408 10 ON VIDEO 640x480 10 ON VIDEO - 1000 800 10 50 - 18\t7 48 320x240 10 OFF JPEG 1408x1408 10 ON JPEG 640x480 10 ON JPEG - 1000 800 10 50 - 19\t- - - - - - 1408x1408 10 ON JPEG 640x480 10 ON JPEG 1 1000 800 10 50 10 20\t2 48 - - - - - - - - - - - - - 1000 800 - - - 21\t7 48 320x240 30 OFF JPEG 1408x1408 15 ON JPEG 640x480 15 ON JPEG - 1000 800 10 50 22\t7 48 320x240 10 OFF JPEG 1408x1408 30 ON JPEG 640x480 10 ON JPEG - 1000 800 10 50 23\t7 48 320x240 10 OFF JPEG 1408x1408 30 ON JPEG 640x480 10 ON JPEG - 1000 800 10 50 24\t- - - - - JPEG 2880x2880 10 ON JPEG 640x480 10 ON JPEG - 1000 800 10 50 25\t- - - - - JPEG 1408x1408 10 ON JPEG 640x480 20 ON JPEG 1 1000 800 - - 10 ","keywords":""},{"title":"Camera intrinsic models","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights/camera_intrinsic_models","content":"","keywords":""},{"title":"The linear camera model​","type":1,"pageTitle":"Camera intrinsic models","url":"/projectaria_tools/docs/tech_insights/camera_intrinsic_models#the-linear-camera-model","content":"The linear camera model (a.k.a pinhole model) is parametrized by 4 coefficients : f_x, f_y, c_x, c_y. (fx,fy)(f_x, f_y)(fx​,fy​) are the focal lengths, and cx,cyc_x, c_ycx​,cy​ are the coordinate of the projection of the optical axis. It maps from world point (x,y,z)(x,y,z)(x,y,z) to 2D camera pixel p=(u,v)\\mathbf{p}=(u, v)p=(u,v) with the following formulae. u=fxx/z+cxv=fyy/z+cyu = f_x x/z + c_x \\\\ v = f_y y/z + c_yu=fx​x/z+cx​v=fy​y/z+cy​ Or, in polar coordinates: u=fxtan(θ)cos⁡(φ)+cx,v=fytan(θ)sin⁡(φ)+cy.u = f_x tan(\\theta) \\cos(\\varphi) + c_x, \\\\ v = f_y tan(\\theta) \\sin(\\varphi) + c_y.u=fx​tan(θ)cos(φ)+cx​,v=fy​tan(θ)sin(φ)+cy​. Inversely, we can unproject from 2D camera pixel p=(u,v)\\mathbf{p}=(u, v)p=(u,v) to the homogeneous coordinate of the world point by x/z=(u−cx)/fx,y/z=(v−cy)/fy.x/z=(u-c_x)/f_x, \\\\ y/z=(v-c_y)/f_y.x/z=(u−cx​)/fx​,y/z=(v−cy​)/fy​. The linear camera model preserves linearity in 3D space, thus straight lines in the real world are supposed to look straight under the linear camera model. "},{"title":"The spherical camera model​","type":1,"pageTitle":"Camera intrinsic models","url":"/projectaria_tools/docs/tech_insights/camera_intrinsic_models#the-spherical-camera-model","content":"The spherical camera model is, similarly from the linear camera model parametrized by 4 coefficients : f_x, f_y, c_x, c_y. The pixel coordinates are linear to solid angles rather than the homography coordinate system. The projection function can be written in polar coordinates u=fxθcos⁡(φ)+cx,v=fyθsin⁡(φ)+cy.u = f_x \\theta \\cos(\\varphi) + c_x, \\\\ v = f_y \\theta \\sin(\\varphi) + c_y.u=fx​θcos(φ)+cx​,v=fy​θsin(φ)+cy​. Note the difference from the linear camera model — under spherical projection, 3D straight lines look curved in images. Inversely, we can unproject from 2D camera pixel p=(u,v)\\mathbf{p}=(u, v)p=(u,v) to the homogeneous coordinate of the world point by θ=(u−cx)2/fx2+(v−cy)2/fy2,φ=arctan⁡((u−cx)/fx,(v−cy)/fy).\\theta = \\sqrt{(u - c_x)^2/f_x^2 + (v - c_y)^2/f_y^2}, \\\\ \\varphi = \\arctan((u - c_x)/f_x, (v - c_y)/f_y).θ=(u−cx​)2/fx2​+(v−cy​)2/fy2​​,φ=arctan((u−cx​)/fx​,(v−cy​)/fy​). "},{"title":"The KannalaBrandtK3 (KB3) model​","type":1,"pageTitle":"Camera intrinsic models","url":"/projectaria_tools/docs/tech_insights/camera_intrinsic_models#the-kannalabrandtk3-kb3-model","content":"The KannalaBrandtK3 model adds radial distortion to the linear model u=fxr(θ)cos⁡(φ)+cx,v=fyr(θ)sin⁡(φ)+cy.u = f_x r(\\theta) \\cos(\\varphi) + c_x, \\quad v = f_y r(\\theta) \\sin(\\varphi) + c_y.u=fx​r(θ)cos(φ)+cx​,v=fy​r(θ)sin(φ)+cy​. where r(θ)=θ+k0θ3+k1θ5+k2θ7+k3θ9+...r(\\theta) = \\theta + k_0 \\theta^3 + k_1 \\theta^5 + k_2 \\theta^7 + k_3 \\theta^9 + ...r(θ)=θ+k0​θ3+k1​θ5+k2​θ7+k3​θ9+... In KannalaBrandtK3 model we use a 9-th order polynomial with four radial distortion parameters k0,...k3k_0, ... k_3k0​,...k3​. To unproject from camera pixel (u,v)(u, v)(u,v) to the world point (θ,φ)(\\theta, \\varphi)(θ,φ), we first compute φ=arctan⁡((u−cx)/fx,(v−cy)/fy)r(θ)=(u−cx)2/fx2+(v−cy)2/fy2\\varphi = \\arctan((u - c_x)/f_x, (v - c_y)/f_y) \\\\ r(\\theta) = \\sqrt{(u - c_x)^2/f_x^2 + (v - c_y)^2/f_y^2}φ=arctan((u−cx​)/fx​,(v−cy​)/fy​)r(θ)=(u−cx​)2/fx2​+(v−cy​)2/fy2​​ Then we use Newton method to inverse the function r(θ)r(\\theta)r(θ) to compute θ\\thetaθ. See the code here. "},{"title":"The Fisheye62 model​","type":1,"pageTitle":"Camera intrinsic models","url":"/projectaria_tools/docs/tech_insights/camera_intrinsic_models#the-fisheye62-model","content":"The Fisheye62 model adds tangential distortion on top of the KB3 model parametrized by two new coefficients: p_0 p_1. u=fx.(ur+tx(ur,vr))+cx,v=fy.(vr+ty(ur,vr))+cy.u = f_x . (u_r + t_x(u_r, v_r)) + c_x, \\\\ v = f_y . (v_r + t_y(u_r, v_r)) + c_y.u=fx​.(ur​+tx​(ur​,vr​))+cx​,v=fy​.(vr​+ty​(ur​,vr​))+cy​. where ur=r(θ)cos⁡(φ),vr=r(θ)sin⁡(φ).u_r = r(\\theta) \\cos(\\varphi), \\\\ v_r = r(\\theta) \\sin(\\varphi).ur​=r(θ)cos(φ),vr​=r(θ)sin(φ). and tx(ur,vr)=p0(2ur2+r(θ)2)+2p1urvr,ty(ur,vr)=p1(2vr2+r(θ)2)+2p0urvr.t_x(u_r, v_r) = p_0(2 u_r^2 + r(\\theta)^2) + 2p_1u_rv_r, \\\\ t_y(u_r, v_r) = p_1(2 v_r^2 + r(\\theta)^2) + 2p_0u_rv_r.tx​(ur​,vr​)=p0​(2ur2​+r(θ)2)+2p1​ur​vr​,ty​(ur​,vr​)=p1​(2vr2​+r(θ)2)+2p0​ur​vr​. To unproject from camera pixel (u,v)(u, v)(u,v) to the world point (θ,φ)(\\theta, \\varphi)(θ,φ), we first use Newton method to compute uru_rur​ and vrv_rvr​ from (u−cx)/fx(u - c_x)/f_x(u−cx​)/fx​ and (v−cy)/fy(v - cy)/f_y(v−cy)/fy​, and then compute (θ,φ)(\\theta, \\varphi)(θ,φ) using the above KB3 unproject method. "},{"title":"The FisheyeRadTanThinPrism (Fisheye624) model​","type":1,"pageTitle":"Camera intrinsic models","url":"/projectaria_tools/docs/tech_insights/camera_intrinsic_models#the-fisheyeradtanthinprism-fisheye624-model","content":"The FisheyeRadTanThinPrism (also called Fisheye624 in file and codebase) models thin-prism distortion (noted tptptp) on top of the Fisheye62 model above. Its parametrization contains 4 additional coefficients: s_0 s_1 s_2 s_3. The projection function writes: u=fx⋅(ur+tx(ur,vr)+tpx(ur,vr))+cx,v=fy⋅(vr+ty(ur,vr)+tpy(ur,vr))+cy.u = f_x \\cdot (u_r + t_x(u_r, v_r) + tp_x(u_r, v_r)) + c_x, \\\\ v = f_y \\cdot (v_r + t_y(u_r, v_r) + tp_y(u_r, v_r)) + c_y.u=fx​⋅(ur​+tx​(ur​,vr​)+tpx​(ur​,vr​))+cx​,v=fy​⋅(vr​+ty​(ur​,vr​)+tpy​(ur​,vr​))+cy​. u_r, v_r, t_x, t_y are defined as in the Fisheye62 model, while tpxtp_xtpx​ and tpytp_ytpy​ are defined as: tpx(ur,vr)=s0r(θ)2+s1r(θ)4tpx(ur,vr)=s2r(θ)2+s3r(θ)4tp_x(u_r, v_r) = s_0 r(\\theta)^2 + s_1 r(\\theta)^4 tp_x(u_r, v_r) = s_2 r(\\theta)^2 + s_3 r(\\theta)^4tpx​(ur​,vr​)=s0​r(θ)2+s1​r(θ)4tpx​(ur​,vr​)=s2​r(θ)2+s3​r(θ)4 To unproject from camera pixel (u,v)(u, v)(u,v) to the world point (θ,φ)(\\theta, \\varphi)(θ,φ), we first use Newton method to compute uru_rur​ and vrv_rvr​ from (u−cx)/fx(u - c_x)/f_x(u−cx​)/fx​ and (v−cy)/fy(v - cy)/f_y(v−cy)/fy​, and then compute (θ,φ)(\\theta, \\varphi)(θ,φ) using the above KB3 unproject method. Note that in practice, in our codebase and calibration file we assume fxf_xfx​ and fyf_yfy​ are equal. "}]