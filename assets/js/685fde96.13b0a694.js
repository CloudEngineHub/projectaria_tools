"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[8684],{15680:(e,t,o)=>{o.r(t),o.d(t,{MDXContext:()=>l,MDXProvider:()=>u,mdx:()=>b,useMDXComponents:()=>p,withMDXComponents:()=>c});var a=o(96540);function n(e,t,o){return t in e?Object.defineProperty(e,t,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[t]=o,e}function r(){return r=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var o=arguments[t];for(var a in o)Object.prototype.hasOwnProperty.call(o,a)&&(e[a]=o[a])}return e},r.apply(this,arguments)}function i(e,t){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),o.push.apply(o,a)}return o}function s(e){for(var t=1;t<arguments.length;t++){var o=null!=arguments[t]?arguments[t]:{};t%2?i(Object(o),!0).forEach((function(t){n(e,t,o[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):i(Object(o)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(o,t))}))}return e}function d(e,t){if(null==e)return{};var o,a,n=function(e,t){if(null==e)return{};var o,a,n={},r=Object.keys(e);for(a=0;a<r.length;a++)o=r[a],t.indexOf(o)>=0||(n[o]=e[o]);return n}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)o=r[a],t.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(n[o]=e[o])}return n}var l=a.createContext({}),c=function(e){return function(t){var o=p(t.components);return a.createElement(e,r({},t,{components:o}))}},p=function(e){var t=a.useContext(l),o=t;return e&&(o="function"==typeof e?e(t):s(s({},t),e)),o},u=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},m="mdxType",f={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var o=e.components,n=e.mdxType,r=e.originalType,i=e.parentName,l=d(e,["components","mdxType","originalType","parentName"]),c=p(o),u=n,m=c["".concat(i,".").concat(u)]||c[u]||f[u]||r;return o?a.createElement(m,s(s({ref:t},l),{},{components:o})):a.createElement(m,s({ref:t},l))}));function b(e,t){var o=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var r=o.length,i=new Array(r);i[0]=h;var s={};for(var d in t)hasOwnProperty.call(t,d)&&(s[d]=t[d]);s.originalType=e,s[m]="string"==typeof e?e:n,i[1]=s;for(var l=2;l<r;l++)i[l]=o[l];return a.createElement.apply(null,i)}return a.createElement.apply(null,o)}h.displayName="MDXCreateElement"},59770:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>d,contentTitle:()=>i,default:()=>u,frontMatter:()=>r,metadata:()=>s,toc:()=>l});var a=o(58168),n=(o(96540),o(15680));const r={sidebar_position:60,title:"HOT3D Dataset"},i="HOT3D Dataset",s={unversionedId:"open_datasets/hot3d",id:"open_datasets/hot3d",title:"HOT3D Dataset",description:"HOT3D is a new benchmark dataset for vision-based understanding of 3D hand-object interactions. This dataset contains over 800 minutes of egocentric recordings, with 33 diverse hand-held objects, capturing over one million multi-view frames of hand-object interactions.",source:"@site/docs/open_datasets/hot3d.mdx",sourceDirName:"open_datasets",slug:"/open_datasets/hot3d",permalink:"/projectaria_tools/docs/open_datasets/hot3d",draft:!1,editUrl:"https://github.com/facebookresearch/projectaria_tools/tree/main/website/docs/open_datasets/hot3d.mdx",tags:[],version:"current",sidebarPosition:60,frontMatter:{sidebar_position:60,title:"HOT3D Dataset"},sidebar:"tutorialSidebar",previous:{title:"Ego-Exo4D Data Format and Loader",permalink:"/projectaria_tools/docs/open_datasets/ego-exo4d/ego-exo4d_data_format"},next:{title:"Tech Insights",permalink:"/projectaria_tools/docs/tech_insights/"}},d={},l=[{value:"Getting Started",id:"getting-started",level:2}],c={toc:l},p="wrapper";function u(e){let{components:t,...o}=e;return(0,n.mdx)(p,(0,a.A)({},c,o,{components:t,mdxType:"MDXLayout"}),(0,n.mdx)("h1",{id:"hot3d-dataset"},"HOT3D Dataset"),(0,n.mdx)("p",null,"HOT3D is a new benchmark dataset for vision-based understanding of 3D hand-object interactions. This dataset contains over 800 minutes of egocentric recordings, with 33 diverse hand-held objects, capturing over one million multi-view frames of hand-object interactions."),(0,n.mdx)("p",null,"The dataset contains:"),(0,n.mdx)("ul",null,(0,n.mdx)("li",{parentName:"ul"},"Synchronized multi-view egocentric videos from Project Aria glasses and Quest 3 VR headset"),(0,n.mdx)("li",{parentName:"ul"},"High-quality 3D pose annotations of hands and objects"),(0,n.mdx)("li",{parentName:"ul"},"3D object models with PBR materials"),(0,n.mdx)("li",{parentName:"ul"},"2D bounding boxes"),(0,n.mdx)("li",{parentName:"ul"},(0,n.mdx)("a",{parentName:"li",href:"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze"},"Eye Gaze MPS data")," (Aria only)"),(0,n.mdx)("li",{parentName:"ul"},(0,n.mdx)("a",{parentName:"li",href:"/projectaria_tools/docs/data_formats/mps/slam/mps_pointcloud"},"Semi-Dense Point Cloud MPS data"),"(Aria only)")),(0,n.mdx)("p",null,"HOT3D uses its own specific downloader, available in the HOT3D GitHub repository, enabling you to download Quest3, Aria and object models data."),(0,n.mdx)("h2",{id:"getting-started"},"Getting Started"),(0,n.mdx)("p",null,"Go to the ",(0,n.mdx)("a",{parentName:"p",href:"http://Projectaria.com"},"https://www.projectaria.com/datasets/hot3D/")," to find out more about the dataset and to get access to it."),(0,n.mdx)("p",null,"Go to the ",(0,n.mdx)("a",{parentName:"p",href:"https://github.com/facebookresearch/hot3d"},"HOT3D GitHub repository")," to install HOT3D Python tooling that will enable you to download and visualize HOT3D data."),(0,n.mdx)("ul",null,(0,n.mdx)("li",{parentName:"ul"},"Use the ",(0,n.mdx)("a",{parentName:"li",href:"https://github.com/facebookresearch/hot3d/blob/main/hot3d/HOT3D_Tutorial.ipynb"},"HOT3D Jupyter notebook tutorial")," to get to know the downloader and visualizers.")))}u.isMDXComponent=!0}}]);